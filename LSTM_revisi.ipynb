{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import date, datetime\n",
    "from multiprocessing.spawn import import_main_path\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Attributes</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Adj Close</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Close</th>\n",
       "      <th colspan=\"2\" halign=\"left\">High</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Low</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Open</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symbols</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>...</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>1576.277344</td>\n",
       "      <td>2454.148438</td>\n",
       "      <td>1408.559082</td>\n",
       "      <td>1530.937988</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1837.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1762.5</td>\n",
       "      <td>1805.770874</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1787.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>39619544.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>26442000.0</td>\n",
       "      <td>6978806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1576.277344</td>\n",
       "      <td>2468.053955</td>\n",
       "      <td>1437.305054</td>\n",
       "      <td>1547.488525</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>62041590.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42874000.0</td>\n",
       "      <td>7988164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1576.277344</td>\n",
       "      <td>2454.148438</td>\n",
       "      <td>1427.722900</td>\n",
       "      <td>1555.763916</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1875.607300</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1855.654053</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1875.607300</td>\n",
       "      <td>30916328.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44946000.0</td>\n",
       "      <td>7538113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1610.544189</td>\n",
       "      <td>2377.673828</td>\n",
       "      <td>1437.305054</td>\n",
       "      <td>1547.488525</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>30624653.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24863000.0</td>\n",
       "      <td>2048787.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1610.544189</td>\n",
       "      <td>2391.578613</td>\n",
       "      <td>1446.887085</td>\n",
       "      <td>1539.213135</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>1887.5</td>\n",
       "      <td>1855.654053</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1855.654053</td>\n",
       "      <td>15857579.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19118000.0</td>\n",
       "      <td>2441705.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Attributes    Adj Close                                               Close  \\\n",
       "Symbols         ANTM.JK      ASII.JK      ICBP.JK      JSMR.JK      ANTM.JK   \n",
       "Date                                                                          \n",
       "2010-01-04  1576.277344  2454.148438  1408.559082  1530.937988  1931.946777   \n",
       "2010-01-05  1576.277344  2468.053955  1437.305054  1547.488525  1931.946777   \n",
       "2010-01-06  1576.277344  2454.148438  1427.722900  1555.763916  1931.946777   \n",
       "2010-01-07  1610.544189  2377.673828  1437.305054  1547.488525  1973.945557   \n",
       "2010-01-08  1610.544189  2391.578613  1446.887085  1539.213135  1973.945557   \n",
       "\n",
       "Attributes                                      High          ...     Low  \\\n",
       "Symbols    ASII.JK ICBP.JK      JSMR.JK      ANTM.JK ASII.JK  ... ICBP.JK   \n",
       "Date                                                          ...           \n",
       "2010-01-04  3530.0  1837.5  1845.677368  1931.946777  3550.0  ...  1762.5   \n",
       "2010-01-05  3550.0  1875.0  1865.630737  1994.945068  3570.0  ...  1825.0   \n",
       "2010-01-06  3530.0  1862.5  1875.607300  1994.945068  3580.0  ...  1862.5   \n",
       "2010-01-07  3420.0  1875.0  1865.630737  1994.945068  3560.0  ...  1862.5   \n",
       "2010-01-08  3440.0  1887.5  1855.654053  1994.945068  3450.0  ...  1862.5   \n",
       "\n",
       "Attributes                      Open                                   Volume  \\\n",
       "Symbols         JSMR.JK      ANTM.JK ASII.JK ICBP.JK      JSMR.JK     ANTM.JK   \n",
       "Date                                                                            \n",
       "2010-01-04  1805.770874  1931.946777  3530.0  1787.5  1845.677368  39619544.0   \n",
       "2010-01-05  1845.677368  1931.946777  3550.0  1875.0  1865.630737  62041590.0   \n",
       "2010-01-06  1855.654053  1931.946777  3530.0  1900.0  1875.607300  30916328.0   \n",
       "2010-01-07  1845.677368  1973.945557  3420.0  1875.0  1865.630737  30624653.0   \n",
       "2010-01-08  1845.677368  1973.945557  3440.0  1900.0  1855.654053  15857579.0   \n",
       "\n",
       "Attributes                                 \n",
       "Symbols    ASII.JK     ICBP.JK    JSMR.JK  \n",
       "Date                                       \n",
       "2010-01-04    40.0  26442000.0  6978806.0  \n",
       "2010-01-05    40.0  42874000.0  7988164.0  \n",
       "2010-01-06    40.0  44946000.0  7538113.0  \n",
       "2010-01-07    40.0  24863000.0  2048787.0  \n",
       "2010-01-08    40.0  19118000.0  2441705.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = [\"ANTM.JK\",\"ASII.JK\",\"ICBP.JK\",\"JSMR.JK\"]\n",
    "\n",
    "today = date.today()\n",
    "# print(today)\n",
    "start_date = '2010-01-01'\n",
    "# end_date = '2022-08-02'\n",
    "\n",
    "panel_data = data.DataReader(tickers,'yahoo',start_date,today)\n",
    "panel_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Symbols</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>1576.28</td>\n",
       "      <td>2454.15</td>\n",
       "      <td>1408.56</td>\n",
       "      <td>1530.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1576.28</td>\n",
       "      <td>2468.05</td>\n",
       "      <td>1437.31</td>\n",
       "      <td>1547.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1576.28</td>\n",
       "      <td>2454.15</td>\n",
       "      <td>1427.72</td>\n",
       "      <td>1555.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1610.54</td>\n",
       "      <td>2377.67</td>\n",
       "      <td>1437.31</td>\n",
       "      <td>1547.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1610.54</td>\n",
       "      <td>2391.58</td>\n",
       "      <td>1446.89</td>\n",
       "      <td>1539.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Symbols     ANTM.JK  ASII.JK  ICBP.JK  JSMR.JK\n",
       "Date                                          \n",
       "2010-01-04  1576.28  2454.15  1408.56  1530.94\n",
       "2010-01-05  1576.28  2468.05  1437.31  1547.49\n",
       "2010-01-06  1576.28  2454.15  1427.72  1555.76\n",
       "2010-01-07  1610.54  2377.67  1437.31  1547.49\n",
       "2010-01-08  1610.54  2391.58  1446.89  1539.21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_close = panel_data[\"Adj Close\"]\n",
    "data_close.head(5).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2163070a3a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot everything by leveraging the very powerful matplotlib package\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "for name in tickers:\n",
    "    ax.plot(data_close[name].index, data_close[name], label='Saham {}'.format(name))\n",
    "\n",
    "\n",
    "# Define the date format\n",
    "date_form = DateFormatter(\"%b-%y\")\n",
    "ax.xaxis.set_major_formatter(date_form)\n",
    "\n",
    "ax.set_title('Harga saham ')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Closing price (Rp)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antm</th>\n",
       "      <th>asii</th>\n",
       "      <th>icbp</th>\n",
       "      <th>jsmr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>1576.277344</td>\n",
       "      <td>2454.148438</td>\n",
       "      <td>1408.559082</td>\n",
       "      <td>1530.937988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1576.277344</td>\n",
       "      <td>2468.053955</td>\n",
       "      <td>1437.305054</td>\n",
       "      <td>1547.488525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1576.277344</td>\n",
       "      <td>2454.148438</td>\n",
       "      <td>1427.722900</td>\n",
       "      <td>1555.763916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1610.544189</td>\n",
       "      <td>2377.673828</td>\n",
       "      <td>1437.305054</td>\n",
       "      <td>1547.488525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1610.544189</td>\n",
       "      <td>2391.578613</td>\n",
       "      <td>1446.887085</td>\n",
       "      <td>1539.213135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>1785.000000</td>\n",
       "      <td>6100.000000</td>\n",
       "      <td>9050.941406</td>\n",
       "      <td>3510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>6050.000000</td>\n",
       "      <td>8758.188477</td>\n",
       "      <td>3580.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>1955.000000</td>\n",
       "      <td>6325.000000</td>\n",
       "      <td>8611.811523</td>\n",
       "      <td>3560.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>2080.000000</td>\n",
       "      <td>6475.000000</td>\n",
       "      <td>8685.000000</td>\n",
       "      <td>3520.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-02</th>\n",
       "      <td>1995.000000</td>\n",
       "      <td>6475.000000</td>\n",
       "      <td>8975.000000</td>\n",
       "      <td>3450.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3117 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   antm         asii         icbp         jsmr\n",
       "Date                                                          \n",
       "2010-01-04  1576.277344  2454.148438  1408.559082  1530.937988\n",
       "2010-01-05  1576.277344  2468.053955  1437.305054  1547.488525\n",
       "2010-01-06  1576.277344  2454.148438  1427.722900  1555.763916\n",
       "2010-01-07  1610.544189  2377.673828  1437.305054  1547.488525\n",
       "2010-01-08  1610.544189  2391.578613  1446.887085  1539.213135\n",
       "...                 ...          ...          ...          ...\n",
       "2022-07-27  1785.000000  6100.000000  9050.941406  3510.000000\n",
       "2022-07-28  2000.000000  6050.000000  8758.188477  3580.000000\n",
       "2022-07-29  1955.000000  6325.000000  8611.811523  3560.000000\n",
       "2022-08-01  2080.000000  6475.000000  8685.000000  3520.000000\n",
       "2022-08-02  1995.000000  6475.000000  8975.000000  3450.000000\n",
       "\n",
       "[3117 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_close)\n",
    "data_close.columns = ['antm', 'asii', 'icbp', 'jsmr']\n",
    "data_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_antm = pd.DataFrame(data_close.antm)\n",
    "df_asii = pd.DataFrame(data_close.asii)\n",
    "df_icbp = pd.DataFrame(data_close.icbp)\n",
    "df_jsmr = pd.DataFrame(data_close.jsmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_antm = pd.DataFrame(scaler.fit_transform(df_antm), columns = ['antm'])\n",
    "df_asii = pd.DataFrame(scaler.fit_transform(df_asii), columns = ['asii'])\n",
    "df_icbp = pd.DataFrame(scaler.fit_transform(df_icbp), columns = ['icbp'])\n",
    "df_jsmr = pd.DataFrame(scaler.fit_transform(df_jsmr), columns = ['jsmr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "def split_sequence(seq, n_steps):\n",
    "    X,y = list(), list()\n",
    "    for i in range(len(seq)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(seq)-1:\n",
    "            break\n",
    "        seq_x, seq_y =seq[i:end_ix],seq[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNING PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import DataFrame\n",
    "# from pandas import Series\n",
    "# from pandas import concat\n",
    "# from pandas import read_csv\n",
    "# from pandas import datetime\n",
    "# from math import sqrt\n",
    "# import matplotlib\n",
    "# import numpy\n",
    "# from numpy import concatenate\n",
    "\n",
    "# # date-time parsing function for loading the dataset\n",
    "# def parser(x):\n",
    "# \treturn datetime.strptime('190'+x, '%Y-%m')\n",
    " \n",
    "\n",
    "# # frame a sequence as a supervised learning problem\n",
    "# def timeseries_to_supervised(data, lag=1):\n",
    "# \tdf = DataFrame(data)\n",
    "# \tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "# \tcolumns.append(df)\n",
    "# \tdf = concat(columns, axis=1)\n",
    "# \treturn df\n",
    " \n",
    "# # create a differenced series\n",
    "# def difference(dataset, interval=1):\n",
    "# \tdiff = list()\n",
    "# \tfor i in range(interval, len(dataset)):\n",
    "# \t\tvalue = dataset[i] - dataset[i - interval]\n",
    "# \t\tdiff.append(value)\n",
    "# \treturn Series(diff)\n",
    " \n",
    "# # invert differenced value\n",
    "# def inverse_difference(history, yhat, interval=1):\n",
    "# \treturn yhat + history[-interval]\n",
    " \n",
    "# # scale train and test data to [-1, 1]\n",
    "# def scale(train, test):\n",
    "# \t# fit scaler\n",
    "# \tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# \tscaler = scaler.fit(train)\n",
    "# \t# transform train\n",
    "# \ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "# \ttrain_scaled = scaler.transform(train)\n",
    "# \t# transform test\n",
    "# \ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "# \ttest_scaled = scaler.transform(test)\n",
    "# \treturn scaler, train_scaled, test_scaled\n",
    " \n",
    "# # inverse scaling for a forecasted value\n",
    "# def invert_scale(scaler, X, yhat):\n",
    "# \tnew_row = [x for x in X] + [yhat]\n",
    "# \tarray = numpy.array(new_row)\n",
    "# \tarray = array.reshape(1, len(array))\n",
    "# \tinverted = scaler.inverse_transform(array)\n",
    "# \treturn inverted[0, -1]\n",
    " \n",
    "# # fit an LSTM network to training data\n",
    "# def fit_lstm(train, batch_size, nb_epoch, neurons, timesteps):\n",
    "# \tX, y = train[:, 0:-1], train[:, -1]\n",
    "# \tX = X.reshape(X.shape[0], timesteps, 1)\n",
    "# \tmodel = Sequential()\n",
    "# \tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "# \tmodel.add(Dense(1))\n",
    "# \tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# \tfor i in range(nb_epoch):\n",
    "# \t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "# \t\tmodel.reset_states()\n",
    "# \treturn model\n",
    " \n",
    "# # make a one-step forecast\n",
    "# def forecast_lstm(model, batch_size, X):\n",
    "# \tX = X.reshape(1, len(X), 1)\n",
    "# \tyhat = model.predict(X, batch_size=batch_size)\n",
    "# \treturn yhat[0,0]\n",
    " \n",
    "# # run a repeated experiment\n",
    "# def experiment(repeats, series, timesteps):\n",
    "# \t# transform data to be stationary\n",
    "# \traw_values = series.values\n",
    "# \tdiff_values = difference(raw_values, 1)\n",
    "# \t# transform data to be supervised learning\n",
    "# \tsupervised = timeseries_to_supervised(diff_values, timesteps)\n",
    "# \tsupervised_values = supervised.values[timesteps:,:]\n",
    "# \t# split data into train and test-sets\n",
    "# \ttrain, test = supervised_values[0:-12, :], supervised_values[-12:, :]\n",
    "# \t# transform the scale of the data\n",
    "# \tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "# \t# run experiment\n",
    "# \terror_scores = list()\n",
    "# \tfor r in range(repeats):\n",
    "# \t\t# fit the base model\n",
    "# \t\tlstm_model = fit_lstm(train_scaled, 1, 100, 1, timesteps)\n",
    "# \t\t# forecast test dataset\n",
    "# \t\tpredictions = list()\n",
    "# \t\tfor i in range(len(test_scaled)):\n",
    "# \t\t\t# predict\n",
    "# \t\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "# \t\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "# \t\t\t# invert scaling\n",
    "# \t\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "# \t\t\t# invert differencing\n",
    "# \t\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "# \t\t\t# store forecast\n",
    "# \t\t\tpredictions.append(yhat)\n",
    "# \t\t# report performance\n",
    "# \t\trmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "# \t\tprint('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "# \t\terror_scores.append(rmse)\n",
    "# \treturn error_scores\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # execute the experiment\n",
    "# def run():\n",
    "# \t# load dataset\n",
    "# \t# experiment\n",
    "# \trepeats = 2\n",
    "# \tresults = DataFrame()\n",
    "# \t# run experiment\n",
    "# \ttimesteps = 1\n",
    "# \tresults['results'] = experiment(repeats, df_antm, timesteps)\n",
    "# \t# summarize results\n",
    "# \tprint(results.describe())\n",
    "# \t# save results\n",
    "# \tresults.to_csv('antm_experiment_timesteps_1.csv', index=False)\n",
    " \n",
    "#  # entry point\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import DataFrame\n",
    "# from pandas import read_csv\n",
    "# from matplotlib import pyplot\n",
    "# # load results into a dataframe\n",
    "# filenames = ['experiment_timesteps_1.csv', 'experiment_timesteps_2.csv',\n",
    "# \t'experiment_timesteps_3.csv','experiment_timesteps_4.csv','experiment_timesteps_5.csv']\n",
    "# results = DataFrame()\n",
    "# for name in filenames:\n",
    "# \tresults[name[11:-4]] = read_csv(name, header=0)\n",
    "# # describe all results\n",
    "# print(results.describe())\n",
    "# # box and whisker plot\n",
    "# results.boxplot()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIMESTEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_1 = 1\n",
    "n_steps_2 = 2\n",
    "n_steps_3 = 3\n",
    "n_steps_4 = 4\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_antm = df_antm.reset_index(drop=True)\n",
    "arr_antm = df_antm.to_numpy()\n",
    "flat_antm = arr_antm.flatten()\n",
    "antm_X_1, antm_y_1 = split_sequence(flat_antm, n_steps_1)\n",
    "antm_X_2, antm_y_2 = split_sequence(flat_antm, n_steps_2)\n",
    "antm_X_3, antm_y_3 = split_sequence(flat_antm, n_steps_3)\n",
    "antm_X_4, antm_y_4 = split_sequence(flat_antm, n_steps_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_antm_1, X_test_antm_1, y_train_antm_1, y_test_antm_1 = train_test_split(antm_X_1, antm_y_1, test_size=0.33, random_state=42)\n",
    "X_train_antm_2, X_test_antm_2, y_train_antm_2, y_test_antm_2 = train_test_split(antm_X_2, antm_y_2, test_size=0.33, random_state=42)\n",
    "X_train_antm_3, X_test_antm_3, y_train_antm_3, y_test_antm_3 = train_test_split(antm_X_3, antm_y_3, test_size=0.33, random_state=42)\n",
    "X_train_antm_4, X_test_antm_4, y_train_antm_4, y_test_antm_4 = train_test_split(antm_X_4, antm_y_4, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_antm_1 = X_train_antm_1.reshape(X_train_antm_1.shape[0],X_train_antm_1.shape[1],n_features)\n",
    "X_test_antm_1 = X_test_antm_1.reshape(X_test_antm_1.shape[0],X_test_antm_1.shape[1],n_features)\n",
    "X_train_antm_2 = X_train_antm_2.reshape(X_train_antm_2.shape[0],X_train_antm_2.shape[1],n_features)\n",
    "X_test_antm_2 = X_test_antm_2.reshape(X_test_antm_2.shape[0],X_test_antm_2.shape[1],n_features)\n",
    "X_train_antm_3 = X_train_antm_3.reshape(X_train_antm_3.shape[0],X_train_antm_3.shape[1],n_features)\n",
    "X_test_antm_3 = X_test_antm_3.reshape(X_test_antm_3.shape[0],X_test_antm_3.shape[1],n_features)\n",
    "X_train_antm_4 = X_train_antm_4.reshape(X_train_antm_4.shape[0],X_train_antm_4.shape[1],n_features)\n",
    "X_test_antm_4 = X_test_antm_4.reshape(X_test_antm_4.shape[0],X_test_antm_4.shape[1],n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_antm_1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_antm_2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_antm_3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_antm_4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_4, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 12ms/step - loss: 0.1469 - mae: 0.1469 - val_loss: 0.1002 - val_mae: 0.1002\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1293 - mae: 0.1293 - val_loss: 0.0810 - val_mae: 0.0810\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1278 - mae: 0.1278 - val_loss: 0.0666 - val_mae: 0.0666\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1275 - mae: 0.1275 - val_loss: 0.0564 - val_mae: 0.0564\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1278 - mae: 0.1278 - val_loss: 0.0539 - val_mae: 0.0539\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1266 - mae: 0.1266 - val_loss: 0.0459 - val_mae: 0.0459\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1263 - mae: 0.1263 - val_loss: 0.0434 - val_mae: 0.0434\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1265 - mae: 0.1265 - val_loss: 0.0397 - val_mae: 0.0397\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1264 - mae: 0.126 - 0s 3ms/step - loss: 0.1260 - mae: 0.1260 - val_loss: 0.0430 - val_mae: 0.0430\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1259 - mae: 0.1259 - val_loss: 0.0396 - val_mae: 0.0396\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1257 - mae: 0.1257 - val_loss: 0.0452 - val_mae: 0.0452\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1266 - mae: 0.1266 - val_loss: 0.0525 - val_mae: 0.0525\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1259 - mae: 0.1259 - val_loss: 0.0530 - val_mae: 0.0530\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1257 - mae: 0.1257 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1251 - mae: 0.1251 - val_loss: 0.0433 - val_mae: 0.0433\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.1256 - val_loss: 0.0538 - val_mae: 0.0538\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1251 - mae: 0.1251 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1252 - mae: 0.1252 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1249 - mae: 0.1249 - val_loss: 0.0505 - val_mae: 0.0505\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1246 - mae: 0.1246 - val_loss: 0.0453 - val_mae: 0.0453\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1248 - mae: 0.1248 - val_loss: 0.0452 - val_mae: 0.0452\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1239 - mae: 0.1239 - val_loss: 0.0472 - val_mae: 0.0472\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1247 - mae: 0.1247 - val_loss: 0.0519 - val_mae: 0.0519\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1243 - mae: 0.1243 - val_loss: 0.0483 - val_mae: 0.0483\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1244 - mae: 0.1244 - val_loss: 0.0470 - val_mae: 0.0470\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1241 - mae: 0.1241 - val_loss: 0.0583 - val_mae: 0.0583\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1243 - mae: 0.1243 - val_loss: 0.0567 - val_mae: 0.0567\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1239 - mae: 0.1239 - val_loss: 0.0553 - val_mae: 0.0553\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1238 - mae: 0.1238 - val_loss: 0.0555 - val_mae: 0.0555\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1229 - mae: 0.1229 - val_loss: 0.0560 - val_mae: 0.0560\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1246 - mae: 0.1246 - val_loss: 0.0587 - val_mae: 0.0587\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1250 - mae: 0.1250 - val_loss: 0.0585 - val_mae: 0.0585\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1247 - mae: 0.1247 - val_loss: 0.0654 - val_mae: 0.0654\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1228 - mae: 0.1228 - val_loss: 0.0523 - val_mae: 0.0523\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1228 - mae: 0.1228 - val_loss: 0.0537 - val_mae: 0.0537\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1236 - mae: 0.1236 - val_loss: 0.0604 - val_mae: 0.0604\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1231 - mae: 0.1231 - val_loss: 0.0568 - val_mae: 0.0568\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1224 - mae: 0.1224 - val_loss: 0.0636 - val_mae: 0.0636\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1221 - mae: 0.1221 - val_loss: 0.0600 - val_mae: 0.0600\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1218 - mae: 0.1218 - val_loss: 0.0595 - val_mae: 0.0595\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1222 - mae: 0.1222 - val_loss: 0.0625 - val_mae: 0.0625\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1222 - mae: 0.1222 - val_loss: 0.0577 - val_mae: 0.0577\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1219 - mae: 0.1219 - val_loss: 0.0568 - val_mae: 0.0568\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1228 - mae: 0.1228 - val_loss: 0.0600 - val_mae: 0.0600\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1217 - mae: 0.1217 - val_loss: 0.0612 - val_mae: 0.0612\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1219 - mae: 0.1219 - val_loss: 0.0625 - val_mae: 0.0625\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1221 - mae: 0.1221 - val_loss: 0.0635 - val_mae: 0.0635\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1224 - mae: 0.1224 - val_loss: 0.0762 - val_mae: 0.0762\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1236 - mae: 0.1236 - val_loss: 0.0630 - val_mae: 0.0630\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1220 - mae: 0.1220 - val_loss: 0.0668 - val_mae: 0.0668\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1227 - mae: 0.1227 - val_loss: 0.0652 - val_mae: 0.0652\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1220 - mae: 0.1220 - val_loss: 0.0630 - val_mae: 0.0630\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1218 - mae: 0.1218 - val_loss: 0.0657 - val_mae: 0.0657\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1227 - mae: 0.1227 - val_loss: 0.0659 - val_mae: 0.0659\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1220 - mae: 0.1220 - val_loss: 0.0589 - val_mae: 0.0589\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1218 - mae: 0.1218 - val_loss: 0.0666 - val_mae: 0.0666\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1221 - mae: 0.1221 - val_loss: 0.0669 - val_mae: 0.0669\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1226 - mae: 0.1226 - val_loss: 0.0703 - val_mae: 0.0703\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1218 - mae: 0.1218 - val_loss: 0.0682 - val_mae: 0.0682\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1223 - mae: 0.1223 - val_loss: 0.0656 - val_mae: 0.0656\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1225 - mae: 0.1225 - val_loss: 0.0600 - val_mae: 0.0600\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1220 - mae: 0.1220 - val_loss: 0.0632 - val_mae: 0.0632\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1217 - mae: 0.1217 - val_loss: 0.0641 - val_mae: 0.0641\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1217 - mae: 0.1217 - val_loss: 0.0620 - val_mae: 0.0620\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1224 - mae: 0.1224 - val_loss: 0.0639 - val_mae: 0.0639\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1230 - mae: 0.1230 - val_loss: 0.0658 - val_mae: 0.0658\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1223 - mae: 0.1223 - val_loss: 0.0650 - val_mae: 0.0650\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1234 - mae: 0.1234 - val_loss: 0.0720 - val_mae: 0.0720\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1220 - mae: 0.1220 - val_loss: 0.0687 - val_mae: 0.0687\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1233 - mae: 0.1233 - val_loss: 0.0675 - val_mae: 0.0675\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1225 - mae: 0.1225 - val_loss: 0.0664 - val_mae: 0.0664\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1214 - mae: 0.1214 - val_loss: 0.0686 - val_mae: 0.0686\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1214 - mae: 0.1214 - val_loss: 0.0701 - val_mae: 0.0701\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1234 - mae: 0.1234 - val_loss: 0.0752 - val_mae: 0.0752\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1225 - mae: 0.1225 - val_loss: 0.0713 - val_mae: 0.0713\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1216 - mae: 0.1216 - val_loss: 0.0633 - val_mae: 0.0633\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1214 - mae: 0.1214 - val_loss: 0.0632 - val_mae: 0.0632\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1222 - mae: 0.1222 - val_loss: 0.0701 - val_mae: 0.0701\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1222 - mae: 0.1222 - val_loss: 0.0671 - val_mae: 0.0671\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1215 - mae: 0.1215 - val_loss: 0.0631 - val_mae: 0.0631\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1213 - mae: 0.1213 - val_loss: 0.0648 - val_mae: 0.0648\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1212 - mae: 0.1212 - val_loss: 0.0649 - val_mae: 0.0649\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1217 - mae: 0.1217 - val_loss: 0.0624 - val_mae: 0.0624\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1218 - mae: 0.1218 - val_loss: 0.0670 - val_mae: 0.0670\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1216 - mae: 0.1216 - val_loss: 0.0704 - val_mae: 0.0704\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1214 - mae: 0.1214 - val_loss: 0.0625 - val_mae: 0.0625\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1213 - mae: 0.1213 - val_loss: 0.0612 - val_mae: 0.0612\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1214 - mae: 0.1214 - val_loss: 0.0675 - val_mae: 0.0675\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1221 - mae: 0.1221 - val_loss: 0.0718 - val_mae: 0.0718\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1215 - mae: 0.1215 - val_loss: 0.0658 - val_mae: 0.0658\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1214 - mae: 0.1214 - val_loss: 0.0674 - val_mae: 0.0674\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1211 - mae: 0.1211 - val_loss: 0.0608 - val_mae: 0.0608\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1220 - mae: 0.1220 - val_loss: 0.0692 - val_mae: 0.0692\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1214 - mae: 0.1214 - val_loss: 0.0637 - val_mae: 0.0637\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1218 - mae: 0.1218 - val_loss: 0.0590 - val_mae: 0.0590\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1211 - mae: 0.1211 - val_loss: 0.0591 - val_mae: 0.0591\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1210 - mae: 0.1210 - val_loss: 0.0582 - val_mae: 0.0582\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1210 - mae: 0.1210 - val_loss: 0.0627 - val_mae: 0.0627\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1216 - mae: 0.1216 - val_loss: 0.0601 - val_mae: 0.0601\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1212 - mae: 0.1212 - val_loss: 0.0588 - val_mae: 0.0588\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_1 = simple_model_antm_1.fit(X_train_antm_1, y_train_antm_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "# time 19.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.0735 - mae: 0.0735 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0091 - mae: 0.009 - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0090 - mae: 0.009 - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0076 - mae: 0.007 - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0080 - val_mae: 0.0080\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_2 = simple_model_antm_2.fit(X_train_antm_2, y_train_antm_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "# time 21.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.1429 - mae: 0.1429 - val_loss: 0.0871 - val_mae: 0.0871\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0357 - mae: 0.0357 - val_loss: 0.0253 - val_mae: 0.0253\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0115 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0115 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0077 - mae: 0.0077 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0091 - val_mae: 0.0091\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_3 = simple_model_antm_3.fit(X_train_antm_3, y_train_antm_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 22.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 9ms/step - loss: 0.0524 - mae: 0.0524 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0117 - mae: 0.0117 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0076 - mae: 0.0076 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0088 - val_mae: 0.0088\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_4 = simple_model_antm_4.fit(X_train_antm_4, y_train_antm_4,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 24.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_antm_1 = simple_model_antm_1.predict(X_test_antm_1)\n",
    "preds_antm_2 = simple_model_antm_2.predict(X_test_antm_2)\n",
    "preds_antm_3 = simple_model_antm_3.predict(X_test_antm_3)\n",
    "preds_antm_4 = simple_model_antm_4.predict(X_test_antm_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps 1\n",
      "mae score antm_1: 0.05351210758470884\n",
      "r2 score antm_1: -0.012847259777391207\n",
      "mape score antm_1: 0.17672596212047678\n",
      "rmse score antm_1: 0.1109104112490807\n",
      "timesteps 2\n",
      "mae score antm_2: 0.008161751530795817\n",
      "r2 score antm_2: 0.9952483822666641\n",
      "mape score antm_2: 0.036884902423474634\n",
      "rmse score antm_2: 0.013446005459325163\n",
      "timesteps 3\n",
      "mae score antm_3: 0.009117184707894537\n",
      "r2 score antm_3: 0.9943208018711837\n",
      "mape score antm_3: 0.0415330433575899\n",
      "rmse score antm_3: 0.014982986270849675\n",
      "timesteps 4\n",
      "mae score antm_4: 0.008764602393161843\n",
      "r2 score antm_4: 0.9943954589232934\n",
      "mape score antm_4: 0.041414121470253945\n",
      "rmse score antm_4: 0.014978998319218338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 1\")\n",
    "print(\"mae score antm_1: \"+str(mean_absolute_error(preds_antm_1, y_test_antm_1)))\n",
    "print(\"r2 score antm_1: \"+str(r2_score(preds_antm_1, y_test_antm_1)))\n",
    "print(\"mape score antm_1: \"+str(mean_absolute_percentage_error(preds_antm_1, y_test_antm_1)))\n",
    "print(\"rmse score antm_1: \"+str(np.sqrt(mean_squared_error(preds_antm_1, y_test_antm_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 2\")\n",
    "print(\"mae score antm_2: \"+str(mean_absolute_error(preds_antm_2, y_test_antm_2)))\n",
    "print(\"r2 score antm_2: \"+str(r2_score(preds_antm_2, y_test_antm_2)))\n",
    "print(\"mape score antm_2: \"+str(mean_absolute_percentage_error(preds_antm_2, y_test_antm_2)))\n",
    "print(\"rmse score antm_2: \"+str(np.sqrt(mean_squared_error(preds_antm_2, y_test_antm_2))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 3\")\n",
    "print(\"mae score antm_3: \"+str(mean_absolute_error(preds_antm_3, y_test_antm_3)))\n",
    "print(\"r2 score antm_3: \"+str(r2_score(preds_antm_3, y_test_antm_3)))\n",
    "print(\"mape score antm_3: \"+str(mean_absolute_percentage_error(preds_antm_3, y_test_antm_3)))\n",
    "print(\"rmse score antm_3: \"+str(np.sqrt(mean_squared_error(preds_antm_3, y_test_antm_3))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 4\")\n",
    "print(\"mae score antm_4: \"+str(mean_absolute_error(preds_antm_4, y_test_antm_4)))\n",
    "print(\"r2 score antm_4: \"+str(r2_score(preds_antm_4, y_test_antm_4)))\n",
    "print(\"mape score antm_4: \"+str(mean_absolute_percentage_error(preds_antm_4, y_test_antm_4)))\n",
    "print(\"rmse score antm_4: \"+str(np.sqrt(mean_squared_error(preds_antm_4, y_test_antm_4))))\n",
    "\n",
    "mae_antm = {'timesteps_1':mean_absolute_error(preds_antm_1, y_test_antm_1),'timesteps_2':mean_absolute_error(preds_antm_2, y_test_antm_2),'timesteps_3':mean_absolute_error(preds_antm_3, y_test_antm_3),'timesteps_4':mean_absolute_error(preds_antm_4, y_test_antm_4)}\n",
    "\n",
    "mape_antm = {'timesteps_1':mean_absolute_percentage_error(preds_antm_1, y_test_antm_1),'timesteps_2':mean_absolute_percentage_error(preds_antm_2, y_test_antm_2),'timesteps_3':mean_absolute_percentage_error(preds_antm_3, y_test_antm_3),'timesteps_4':mean_absolute_percentage_error(preds_antm_4, y_test_antm_4)}\n",
    "\n",
    "rmse_antm = {'timesteps_1':np.sqrt(mean_squared_error(preds_antm_1, y_test_antm_1)),'timesteps_2':np.sqrt(mean_squared_error(preds_antm_2, y_test_antm_2)),'timesteps_3':np.sqrt(mean_squared_error(preds_antm_3, y_test_antm_3)),'timesteps_4':np.sqrt(mean_squared_error(preds_antm_4, y_test_antm_4))}\n",
    "\n",
    "r2_antm = {'timesteps_1':r2_score(preds_antm_1, y_test_antm_1),'timesteps_2':r2_score(preds_antm_2, y_test_antm_2),'timesteps_3':r2_score(preds_antm_3, y_test_antm_3),'timesteps_4':r2_score(preds_antm_4, y_test_antm_4)} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps 1\n",
    "# mae score antm_1: 0.05351210758470884\n",
    "# r2 score antm_1: -0.012847259777391207\n",
    "# mape score antm_1: 0.17672596212047678\n",
    "# rmse score antm_1: 0.1109104112490807\n",
    "# timesteps 2\n",
    "# mae score antm_2: 0.008161751530795817\n",
    "# r2 score antm_2: 0.9952483822666641\n",
    "# mape score antm_2: 0.036884902423474634\n",
    "# rmse score antm_2: 0.013446005459325163\n",
    "# timesteps 3\n",
    "# mae score antm_3: 0.009117184707894537\n",
    "# r2 score antm_3: 0.9943208018711837\n",
    "# mape score antm_3: 0.0415330433575899\n",
    "# rmse score antm_3: 0.014982986270849675\n",
    "# timesteps 4\n",
    "# mae score antm_4: 0.008764602393161843\n",
    "# r2 score antm_4: 0.9943954589232934\n",
    "# mape score antm_4: 0.041414121470253945\n",
    "# rmse score antm_4: 0.014978998319218338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'timesteps_2': 0.008161751530795817, 'timesteps_4': 0.008764602393161843, 'timesteps_3': 0.009117184707894537, 'timesteps_1': 0.05351210758470884}\n",
      "sorted rmse\n",
      "{'timesteps_2': 0.013446005459325163, 'timesteps_4': 0.014978998319218338, 'timesteps_3': 0.014982986270849675, 'timesteps_1': 0.1109104112490807}\n",
      "sorted mape\n",
      "{'timesteps_2': 0.036884902423474634, 'timesteps_4': 0.041414121470253945, 'timesteps_3': 0.0415330433575899, 'timesteps_1': 0.17672596212047678}\n",
      "sorted r2\n",
      "{'timesteps_2': 0.9952483822666641, 'timesteps_4': 0.9943954589232934, 'timesteps_3': 0.9943208018711837, 'timesteps_1': -0.012847259777391207}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_antm_1_sorted = dict(sorted(mae_antm.items(),key=lambda item: item[1]))\n",
    "print(mae_antm_1_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_antm_sorted = dict(sorted(rmse_antm.items(),key=lambda item: item[1]))\n",
    "print(rmse_antm_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_antm_sorted = dict(sorted(mape_antm.items(),key=lambda item: item[1]))\n",
    "print(mape_antm_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_antm_sorted = dict(sorted(r2_antm.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_antm_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted mae\n",
    "# {'timesteps_2': 0.008161751530795817, 'timesteps_4': 0.008764602393161843, 'timesteps_3': 0.009117184707894537, 'timesteps_1': 0.05351210758470884}\n",
    "# sorted rmse\n",
    "# {'timesteps_2': 0.013446005459325163, 'timesteps_4': 0.014978998319218338, 'timesteps_3': 0.014982986270849675, 'timesteps_1': 0.1109104112490807}\n",
    "# sorted mape\n",
    "# {'timesteps_2': 0.036884902423474634, 'timesteps_4': 0.041414121470253945, 'timesteps_3': 0.0415330433575899, 'timesteps_1': 0.17672596212047678}\n",
    "# sorted r2\n",
    "# {'timesteps_2': 0.9952483822666641, 'timesteps_4': 0.9943954589232934, 'timesteps_3': 0.9943208018711837, 'timesteps_1': -0.012847259777391207}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_asii = df_asii.reset_index(drop=True)\n",
    "arr_asii = df_asii.to_numpy()\n",
    "flat_asii = arr_asii.flatten()\n",
    "asii_X_1, asii_y_1 = split_sequence(flat_asii, n_steps_1)\n",
    "asii_X_2, asii_y_2 = split_sequence(flat_asii, n_steps_2)\n",
    "asii_X_3, asii_y_3 = split_sequence(flat_asii, n_steps_3)\n",
    "asii_X_4, asii_y_4 = split_sequence(flat_asii, n_steps_4)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_asii_1, X_test_asii_1, y_train_asii_1, y_test_asii_1 = train_test_split(asii_X_1, asii_y_1, test_size=0.33, random_state=42)\n",
    "X_train_asii_2, X_test_asii_2, y_train_asii_2, y_test_asii_2 = train_test_split(asii_X_2, asii_y_2, test_size=0.33, random_state=42)\n",
    "X_train_asii_3, X_test_asii_3, y_train_asii_3, y_test_asii_3 = train_test_split(asii_X_3, asii_y_3, test_size=0.33, random_state=42)\n",
    "X_train_asii_4, X_test_asii_4, y_train_asii_4, y_test_asii_4 = train_test_split(asii_X_4, asii_y_4, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_asii_1 = X_train_asii_1.reshape(X_train_asii_1.shape[0],X_train_asii_1.shape[1],n_features)\n",
    "X_test_asii_1 = X_test_asii_1.reshape(X_test_asii_1.shape[0],X_test_asii_1.shape[1],n_features)\n",
    "X_train_asii_2 = X_train_asii_2.reshape(X_train_asii_2.shape[0],X_train_asii_2.shape[1],n_features)\n",
    "X_test_asii_2 = X_test_asii_2.reshape(X_test_asii_2.shape[0],X_test_asii_2.shape[1],n_features)\n",
    "X_train_asii_3 = X_train_asii_3.reshape(X_train_asii_3.shape[0],X_train_asii_3.shape[1],n_features)\n",
    "X_test_asii_3 = X_test_asii_3.reshape(X_test_asii_3.shape[0],X_test_asii_3.shape[1],n_features)\n",
    "X_train_asii_4 = X_train_asii_4.reshape(X_train_asii_4.shape[0],X_train_asii_4.shape[1],n_features)\n",
    "X_test_asii_4 = X_test_asii_4.reshape(X_test_asii_4.shape[0],X_test_asii_4.shape[1],n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_asii_1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_asii_2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_asii_3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_asii_4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_4, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 10ms/step - loss: 0.2262 - mae: 0.2262 - val_loss: 0.0950 - val_mae: 0.0950\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0697 - mae: 0.0697 - val_loss: 0.0380 - val_mae: 0.0380\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0270 - mae: 0.0270 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0213 - mae: 0.0213 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0216 - mae: 0.0216 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.0206 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0202 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0203\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0193 - val_mae: 0.0193\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0183 - val_mae: 0.0183\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0151 - val_mae: 0.0151\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_1 = simple_model_asii_1.fit(X_train_asii_1, y_train_asii_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 19.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.1739 - mae: 0.1739 - val_loss: 0.0439 - val_mae: 0.0439\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0202 - mae: 0.0202 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0203 - val_mae: 0.0203\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0186 - mae: 0.018 - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0210 - val_mae: 0.0210\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0169 - mae: 0.016 - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0193 - val_mae: 0.0193\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0205 - val_mae: 0.0205\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0152 - val_mae: 0.0152\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_2 = simple_model_asii_2.fit(X_train_asii_2, y_train_asii_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 21.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 10ms/step - loss: 0.1505 - mae: 0.1505 - val_loss: 0.0292 - val_mae: 0.0292\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0201 - mae: 0.0201 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0192 - mae: 0.0192 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0268 - val_mae: 0.0268\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0258 - val_mae: 0.0258\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0197 - val_mae: 0.0197\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0197 - val_mae: 0.0197\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0194 - mae: 0.0194 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0258 - val_mae: 0.0258\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0205 - val_mae: 0.0205\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0203 - val_mae: 0.0203\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0197 - val_mae: 0.0197\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0146 - val_mae: 0.0146\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_3 = simple_model_asii_3.fit(X_train_asii_3, y_train_asii_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 23.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 12ms/step - loss: 0.1645 - mae: 0.1645 - val_loss: 0.0434 - val_mae: 0.0434\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0281 - mae: 0.0281 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0256 - val_mae: 0.0256\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0192 - mae: 0.0192 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0195 - mae: 0.0195 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0214 - mae: 0.0214 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0214 - mae: 0.0214 - val_loss: 0.0260 - val_mae: 0.0260\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0195 - mae: 0.0195 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0201 - mae: 0.0201 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0197 - mae: 0.0197 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0201 - mae: 0.0201 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0183 - val_mae: 0.0183\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0210 - val_mae: 0.0210\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0152 - val_mae: 0.0152\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_4 = simple_model_asii_4.fit(X_train_asii_4, y_train_asii_4,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 24.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_asii_1 = simple_model_asii_1.predict(X_test_asii_1)\n",
    "preds_asii_2 = simple_model_asii_2.predict(X_test_asii_2)\n",
    "preds_asii_3 = simple_model_asii_3.predict(X_test_asii_3)\n",
    "preds_asii_4 = simple_model_asii_4.predict(X_test_asii_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps 1\n",
      "mae score asii_1: 0.015697579842078434\n",
      "r2 score asii_1: 0.9891798496508\n",
      "mape score asii_1: 0.03516537230185299\n",
      "rmse score asii_1: 0.02077293181651314\n",
      "timesteps 2\n",
      "mae score asii_2: 0.015313086808529379\n",
      "r2 score asii_2: 0.9900058330429263\n",
      "mape score asii_2: 0.05661475089706797\n",
      "rmse score asii_2: 0.020241523730058424\n",
      "timesteps 3\n",
      "mae score asii_3: 0.01468973459169929\n",
      "r2 score asii_3: 0.9901866526074022\n",
      "mape score asii_3: 0.0329870319926044\n",
      "rmse score asii_3: 0.019597623902703688\n",
      "timesteps 4\n",
      "mae score asii_4: 0.015275431236995839\n",
      "r2 score asii_4: 0.9896423031917067\n",
      "mape score asii_4: 0.04428520573595462\n",
      "rmse score asii_4: 0.020267971711778642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 1\")\n",
    "print(\"mae score asii_1: \"+str(mean_absolute_error(preds_asii_1, y_test_asii_1)))\n",
    "print(\"r2 score asii_1: \"+str(r2_score(preds_asii_1, y_test_asii_1)))\n",
    "print(\"mape score asii_1: \"+str(mean_absolute_percentage_error(preds_asii_1, y_test_asii_1)))\n",
    "print(\"rmse score asii_1: \"+str(np.sqrt(mean_squared_error(preds_asii_1, y_test_asii_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 2\")\n",
    "print(\"mae score asii_2: \"+str(mean_absolute_error(preds_asii_2, y_test_asii_2)))\n",
    "print(\"r2 score asii_2: \"+str(r2_score(preds_asii_2, y_test_asii_2)))\n",
    "print(\"mape score asii_2: \"+str(mean_absolute_percentage_error(preds_asii_2, y_test_asii_2)))\n",
    "print(\"rmse score asii_2: \"+str(np.sqrt(mean_squared_error(preds_asii_2, y_test_asii_2))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 3\")\n",
    "print(\"mae score asii_3: \"+str(mean_absolute_error(preds_asii_3, y_test_asii_3)))\n",
    "print(\"r2 score asii_3: \"+str(r2_score(preds_asii_3, y_test_asii_3)))\n",
    "print(\"mape score asii_3: \"+str(mean_absolute_percentage_error(preds_asii_3, y_test_asii_3)))\n",
    "print(\"rmse score asii_3: \"+str(np.sqrt(mean_squared_error(preds_asii_3, y_test_asii_3))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 4\")\n",
    "print(\"mae score asii_4: \"+str(mean_absolute_error(preds_asii_4, y_test_asii_4)))\n",
    "print(\"r2 score asii_4: \"+str(r2_score(preds_asii_4, y_test_asii_4)))\n",
    "print(\"mape score asii_4: \"+str(mean_absolute_percentage_error(preds_asii_4, y_test_asii_4)))\n",
    "print(\"rmse score asii_4: \"+str(np.sqrt(mean_squared_error(preds_asii_4, y_test_asii_4))))\n",
    "\n",
    "mae_asii = {'timesteps_1':mean_absolute_error(preds_asii_1, y_test_asii_1),'timesteps_2':mean_absolute_error(preds_asii_2, y_test_asii_2),'timesteps_3':mean_absolute_error(preds_asii_3, y_test_asii_3),'timesteps_4':mean_absolute_error(preds_asii_4, y_test_asii_4)}\n",
    "\n",
    "mape_asii = {'timesteps_1':mean_absolute_percentage_error(preds_asii_1, y_test_asii_1),'timesteps_2':mean_absolute_percentage_error(preds_asii_2, y_test_asii_2),'timesteps_3':mean_absolute_percentage_error(preds_asii_3, y_test_asii_3),'timesteps_4':mean_absolute_percentage_error(preds_asii_4, y_test_asii_4)}\n",
    "\n",
    "rmse_asii = {'timesteps_1':np.sqrt(mean_squared_error(preds_asii_1, y_test_asii_1)),'timesteps_2':np.sqrt(mean_squared_error(preds_asii_2, y_test_asii_2)),'timesteps_3':np.sqrt(mean_squared_error(preds_asii_3, y_test_asii_3)),'timesteps_4':np.sqrt(mean_squared_error(preds_asii_4, y_test_asii_4))}\n",
    "\n",
    "r2_asii = {'timesteps_1':r2_score(preds_asii_1, y_test_asii_1),'timesteps_2':r2_score(preds_asii_2, y_test_asii_2),'timesteps_3':r2_score(preds_asii_3, y_test_asii_3),'timesteps_4':r2_score(preds_asii_4, y_test_asii_4)} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps 1\n",
    "# mae score asii_1: 0.015697579842078434\n",
    "# r2 score asii_1: 0.9891798496508\n",
    "# mape score asii_1: 0.03516537230185299\n",
    "# rmse score asii_1: 0.02077293181651314\n",
    "# timesteps 2\n",
    "# mae score asii_2: 0.015313086808529379\n",
    "# r2 score asii_2: 0.9900058330429263\n",
    "# mape score asii_2: 0.05661475089706797\n",
    "# rmse score asii_2: 0.020241523730058424\n",
    "# timesteps 3\n",
    "# mae score asii_3: 0.01468973459169929\n",
    "# r2 score asii_3: 0.9901866526074022\n",
    "# mape score asii_3: 0.0329870319926044\n",
    "# rmse score asii_3: 0.019597623902703688\n",
    "# timesteps 4\n",
    "# mae score asii_4: 0.015275431236995839\n",
    "# r2 score asii_4: 0.9896423031917067\n",
    "# mape score asii_4: 0.04428520573595462\n",
    "# rmse score asii_4: 0.020267971711778642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'timesteps_3': 0.01468973459169929, 'timesteps_4': 0.015275431236995839, 'timesteps_2': 0.015313086808529379, 'timesteps_1': 0.015697579842078434}\n",
      "sorted rmse\n",
      "{'timesteps_3': 0.019597623902703688, 'timesteps_2': 0.020241523730058424, 'timesteps_4': 0.020267971711778642, 'timesteps_1': 0.02077293181651314}\n",
      "sorted mape\n",
      "{'timesteps_3': 0.0329870319926044, 'timesteps_1': 0.03516537230185299, 'timesteps_4': 0.04428520573595462, 'timesteps_2': 0.05661475089706797}\n",
      "sorted r2\n",
      "{'timesteps_3': 0.9901866526074022, 'timesteps_2': 0.9900058330429263, 'timesteps_4': 0.9896423031917067, 'timesteps_1': 0.9891798496508}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_asii_1_sorted = dict(sorted(mae_asii.items(),key=lambda item: item[1]))\n",
    "print(mae_asii_1_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_asii_sorted = dict(sorted(rmse_asii.items(),key=lambda item: item[1]))\n",
    "print(rmse_asii_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_asii_sorted = dict(sorted(mape_asii.items(),key=lambda item: item[1]))\n",
    "print(mape_asii_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_asii_sorted = dict(sorted(r2_asii.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_asii_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted mae\n",
    "# {'timesteps_3': 0.01468973459169929, 'timesteps_4': 0.015275431236995839, 'timesteps_2': 0.015313086808529379, 'timesteps_1': 0.015697579842078434}\n",
    "# sorted rmse\n",
    "# {'timesteps_3': 0.019597623902703688, 'timesteps_2': 0.020241523730058424, 'timesteps_4': 0.020267971711778642, 'timesteps_1': 0.02077293181651314}\n",
    "# sorted mape\n",
    "# {'timesteps_3': 0.0329870319926044, 'timesteps_1': 0.03516537230185299, 'timesteps_4': 0.04428520573595462, 'timesteps_2': 0.05661475089706797}\n",
    "# sorted r2\n",
    "# {'timesteps_3': 0.9901866526074022, 'timesteps_2': 0.9900058330429263, 'timesteps_4': 0.9896423031917067, 'timesteps_1': 0.9891798496508}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icbp = df_icbp.reset_index(drop=True)\n",
    "arr_icbp = df_icbp.to_numpy()\n",
    "flat_icbp = arr_icbp.flatten()\n",
    "icbp_X_1, icbp_y_1 = split_sequence(flat_icbp, n_steps_1)\n",
    "icbp_X_2, icbp_y_2 = split_sequence(flat_icbp, n_steps_2)\n",
    "icbp_X_3, icbp_y_3 = split_sequence(flat_icbp, n_steps_3)\n",
    "icbp_X_4, icbp_y_4 = split_sequence(flat_icbp, n_steps_4)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_icbp_1, X_test_icbp_1, y_train_icbp_1, y_test_icbp_1 = train_test_split(icbp_X_1, icbp_y_1, test_size=0.33, random_state=42)\n",
    "X_train_icbp_2, X_test_icbp_2, y_train_icbp_2, y_test_icbp_2 = train_test_split(icbp_X_2, icbp_y_2, test_size=0.33, random_state=42)\n",
    "X_train_icbp_3, X_test_icbp_3, y_train_icbp_3, y_test_icbp_3 = train_test_split(icbp_X_3, icbp_y_3, test_size=0.33, random_state=42)\n",
    "X_train_icbp_4, X_test_icbp_4, y_train_icbp_4, y_test_icbp_4 = train_test_split(icbp_X_4, icbp_y_4, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_icbp_1 = X_train_icbp_1.reshape(X_train_icbp_1.shape[0],X_train_icbp_1.shape[1],n_features)\n",
    "X_test_icbp_1 = X_test_icbp_1.reshape(X_test_icbp_1.shape[0],X_test_icbp_1.shape[1],n_features)\n",
    "X_train_icbp_2 = X_train_icbp_2.reshape(X_train_icbp_2.shape[0],X_train_icbp_2.shape[1],n_features)\n",
    "X_test_icbp_2 = X_test_icbp_2.reshape(X_test_icbp_2.shape[0],X_test_icbp_2.shape[1],n_features)\n",
    "X_train_icbp_3 = X_train_icbp_3.reshape(X_train_icbp_3.shape[0],X_train_icbp_3.shape[1],n_features)\n",
    "X_test_icbp_3 = X_test_icbp_3.reshape(X_test_icbp_3.shape[0],X_test_icbp_3.shape[1],n_features)\n",
    "X_train_icbp_4 = X_train_icbp_4.reshape(X_train_icbp_4.shape[0],X_train_icbp_4.shape[1],n_features)\n",
    "X_test_icbp_4 = X_test_icbp_4.reshape(X_test_icbp_4.shape[0],X_test_icbp_4.shape[1],n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_icbp_1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_icbp_2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_icbp_3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_icbp_4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_4, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 3s 10ms/step - loss: 0.1810 - mae: 0.1810 - val_loss: 0.0728 - val_mae: 0.0728\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0328 - mae: 0.0328 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0201 - mae: 0.0201 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0092 - mae: 0.009 - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0077 - mae: 0.0077 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0079 - mae: 0.007 - 0s 4ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0077 - mae: 0.0077 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0074 - val_mae: 0.0074\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_1 = simple_model_icbp_1.fit(X_train_icbp_1, y_train_icbp_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 19.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.2130 - mae: 0.2130 - val_loss: 0.0298 - val_mae: 0.0298\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0119 - mae: 0.0119 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0092 - mae: 0.009 - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0070 - val_mae: 0.0070\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0070 - val_mae: 0.0070\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0084 - val_mae: 0.0084\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_2 = simple_model_icbp_2.fit(X_train_icbp_2, y_train_icbp_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 19.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.1949 - mae: 0.1949 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0089 - mae: 0.008 - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - mae: 0.0119 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0071 - val_mae: 0.0071\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0086 - mae: 0.008 - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0070 - val_mae: 0.0070\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0094 - val_mae: 0.0094\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_3 = simple_model_icbp_3.fit(X_train_icbp_3, y_train_icbp_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 23.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 10ms/step - loss: 0.3389 - mae: 0.3389 - val_loss: 0.2400 - val_mae: 0.2400\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2422 - mae: 0.2422 - val_loss: 0.2267 - val_mae: 0.2267\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2394 - mae: 0.2394 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2246 - val_mae: 0.2246\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2262 - val_mae: 0.2262\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2401 - mae: 0.2401 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2394 - mae: 0.2394 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2244 - val_mae: 0.2244\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2247 - val_mae: 0.2247\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2247 - val_mae: 0.2247\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2247 - val_mae: 0.2247\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2401 - mae: 0.2401 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2259 - val_mae: 0.2259\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2262 - val_mae: 0.2262\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2402 - mae: 0.2402 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2408 - mae: 0.2408 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2404 - mae: 0.2404 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2401 - mae: 0.2401 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2246 - val_mae: 0.2246\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2401 - mae: 0.2401 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2404 - mae: 0.2404 - val_loss: 0.2249 - val_mae: 0.2249\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2404 - mae: 0.2404 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2403 - mae: 0.2403 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2409 - mae: 0.2409 - val_loss: 0.2247 - val_mae: 0.2247\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2401 - mae: 0.2401 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2249 - val_mae: 0.2249\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2248 - val_mae: 0.2248\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2248 - val_mae: 0.2248\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2262 - val_mae: 0.2262\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2248 - val_mae: 0.2248\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2248 - val_mae: 0.2248\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2249 - val_mae: 0.2249\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2262 - val_mae: 0.2262\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2404 - mae: 0.2404 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2259 - val_mae: 0.2259\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2400 - mae: 0.2400 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2253 - val_mae: 0.2253\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_4 = simple_model_icbp_4.fit(X_train_icbp_4, y_train_icbp_4,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 24.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_icbp_1 = simple_model_icbp_1.predict(X_test_icbp_1)\n",
    "preds_icbp_2 = simple_model_icbp_2.predict(X_test_icbp_2)\n",
    "preds_icbp_3 = simple_model_icbp_3.predict(X_test_icbp_3)\n",
    "preds_icbp_4 = simple_model_icbp_4.predict(X_test_icbp_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps 1\n",
      "mae score icbp_1: 0.0071951216005588505\n",
      "r2 score icbp_1: 0.9982912893287746\n",
      "mape score icbp_1: 0.02911766342732286\n",
      "rmse score icbp_1: 0.0115991115450196\n",
      "timesteps 2\n",
      "mae score icbp_2: 0.00874996393974662\n",
      "r2 score icbp_2: 0.998219167116058\n",
      "mape score icbp_2: 0.10803360291602122\n",
      "rmse score icbp_2: 0.011925892437280167\n",
      "timesteps 3\n",
      "mae score icbp_3: 0.009715755021447626\n",
      "r2 score icbp_3: 0.9975988977109971\n",
      "mape score icbp_3: 0.03398883492185345\n",
      "rmse score icbp_3: 0.013880861676693924\n",
      "timesteps 4\n",
      "mae score icbp_4: 0.24760012053707564\n",
      "r2 score icbp_4: 0.0\n",
      "mape score icbp_4: 0.46217821589569885\n",
      "rmse score icbp_4: 0.2913636094589231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 1\")\n",
    "print(\"mae score icbp_1: \"+str(mean_absolute_error(preds_icbp_1, y_test_icbp_1)))\n",
    "print(\"r2 score icbp_1: \"+str(r2_score(preds_icbp_1, y_test_icbp_1)))\n",
    "print(\"mape score icbp_1: \"+str(mean_absolute_percentage_error(preds_icbp_1, y_test_icbp_1)))\n",
    "print(\"rmse score icbp_1: \"+str(np.sqrt(mean_squared_error(preds_icbp_1, y_test_icbp_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 2\")\n",
    "print(\"mae score icbp_2: \"+str(mean_absolute_error(preds_icbp_2, y_test_icbp_2)))\n",
    "print(\"r2 score icbp_2: \"+str(r2_score(preds_icbp_2, y_test_icbp_2)))\n",
    "print(\"mape score icbp_2: \"+str(mean_absolute_percentage_error(preds_icbp_2, y_test_icbp_2)))\n",
    "print(\"rmse score icbp_2: \"+str(np.sqrt(mean_squared_error(preds_icbp_2, y_test_icbp_2))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 3\")\n",
    "print(\"mae score icbp_3: \"+str(mean_absolute_error(preds_icbp_3, y_test_icbp_3)))\n",
    "print(\"r2 score icbp_3: \"+str(r2_score(preds_icbp_3, y_test_icbp_3)))\n",
    "print(\"mape score icbp_3: \"+str(mean_absolute_percentage_error(preds_icbp_3, y_test_icbp_3)))\n",
    "print(\"rmse score icbp_3: \"+str(np.sqrt(mean_squared_error(preds_icbp_3, y_test_icbp_3))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 4\")\n",
    "print(\"mae score icbp_4: \"+str(mean_absolute_error(preds_icbp_4, y_test_icbp_4)))\n",
    "print(\"r2 score icbp_4: \"+str(r2_score(preds_icbp_4, y_test_icbp_4)))\n",
    "print(\"mape score icbp_4: \"+str(mean_absolute_percentage_error(preds_icbp_4, y_test_icbp_4)))\n",
    "print(\"rmse score icbp_4: \"+str(np.sqrt(mean_squared_error(preds_icbp_4, y_test_icbp_4))))\n",
    "\n",
    "mae_icbp = {'timesteps_1':mean_absolute_error(preds_icbp_1, y_test_icbp_1),'timesteps_2':mean_absolute_error(preds_icbp_2, y_test_icbp_2),'timesteps_3':mean_absolute_error(preds_icbp_3, y_test_icbp_3),'timesteps_4':mean_absolute_error(preds_icbp_4, y_test_icbp_4)}\n",
    "\n",
    "mape_icbp = {'timesteps_1':mean_absolute_percentage_error(preds_icbp_1, y_test_icbp_1),'timesteps_2':mean_absolute_percentage_error(preds_icbp_2, y_test_icbp_2),'timesteps_3':mean_absolute_percentage_error(preds_icbp_3, y_test_icbp_3),'timesteps_4':mean_absolute_percentage_error(preds_icbp_4, y_test_icbp_4)}\n",
    "\n",
    "rmse_icbp = {'timesteps_1':np.sqrt(mean_squared_error(preds_icbp_1, y_test_icbp_1)),'timesteps_2':np.sqrt(mean_squared_error(preds_icbp_2, y_test_icbp_2)),'timesteps_3':np.sqrt(mean_squared_error(preds_icbp_3, y_test_icbp_3)),'timesteps_4':np.sqrt(mean_squared_error(preds_icbp_4, y_test_icbp_4))}\n",
    "\n",
    "r2_icbp = {'timesteps_1':r2_score(preds_icbp_1, y_test_icbp_1),'timesteps_2':r2_score(preds_icbp_2, y_test_icbp_2),'timesteps_3':r2_score(preds_icbp_3, y_test_icbp_3),'timesteps_4':r2_score(preds_icbp_4, y_test_icbp_4)} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps 1\n",
    "# mae score icbp_1: 0.0071951216005588505\n",
    "# r2 score icbp_1: 0.9982912893287746\n",
    "# mape score icbp_1: 0.02911766342732286\n",
    "# rmse score icbp_1: 0.0115991115450196\n",
    "# timesteps 2\n",
    "# mae score icbp_2: 0.00874996393974662\n",
    "# r2 score icbp_2: 0.998219167116058\n",
    "# mape score icbp_2: 0.10803360291602122\n",
    "# rmse score icbp_2: 0.011925892437280167\n",
    "# timesteps 3\n",
    "# mae score icbp_3: 0.009715755021447626\n",
    "# r2 score icbp_3: 0.9975988977109971\n",
    "# mape score icbp_3: 0.03398883492185345\n",
    "# rmse score icbp_3: 0.013880861676693924\n",
    "# timesteps 4\n",
    "# mae score icbp_4: 0.24760012053707564\n",
    "# r2 score icbp_4: 0.0\n",
    "# mape score icbp_4: 0.46217821589569885\n",
    "# rmse score icbp_4: 0.2913636094589231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'timesteps_1': 0.0071951216005588505, 'timesteps_2': 0.00874996393974662, 'timesteps_3': 0.009715755021447626, 'timesteps_4': 0.24760012053707564}\n",
      "sorted rmse\n",
      "{'timesteps_1': 0.0115991115450196, 'timesteps_2': 0.011925892437280167, 'timesteps_3': 0.013880861676693924, 'timesteps_4': 0.2913636094589231}\n",
      "sorted mape\n",
      "{'timesteps_1': 0.02911766342732286, 'timesteps_3': 0.03398883492185345, 'timesteps_2': 0.10803360291602122, 'timesteps_4': 0.46217821589569885}\n",
      "sorted r2\n",
      "{'timesteps_1': 0.9982912893287746, 'timesteps_2': 0.998219167116058, 'timesteps_3': 0.9975988977109971, 'timesteps_4': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_icbp_1_sorted = dict(sorted(mae_icbp.items(),key=lambda item: item[1]))\n",
    "print(mae_icbp_1_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_icbp_sorted = dict(sorted(rmse_icbp.items(),key=lambda item: item[1]))\n",
    "print(rmse_icbp_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_icbp_sorted = dict(sorted(mape_icbp.items(),key=lambda item: item[1]))\n",
    "print(mape_icbp_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_icbp_sorted = dict(sorted(r2_icbp.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_icbp_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted mae\n",
    "# {'timesteps_1': 0.0071951216005588505, 'timesteps_2': 0.00874996393974662, 'timesteps_3': 0.009715755021447626, 'timesteps_4': 0.24760012053707564}\n",
    "# sorted rmse\n",
    "# {'timesteps_1': 0.0115991115450196, 'timesteps_2': 0.011925892437280167, 'timesteps_3': 0.013880861676693924, 'timesteps_4': 0.2913636094589231}\n",
    "# sorted mape\n",
    "# {'timesteps_1': 0.02911766342732286, 'timesteps_3': 0.03398883492185345, 'timesteps_2': 0.10803360291602122, 'timesteps_4': 0.46217821589569885}\n",
    "# sorted r2\n",
    "# {'timesteps_1': 0.9982912893287746, 'timesteps_2': 0.998219167116058, 'timesteps_3': 0.9975988977109971, 'timesteps_4': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jsmr = df_jsmr.reset_index(drop=True)\n",
    "arr_jsmr = df_jsmr.to_numpy()\n",
    "flat_jsmr = arr_jsmr.flatten()\n",
    "jsmr_X_1, jsmr_y_1 = split_sequence(flat_jsmr, n_steps_1)\n",
    "jsmr_X_2, jsmr_y_2 = split_sequence(flat_jsmr, n_steps_2)\n",
    "jsmr_X_3, jsmr_y_3 = split_sequence(flat_jsmr, n_steps_3)\n",
    "jsmr_X_4, jsmr_y_4 = split_sequence(flat_jsmr, n_steps_4)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_jsmr_1, X_test_jsmr_1, y_train_jsmr_1, y_test_jsmr_1 = train_test_split(jsmr_X_1, jsmr_y_1, test_size=0.33, random_state=42)\n",
    "X_train_jsmr_2, X_test_jsmr_2, y_train_jsmr_2, y_test_jsmr_2 = train_test_split(jsmr_X_2, jsmr_y_2, test_size=0.33, random_state=42)\n",
    "X_train_jsmr_3, X_test_jsmr_3, y_train_jsmr_3, y_test_jsmr_3 = train_test_split(jsmr_X_3, jsmr_y_3, test_size=0.33, random_state=42)\n",
    "X_train_jsmr_4, X_test_jsmr_4, y_train_jsmr_4, y_test_jsmr_4 = train_test_split(jsmr_X_4, jsmr_y_4, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_jsmr_1 = X_train_jsmr_1.reshape(X_train_jsmr_1.shape[0],X_train_jsmr_1.shape[1],n_features)\n",
    "X_test_jsmr_1 = X_test_jsmr_1.reshape(X_test_jsmr_1.shape[0],X_test_jsmr_1.shape[1],n_features)\n",
    "X_train_jsmr_2 = X_train_jsmr_2.reshape(X_train_jsmr_2.shape[0],X_train_jsmr_2.shape[1],n_features)\n",
    "X_test_jsmr_2 = X_test_jsmr_2.reshape(X_test_jsmr_2.shape[0],X_test_jsmr_2.shape[1],n_features)\n",
    "X_train_jsmr_3 = X_train_jsmr_3.reshape(X_train_jsmr_3.shape[0],X_train_jsmr_3.shape[1],n_features)\n",
    "X_test_jsmr_3 = X_test_jsmr_3.reshape(X_test_jsmr_3.shape[0],X_test_jsmr_3.shape[1],n_features)\n",
    "X_train_jsmr_4 = X_train_jsmr_4.reshape(X_train_jsmr_4.shape[0],X_train_jsmr_4.shape[1],n_features)\n",
    "X_test_jsmr_4 = X_test_jsmr_4.reshape(X_test_jsmr_4.shape[0],X_test_jsmr_4.shape[1],n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_jsmr_1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_jsmr_2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_jsmr_3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_jsmr_4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_4, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "59/59 [==============================] - 2s 10ms/step - loss: 0.2222 - mae: 0.2222 - val_loss: 0.0803 - val_mae: 0.0803\n",
      "Epoch 2/100\n",
      "59/59 [==============================] - 0s 4ms/step - loss: 0.0613 - mae: 0.0613 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 3/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 4/100\n",
      "59/59 [==============================] - 0s 5ms/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 5/100\n",
      "59/59 [==============================] - 0s 4ms/step - loss: 0.0211 - mae: 0.0211 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 6/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0211 - mae: 0.0211 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 7/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0205 - mae: 0.0205 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 8/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 9/100\n",
      "59/59 [==============================] - 0s 4ms/step - loss: 0.0201 - mae: 0.0201 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 10/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0194 - mae: 0.0194 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 11/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 12/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 13/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 14/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 15/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 16/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 17/100\n",
      "59/59 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 18/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 19/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 20/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 21/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 22/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 23/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 24/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 25/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 26/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 27/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 28/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 29/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 30/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 31/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 32/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 33/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 34/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 35/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 36/100\n",
      "59/59 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 37/100\n",
      "59/59 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 38/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 39/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 40/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 41/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 42/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 43/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 44/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 45/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 46/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 47/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 48/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 49/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 50/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 51/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 52/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 53/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 54/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 55/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 56/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 57/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 58/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 59/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 60/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 61/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 62/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 63/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 64/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 65/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 66/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 67/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 68/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 69/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 70/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 71/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 72/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 73/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 74/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 75/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 76/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 77/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 78/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 79/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 80/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 81/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 82/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 83/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 84/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 85/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 86/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 87/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 88/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 89/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 90/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 91/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 92/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 93/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 94/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 95/100\n",
      "59/59 [==============================] - 0s 4ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 96/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 97/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 98/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 99/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 100/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0132 - val_mae: 0.0132\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_1 = simple_model_jsmr_1.fit(X_train_jsmr_1, y_train_jsmr_1,\n",
    "          validation_split=0.1,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 20.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 7ms/step - loss: 0.1468 - mae: 0.1468 - val_loss: 0.0530 - val_mae: 0.0530\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0210 - mae: 0.0210 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0213 - mae: 0.0213 - val_loss: 0.0193 - val_mae: 0.0193\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0191 - mae: 0.0191 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0261 - val_mae: 0.0261\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0193 - val_mae: 0.0193\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0150 - val_mae: 0.0150\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_2 = simple_model_jsmr_2.fit(X_train_jsmr_2, y_train_jsmr_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 18.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.1130 - mae: 0.1130 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0212 - mae: 0.0212 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0199 - val_mae: 0.0199\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0199 - val_mae: 0.0199\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0209 - val_mae: 0.0209\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0122 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0131 - val_mae: 0.0131\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_3 = simple_model_jsmr_3.fit(X_train_jsmr_3, y_train_jsmr_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 20.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.2701 - mae: 0.2701 - val_loss: 0.1021 - val_mae: 0.1021\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0725 - mae: 0.0725 - val_loss: 0.0320 - val_mae: 0.0320\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0320 - mae: 0.0320 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0271 - val_mae: 0.0271\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0276 - mae: 0.0276 - val_loss: 0.0253 - val_mae: 0.0253\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0284 - val_mae: 0.0284\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0216 - mae: 0.0216 - val_loss: 0.0203 - val_mae: 0.0203\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0216 - mae: 0.0216 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0197 - mae: 0.0197 - val_loss: 0.0265 - val_mae: 0.0265\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0214 - val_mae: 0.0214\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0270 - val_mae: 0.0270\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0205 - val_mae: 0.0205\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0192 - val_mae: 0.0192\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0137 - mae: 0.013 - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0136 - val_mae: 0.0136\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_4 = simple_model_jsmr_4.fit(X_train_jsmr_4, y_train_jsmr_4,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 21.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_jsmr_1 = simple_model_jsmr_1.predict(X_test_jsmr_1)\n",
    "preds_jsmr_2 = simple_model_jsmr_2.predict(X_test_jsmr_2)\n",
    "preds_jsmr_3 = simple_model_jsmr_3.predict(X_test_jsmr_3)\n",
    "preds_jsmr_4 = simple_model_jsmr_4.predict(X_test_jsmr_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps 1\n",
      "mae score jsmr_1: 0.012709409360352766\n",
      "r2 score jsmr_1: 0.9935702374781473\n",
      "mape score jsmr_1: 0.030344619679000352\n",
      "rmse score jsmr_1: 0.017236430432879077\n",
      "timesteps 2\n",
      "mae score jsmr_2: 0.014326228000078446\n",
      "r2 score jsmr_2: 0.9923649299409023\n",
      "mape score jsmr_2: 0.03485906558726514\n",
      "rmse score jsmr_2: 0.018798476996323747\n",
      "timesteps 3\n",
      "mae score jsmr_3: 0.013212442520124074\n",
      "r2 score jsmr_3: 0.9929662622837995\n",
      "mape score jsmr_3: 0.04070394123414279\n",
      "rmse score jsmr_3: 0.017914216506633596\n",
      "timesteps 4\n",
      "mae score jsmr_4: 0.012534012823066423\n",
      "r2 score jsmr_4: 0.9933026921297792\n",
      "mape score jsmr_4: 0.029484858211290245\n",
      "rmse score jsmr_4: 0.01756805243763662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 1\")\n",
    "print(\"mae score jsmr_1: \"+str(mean_absolute_error(preds_jsmr_1, y_test_jsmr_1)))\n",
    "print(\"r2 score jsmr_1: \"+str(r2_score(preds_jsmr_1, y_test_jsmr_1)))\n",
    "print(\"mape score jsmr_1: \"+str(mean_absolute_percentage_error(preds_jsmr_1, y_test_jsmr_1)))\n",
    "print(\"rmse score jsmr_1: \"+str(np.sqrt(mean_squared_error(preds_jsmr_1, y_test_jsmr_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 2\")\n",
    "print(\"mae score jsmr_2: \"+str(mean_absolute_error(preds_jsmr_2, y_test_jsmr_2)))\n",
    "print(\"r2 score jsmr_2: \"+str(r2_score(preds_jsmr_2, y_test_jsmr_2)))\n",
    "print(\"mape score jsmr_2: \"+str(mean_absolute_percentage_error(preds_jsmr_2, y_test_jsmr_2)))\n",
    "print(\"rmse score jsmr_2: \"+str(np.sqrt(mean_squared_error(preds_jsmr_2, y_test_jsmr_2))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 3\")\n",
    "print(\"mae score jsmr_3: \"+str(mean_absolute_error(preds_jsmr_3, y_test_jsmr_3)))\n",
    "print(\"r2 score jsmr_3: \"+str(r2_score(preds_jsmr_3, y_test_jsmr_3)))\n",
    "print(\"mape score jsmr_3: \"+str(mean_absolute_percentage_error(preds_jsmr_3, y_test_jsmr_3)))\n",
    "print(\"rmse score jsmr_3: \"+str(np.sqrt(mean_squared_error(preds_jsmr_3, y_test_jsmr_3))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"timesteps 4\")\n",
    "print(\"mae score jsmr_4: \"+str(mean_absolute_error(preds_jsmr_4, y_test_jsmr_4)))\n",
    "print(\"r2 score jsmr_4: \"+str(r2_score(preds_jsmr_4, y_test_jsmr_4)))\n",
    "print(\"mape score jsmr_4: \"+str(mean_absolute_percentage_error(preds_jsmr_4, y_test_jsmr_4)))\n",
    "print(\"rmse score jsmr_4: \"+str(np.sqrt(mean_squared_error(preds_jsmr_4, y_test_jsmr_4))))\n",
    "\n",
    "mae_jsmr = {'timesteps_1':mean_absolute_error(preds_jsmr_1, y_test_jsmr_1),'timesteps_2':mean_absolute_error(preds_jsmr_2, y_test_jsmr_2),'timesteps_3':mean_absolute_error(preds_jsmr_3, y_test_jsmr_3),'timesteps_4':mean_absolute_error(preds_jsmr_4, y_test_jsmr_4)}\n",
    "\n",
    "mape_jsmr = {'timesteps_1':mean_absolute_percentage_error(preds_jsmr_1, y_test_jsmr_1),'timesteps_2':mean_absolute_percentage_error(preds_jsmr_2, y_test_jsmr_2),'timesteps_3':mean_absolute_percentage_error(preds_jsmr_3, y_test_jsmr_3),'timesteps_4':mean_absolute_percentage_error(preds_jsmr_4, y_test_jsmr_4)}\n",
    "\n",
    "rmse_jsmr = {'timesteps_1':np.sqrt(mean_squared_error(preds_jsmr_1, y_test_jsmr_1)),'timesteps_2':np.sqrt(mean_squared_error(preds_jsmr_2, y_test_jsmr_2)),'timesteps_3':np.sqrt(mean_squared_error(preds_jsmr_3, y_test_jsmr_3)),'timesteps_4':np.sqrt(mean_squared_error(preds_jsmr_4, y_test_jsmr_4))}\n",
    "\n",
    "r2_jsmr = {'timesteps_1':r2_score(preds_jsmr_1, y_test_jsmr_1),'timesteps_2':r2_score(preds_jsmr_2, y_test_jsmr_2),'timesteps_3':r2_score(preds_jsmr_3, y_test_jsmr_3),'timesteps_4':r2_score(preds_jsmr_4, y_test_jsmr_4)} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps 1\n",
    "# mae score jsmr_1: 0.012709409360352766\n",
    "# r2 score jsmr_1: 0.9935702374781473\n",
    "# mape score jsmr_1: 0.030344619679000352\n",
    "# rmse score jsmr_1: 0.017236430432879077\n",
    "# timesteps 2\n",
    "# mae score jsmr_2: 0.014326228000078446\n",
    "# r2 score jsmr_2: 0.9923649299409023\n",
    "# mape score jsmr_2: 0.03485906558726514\n",
    "# rmse score jsmr_2: 0.018798476996323747\n",
    "# timesteps 3\n",
    "# mae score jsmr_3: 0.013212442520124074\n",
    "# r2 score jsmr_3: 0.9929662622837995\n",
    "# mape score jsmr_3: 0.04070394123414279\n",
    "# rmse score jsmr_3: 0.017914216506633596\n",
    "# timesteps 4\n",
    "# mae score jsmr_4: 0.012534012823066423\n",
    "# r2 score jsmr_4: 0.9933026921297792\n",
    "# mape score jsmr_4: 0.029484858211290245\n",
    "# rmse score jsmr_4: 0.01756805243763662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'timesteps_4': 0.012534012823066423, 'timesteps_1': 0.012709409360352766, 'timesteps_3': 0.013212442520124074, 'timesteps_2': 0.014326228000078446}\n",
      "sorted rmse\n",
      "{'timesteps_1': 0.017236430432879077, 'timesteps_4': 0.01756805243763662, 'timesteps_3': 0.017914216506633596, 'timesteps_2': 0.018798476996323747}\n",
      "sorted mape\n",
      "{'timesteps_4': 0.029484858211290245, 'timesteps_1': 0.030344619679000352, 'timesteps_2': 0.03485906558726514, 'timesteps_3': 0.04070394123414279}\n",
      "sorted r2\n",
      "{'timesteps_1': 0.9935702374781473, 'timesteps_4': 0.9933026921297792, 'timesteps_3': 0.9929662622837995, 'timesteps_2': 0.9923649299409023}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_jsmr_1_sorted = dict(sorted(mae_jsmr.items(),key=lambda item: item[1]))\n",
    "print(mae_jsmr_1_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_jsmr_sorted = dict(sorted(rmse_jsmr.items(),key=lambda item: item[1]))\n",
    "print(rmse_jsmr_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_jsmr_sorted = dict(sorted(mape_jsmr.items(),key=lambda item: item[1]))\n",
    "print(mape_jsmr_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_jsmr_sorted = dict(sorted(r2_jsmr.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_jsmr_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted mae\n",
    "# {'timesteps_4': 0.012534012823066423, 'timesteps_1': 0.012709409360352766, 'timesteps_3': 0.013212442520124074, 'timesteps_2': 0.014326228000078446}\n",
    "# sorted rmse\n",
    "# {'timesteps_1': 0.017236430432879077, 'timesteps_4': 0.01756805243763662, 'timesteps_3': 0.017914216506633596, 'timesteps_2': 0.018798476996323747}\n",
    "# sorted mape\n",
    "# {'timesteps_4': 0.029484858211290245, 'timesteps_1': 0.030344619679000352, 'timesteps_2': 0.03485906558726514, 'timesteps_3': 0.04070394123414279}\n",
    "# sorted r2\n",
    "# {'timesteps_1': 0.9935702374781473, 'timesteps_4': 0.9933026921297792, 'timesteps_3': 0.9929662622837995, 'timesteps_2': 0.9923649299409023}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan hasil tuning timestep\n",
    "hasil tuning timestep diatas untuk keempat saham yang digunakan,\n",
    "ANTM 2 timestep\n",
    "ASII 3 timestep\n",
    "ICBP 1 timestep\n",
    "JSMR 1 atau 4 timestep\n",
    "\n",
    "keempat saham menunjukan hasil berbeda untuk tuning timesteps ini. untuk JSMR 1 timesteps memberikan hasil terbaik pada matriks RMSE dan R2 sedangkan 4 timestep memberikan hasil terbaik pada MAE dan MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIDDEN LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_37 (LSTM)              (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 329\n",
      "Trainable params: 329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_antm_n2_h1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_n2_h1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_antm_n2_h1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_38 (LSTM)              (None, 2, 8)              320       \n",
      "                                                                 \n",
      " lstm_39 (LSTM)              (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_antm_n2_h2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_n2_h2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_antm_n2_h2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_40 (LSTM)              (None, 2, 8)              320       \n",
      "                                                                 \n",
      " lstm_41 (LSTM)              (None, 2, 8)              544       \n",
      "                                                                 \n",
      " lstm_42 (LSTM)              (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,417\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_antm_n2_h3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_n2_h3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_antm_n2_h3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_43 (LSTM)              (None, 2, 8)              320       \n",
      "                                                                 \n",
      " lstm_44 (LSTM)              (None, 2, 8)              544       \n",
      "                                                                 \n",
      " lstm_45 (LSTM)              (None, 2, 8)              544       \n",
      "                                                                 \n",
      " lstm_46 (LSTM)              (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,961\n",
      "Trainable params: 1,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_antm_n2_h4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_2, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_antm_n2_h4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_antm_n2_h4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.1521 - mae: 0.1521 - val_loss: 0.1064 - val_mae: 0.1064\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0438 - mae: 0.0438 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0079 - val_mae: 0.0079\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_n2_h1 = simple_model_antm_n2_h1.fit(X_train_antm_2, y_train_antm_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "#time 21.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 4s 11ms/step - loss: 0.0977 - mae: 0.0977 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0210 - val_mae: 0.0210\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0115 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0115 - mae: 0.0115 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0086 - mae: 0.008 - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0132 - val_mae: 0.0132\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_n2_h2 = simple_model_antm_n2_h2.fit(X_train_antm_2, y_train_antm_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 28.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 4s 14ms/step - loss: 0.1490 - mae: 0.1490 - val_loss: 0.0780 - val_mae: 0.0780\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0523 - mae: 0.0523 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0376 - mae: 0.0376 - val_loss: 0.0294 - val_mae: 0.0294\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0260 - val_mae: 0.0260\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - mae: 0.0117 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0214 - val_mae: 0.0214\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0117 - mae: 0.0117 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0103 - mae: 0.010 - 0s 6ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0099 - val_mae: 0.0099\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_n2_h3 = simple_model_antm_n2_h3.fit(X_train_antm_2, y_train_antm_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 5s 18ms/step - loss: 0.1465 - mae: 0.1465 - val_loss: 0.0925 - val_mae: 0.0925\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0439 - mae: 0.0439 - val_loss: 0.0279 - val_mae: 0.0279\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - mae: 0.0119 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - mae: 0.0122 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0122 - mae: 0.0122 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0100 - val_mae: 0.0100\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0118 - mae: 0.0118 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0115 - mae: 0.0115 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0118 - mae: 0.0118 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0118 - mae: 0.0118 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - mae: 0.0119 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0200 - val_mae: 0.0200\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0115 - mae: 0.0115 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0118 - mae: 0.0118 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0115 - mae: 0.0115 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0120 - mae: 0.0120 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0105 - val_mae: 0.0105\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0080 - val_mae: 0.0080\n"
     ]
    }
   ],
   "source": [
    "smod_history_antm_n2_h4 = simple_model_antm_n2_h4.fit(X_train_antm_2, y_train_antm_2,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_antm_n2_h1 = simple_model_antm_n2_h1.predict(X_test_antm_2)\n",
    "preds_antm_n2_h2 = simple_model_antm_n2_h2.predict(X_test_antm_2)\n",
    "preds_antm_n2_h3= simple_model_antm_n2_h3.predict(X_test_antm_2)\n",
    "preds_antm_n2_h4= simple_model_antm_n2_h4.predict(X_test_antm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 1\n",
      "mae score antm_n2_h1: 0.008152804872559106\n",
      "r2 score antm_n2_h1: 0.9949518577972858\n",
      "mape score antm_n2_h1: 0.03489248940375355\n",
      "rmse score antm_n2_h1: 0.013849466958302258\n",
      "hidden layer 2\n",
      "mae score antm_n2_h2: 0.013214672773774904\n",
      "r2 score antm_n2_h2: 0.9924275692637212\n",
      "mape score antm_n2_h2: 0.054800074878532695\n",
      "rmse score antm_n2_h2: 0.017459215173058307\n",
      "hidden layer 3\n",
      "mae score antm_n2_h3: 0.009998062981012124\n",
      "r2 score antm_n2_h3: 0.9940292328244711\n",
      "mape score antm_n2_h3: 0.04404339294199065\n",
      "rmse score antm_n2_h3: 0.015687598031636656\n",
      "hidden layer 4\n",
      "mae score antm_n2_h4: 0.008228418334623457\n",
      "r2 score antm_n2_h4: 0.9949234614902839\n",
      "mape score antm_n2_h4: 0.03492159235018264\n",
      "rmse score antm_n2_h4: 0.013806358248692464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 1\")\n",
    "print(\"mae score antm_n2_h1: \"+str(mean_absolute_error(preds_antm_n2_h1, y_test_antm_2)))\n",
    "print(\"r2 score antm_n2_h1: \"+str(r2_score(preds_antm_n2_h1, y_test_antm_2)))\n",
    "print(\"mape score antm_n2_h1: \"+str(mean_absolute_percentage_error(preds_antm_n2_h1, y_test_antm_2)))\n",
    "print(\"rmse score antm_n2_h1: \"+str(np.sqrt(mean_squared_error(preds_antm_n2_h1, y_test_antm_2))))\n",
    "\n",
    "print(\"hidden layer 2\")\n",
    "print(\"mae score antm_n2_h2: \"+str(mean_absolute_error(preds_antm_n2_h2, y_test_antm_2)))\n",
    "print(\"r2 score antm_n2_h2: \"+str(r2_score(preds_antm_n2_h2, y_test_antm_2)))\n",
    "print(\"mape score antm_n2_h2: \"+str(mean_absolute_percentage_error(preds_antm_n2_h2, y_test_antm_2)))\n",
    "print(\"rmse score antm_n2_h2: \"+str(np.sqrt(mean_squared_error(preds_antm_n2_h2, y_test_antm_2))))\n",
    "\n",
    "print(\"hidden layer 3\")\n",
    "print(\"mae score antm_n2_h3: \"+str(mean_absolute_error(preds_antm_n2_h3, y_test_antm_2)))\n",
    "print(\"r2 score antm_n2_h3: \"+str(r2_score(preds_antm_n2_h3, y_test_antm_2)))\n",
    "print(\"mape score antm_n2_h3: \"+str(mean_absolute_percentage_error(preds_antm_n2_h3, y_test_antm_2)))\n",
    "print(\"rmse score antm_n2_h3: \"+str(np.sqrt(mean_squared_error(preds_antm_n2_h3, y_test_antm_2))))\n",
    "\n",
    "print(\"hidden layer 4\")\n",
    "print(\"mae score antm_n2_h4: \"+str(mean_absolute_error(preds_antm_n2_h4, y_test_antm_2)))\n",
    "print(\"r2 score antm_n2_h4: \"+str(r2_score(preds_antm_n2_h4, y_test_antm_2)))\n",
    "print(\"mape score antm_n2_h4: \"+str(mean_absolute_percentage_error(preds_antm_n2_h4, y_test_antm_2)))\n",
    "print(\"rmse score antm_n2_h4: \"+str(np.sqrt(mean_squared_error(preds_antm_n2_h4, y_test_antm_2))))\n",
    "\n",
    "mae_antm_hl = {'hidden layer_n2_h1':mean_absolute_error(preds_antm_n2_h1, y_test_antm_2),'hidden layer_n2_h2':mean_absolute_error(preds_antm_n2_h2, y_test_antm_2),'hidden layer_n2_h3':mean_absolute_error(preds_antm_n2_h3, y_test_antm_2),'hidden layer_n2_h4':mean_absolute_error(preds_antm_n2_h4, y_test_antm_2)}\n",
    "\n",
    "mape_antm_hl = {'hidden layer_n2_h1':mean_absolute_percentage_error(preds_antm_n2_h1, y_test_antm_2),'hidden layer_n2_h2':mean_absolute_percentage_error(preds_antm_n2_h2, y_test_antm_2),'hidden layer_n2_h3':mean_absolute_percentage_error(preds_antm_n2_h3, y_test_antm_2),'hidden layer_n2_h4':mean_absolute_percentage_error(preds_antm_n2_h4, y_test_antm_2)}\n",
    "\n",
    "rmse_antm_hl = {'hidden layer_n2_h1':np.sqrt(mean_squared_error(preds_antm_n2_h1, y_test_antm_2)),'hidden layer_n2_h2':np.sqrt(mean_squared_error(preds_antm_n2_h2, y_test_antm_2)),'hidden layer_n2_h3':np.sqrt(mean_squared_error(preds_antm_n2_h3, y_test_antm_2)),'hidden layer_n2_h4':np.sqrt(mean_squared_error(preds_antm_n2_h4, y_test_antm_2))}\n",
    "\n",
    "r2_antm_hl = {'hidden layer_n2_h1':r2_score(preds_antm_n2_h1, y_test_antm_2),'hidden layer_n2_h2':r2_score(preds_antm_n2_h2, y_test_antm_2),'hidden layer_n2_h3':r2_score(preds_antm_n2_h3, y_test_antm_2),'hidden layer_n2_h4':r2_score(preds_antm_n2_h4, y_test_antm_2)} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 1\n",
    "# mae score antm_n2_h1: 0.008152804872559106\n",
    "# r2 score antm_n2_h1: 0.9949518577972858\n",
    "# mape score antm_n2_h1: 0.03489248940375355\n",
    "# rmse score antm_n2_h1: 0.013849466958302258\n",
    "# hidden layer 2\n",
    "# mae score antm_n2_h2: 0.013214672773774904\n",
    "# r2 score antm_n2_h2: 0.9924275692637212\n",
    "# mape score antm_n2_h2: 0.054800074878532695\n",
    "# rmse score antm_n2_h2: 0.017459215173058307\n",
    "# hidden layer 3\n",
    "# mae score antm_n2_h3: 0.009998062981012124\n",
    "# r2 score antm_n2_h3: 0.9940292328244711\n",
    "# mape score antm_n2_h3: 0.04404339294199065\n",
    "# rmse score antm_n2_h3: 0.015687598031636656\n",
    "# hidden layer 4\n",
    "# mae score antm_n2_h4: 0.008228418334623457\n",
    "# r2 score antm_n2_h4: 0.9949234614902839\n",
    "# mape score antm_n2_h4: 0.03492159235018264\n",
    "# rmse score antm_n2_h4: 0.013806358248692464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'hidden layer_n2_h1': 0.008152804872559106, 'hidden layer_n2_h4': 0.008228418334623457, 'hidden layer_n2_h3': 0.009998062981012124, 'hidden layer_n2_h2': 0.013214672773774904}\n",
      "sorted rmse\n",
      "{'hidden layer_n2_h4': 0.013806358248692464, 'hidden layer_n2_h1': 0.013849466958302258, 'hidden layer_n2_h3': 0.015687598031636656, 'hidden layer_n2_h2': 0.017459215173058307}\n",
      "sorted mape\n",
      "{'hidden layer_n2_h1': 0.03489248940375355, 'hidden layer_n2_h4': 0.03492159235018264, 'hidden layer_n2_h3': 0.04404339294199065, 'hidden layer_n2_h2': 0.054800074878532695}\n",
      "sorted r2\n",
      "{'hidden layer_n2_h1': 0.9949518577972858, 'hidden layer_n2_h4': 0.9949234614902839, 'hidden layer_n2_h3': 0.9940292328244711, 'hidden layer_n2_h2': 0.9924275692637212}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_antm_hl_sorted = dict(sorted(mae_antm_hl.items(),key=lambda item: item[1]))\n",
    "print(mae_antm_hl_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_antm_hl_sorted = dict(sorted(rmse_antm_hl.items(),key=lambda item: item[1]))\n",
    "print(rmse_antm_hl_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_antm_hl_sorted = dict(sorted(mape_antm_hl.items(),key=lambda item: item[1]))\n",
    "print(mape_antm_hl_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_antm_hl_sorted = dict(sorted(r2_antm_hl.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_antm_hl_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted mae\n",
    "# {'hidden layer_n2_h1': 0.008152804872559106, 'hidden layer_n2_h4': 0.008228418334623457, 'hidden layer_n2_h3': 0.009998062981012124, 'hidden layer_n2_h2': 0.013214672773774904}\n",
    "# sorted rmse\n",
    "# {'hidden layer_n2_h4': 0.013806358248692464, 'hidden layer_n2_h1': 0.013849466958302258, 'hidden layer_n2_h3': 0.015687598031636656, 'hidden layer_n2_h2': 0.017459215173058307}\n",
    "# sorted mape\n",
    "# {'hidden layer_n2_h1': 0.03489248940375355, 'hidden layer_n2_h4': 0.03492159235018264, 'hidden layer_n2_h3': 0.04404339294199065, 'hidden layer_n2_h2': 0.054800074878532695}\n",
    "# sorted r2\n",
    "# {'hidden layer_n2_h1': 0.9949518577972858, 'hidden layer_n2_h4': 0.9949234614902839, 'hidden layer_n2_h3': 0.9940292328244711, 'hidden layer_n2_h2': 0.9924275692637212}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_104 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 329\n",
      "Trainable params: 329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_asii_n3_h1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_n3_h1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_asii_n3_h1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_107 (LSTM)             (None, 3, 8)              320       \n",
      "                                                                 \n",
      " lstm_108 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_asii_n3_h2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_n3_h2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_asii_n3_h2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_112 (LSTM)             (None, 3, 8)              320       \n",
      "                                                                 \n",
      " lstm_113 (LSTM)             (None, 3, 8)              544       \n",
      "                                                                 \n",
      " lstm_114 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,417\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_asii_n3_h3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_n3_h3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_asii_n3_h3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_115 (LSTM)             (None, 3, 8)              320       \n",
      "                                                                 \n",
      " lstm_116 (LSTM)             (None, 3, 8)              544       \n",
      "                                                                 \n",
      " lstm_117 (LSTM)             (None, 3, 8)              544       \n",
      "                                                                 \n",
      " lstm_118 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,961\n",
      "Trainable params: 1,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_asii_n3_h4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_3, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_asii_n3_h4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_asii_n3_h4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.0516 - val_mae: 0.0516\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0290 - mae: 0.0290 - val_loss: 0.0200 - val_mae: 0.0200\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0202 - mae: 0.0202 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0205 - val_mae: 0.0205\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0217 - mae: 0.0217 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0258 - val_mae: 0.0258\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0210 - val_mae: 0.0210\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0209 - val_mae: 0.0209\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0192 - val_mae: 0.0192\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0212 - val_mae: 0.0212\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0290 - val_mae: 0.0290\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0195 - mae: 0.0195 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0193 - val_mae: 0.0193\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0146 - val_mae: 0.0146\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_n3_h1 = simple_model_asii_n3_h1.fit(X_train_asii_3, y_train_asii_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 21.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 3s 12ms/step - loss: 0.1849 - mae: 0.1849 - val_loss: 0.0627 - val_mae: 0.0627\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0322 - mae: 0.0322 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0200 - val_mae: 0.0200\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0297 - val_mae: 0.0297\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0251 - mae: 0.0251 - val_loss: 0.0208 - val_mae: 0.0208\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0287 - val_mae: 0.0287\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0201 - mae: 0.0201 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0270 - val_mae: 0.0270\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0212 - mae: 0.0212 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0405 - val_mae: 0.0405\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0260 - mae: 0.0260 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0192 - mae: 0.0192 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0195 - mae: 0.0195 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0203 - mae: 0.0203 - val_loss: 0.0350 - val_mae: 0.0350\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0266 - val_mae: 0.0266\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0211 - mae: 0.0211 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0254 - val_mae: 0.0254\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0188 - mae: 0.0188 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0210 - val_mae: 0.0210\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0194 - mae: 0.0194 - val_loss: 0.0183 - val_mae: 0.0183\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0382 - val_mae: 0.0382\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0214 - mae: 0.0214 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0203 - val_mae: 0.0203\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0279 - val_mae: 0.0279\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.0207 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0165 - val_mae: 0.0165\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_n3_h2 = simple_model_asii_n3_h2.fit(X_train_asii_3, y_train_asii_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "#time 28.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 6s 18ms/step - loss: 0.2144 - mae: 0.2144 - val_loss: 0.0672 - val_mae: 0.0672\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0406 - mae: 0.0406 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0272 - mae: 0.0272 - val_loss: 0.0456 - val_mae: 0.0456\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0256 - mae: 0.0256 - val_loss: 0.0282 - val_mae: 0.0282\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0253 - val_mae: 0.0253\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0266 - mae: 0.0266 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0249 - mae: 0.0249 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0274 - val_mae: 0.0274\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0255 - val_mae: 0.0255\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0269 - val_mae: 0.0269\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.0242 - val_loss: 0.0290 - val_mae: 0.0290\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0259 - mae: 0.0259 - val_loss: 0.0265 - val_mae: 0.0265\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0351 - val_mae: 0.0351\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0261 - mae: 0.0261 - val_loss: 0.0347 - val_mae: 0.0347\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0235 - mae: 0.023 - 0s 7ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0206 - mae: 0.0206 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0210 - mae: 0.0210 - val_loss: 0.0248 - val_mae: 0.0248\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0275 - val_mae: 0.0275\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0276 - val_mae: 0.0276\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0192 - mae: 0.0192 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0253 - mae: 0.0253 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0207 - mae: 0.0207 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0210 - mae: 0.0210 - val_loss: 0.0571 - val_mae: 0.0571\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0379 - val_mae: 0.0379\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0214 - mae: 0.0214 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0192 - mae: 0.0192 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0202 - mae: 0.0202 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0203 - val_mae: 0.0203\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0197 - mae: 0.0197 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0194 - mae: 0.0194 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0260 - val_mae: 0.0260\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0195 - mae: 0.0195 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0299 - val_mae: 0.0299\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0203 - mae: 0.0203 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0265 - val_mae: 0.0265\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0286 - val_mae: 0.0286\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0275 - val_mae: 0.0275\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0212 - val_mae: 0.0212\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0286 - val_mae: 0.0286\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0183 - val_mae: 0.0183\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0203 - val_mae: 0.0203\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0296 - val_mae: 0.0296\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0184 - val_mae: 0.0184\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_n3_h3 = simple_model_asii_n3_h3.fit(X_train_asii_3, y_train_asii_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "#time 38.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 5s 20ms/step - loss: 0.2438 - mae: 0.2438 - val_loss: 0.1089 - val_mae: 0.1089\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0503 - mae: 0.0503 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0266 - mae: 0.0266 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0256 - mae: 0.0256 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0290 - mae: 0.0290 - val_loss: 0.0422 - val_mae: 0.0422\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0283 - mae: 0.0283 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0281 - mae: 0.0281 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0273 - mae: 0.0273 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0362 - val_mae: 0.0362\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0444 - val_mae: 0.0444\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0195 - val_mae: 0.0195\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0210 - mae: 0.0210 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0213 - mae: 0.0213 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0207 - mae: 0.0207 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0209 - mae: 0.0209 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0211 - mae: 0.0211 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0203 - mae: 0.0203 - val_loss: 0.0302 - val_mae: 0.0302\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0197 - mae: 0.0197 - val_loss: 0.0362 - val_mae: 0.0362\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0254 - val_mae: 0.0254\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0240 - val_mae: 0.0240\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0214 - val_mae: 0.0214\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0206 - mae: 0.0206 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0194 - mae: 0.0194 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0404 - val_mae: 0.0404\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0192 - val_mae: 0.0192\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0191 - mae: 0.0191 - val_loss: 0.0209 - val_mae: 0.0209\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0197 - val_mae: 0.0197\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0195 - mae: 0.0195 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0197 - val_mae: 0.0197\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0207 - mae: 0.0207 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0192 - val_mae: 0.0192\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0188 - mae: 0.0188 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0188 - mae: 0.0188 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0249 - val_mae: 0.0249\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0188 - mae: 0.0188 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0173 - mae: 0.017 - 0s 8ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0170 - val_mae: 0.0170\n"
     ]
    }
   ],
   "source": [
    "smod_history_asii_n3_h4 = simple_model_asii_n3_h4.fit(X_train_asii_3, y_train_asii_3,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 43.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_asii_n3_h1 = simple_model_asii_n3_h1.predict(X_test_asii_3)\n",
    "preds_asii_n3_h2 = simple_model_asii_n3_h2.predict(X_test_asii_3)\n",
    "preds_asii_n3_h3= simple_model_asii_n3_h3.predict(X_test_asii_3)\n",
    "preds_asii_n3_h4= simple_model_asii_n3_h4.predict(X_test_asii_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 1\n",
      "mae score asii_n3_h1: 0.014757829851888154\n",
      "r2 score asii_n3_h1: 0.9901460537730357\n",
      "mape score asii_n3_h1: 0.03297773155326224\n",
      "rmse score asii_n3_h1: 0.019715849938205837\n",
      "hidden layer 2\n",
      "mae score asii_n3_h2: 0.016830770740112168\n",
      "r2 score asii_n3_h2: 0.988113325668868\n",
      "mape score asii_n3_h2: 0.03642616801224726\n",
      "rmse score asii_n3_h2: 0.02168610640315832\n",
      "hidden layer 3\n",
      "mae score asii_n3_h3: 0.018521870427194456\n",
      "r2 score asii_n3_h3: 0.9867944354102721\n",
      "mape score asii_n3_h3: 0.03875453612873989\n",
      "rmse score asii_n3_h3: 0.023499048452511002\n",
      "hidden layer 4\n",
      "mae score asii_n3_h4: 0.016859120812902816\n",
      "r2 score asii_n3_h4: 0.9868324360049459\n",
      "mape score asii_n3_h4: 0.04349796703399346\n",
      "rmse score asii_n3_h4: 0.021919565859925376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 1\")\n",
    "print(\"mae score asii_n3_h1: \"+str(mean_absolute_error(preds_asii_n3_h1, y_test_asii_3)))\n",
    "print(\"r2 score asii_n3_h1: \"+str(r2_score(preds_asii_n3_h1, y_test_asii_3)))\n",
    "print(\"mape score asii_n3_h1: \"+str(mean_absolute_percentage_error(preds_asii_n3_h1, y_test_asii_3)))\n",
    "print(\"rmse score asii_n3_h1: \"+str(np.sqrt(mean_squared_error(preds_asii_n3_h1, y_test_asii_3))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 2\")\n",
    "print(\"mae score asii_n3_h2: \"+str(mean_absolute_error(preds_asii_n3_h2, y_test_asii_3)))\n",
    "print(\"r2 score asii_n3_h2: \"+str(r2_score(preds_asii_n3_h2, y_test_asii_3)))\n",
    "print(\"mape score asii_n3_h2: \"+str(mean_absolute_percentage_error(preds_asii_n3_h2, y_test_asii_3)))\n",
    "print(\"rmse score asii_n3_h2: \"+str(np.sqrt(mean_squared_error(preds_asii_n3_h2, y_test_asii_3))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 3\")\n",
    "print(\"mae score asii_n3_h3: \"+str(mean_absolute_error(preds_asii_n3_h3, y_test_asii_3)))\n",
    "print(\"r2 score asii_n3_h3: \"+str(r2_score(preds_asii_n3_h3, y_test_asii_3)))\n",
    "print(\"mape score asii_n3_h3: \"+str(mean_absolute_percentage_error(preds_asii_n3_h3, y_test_asii_3)))\n",
    "print(\"rmse score asii_n3_h3: \"+str(np.sqrt(mean_squared_error(preds_asii_n3_h3, y_test_asii_3))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 4\")\n",
    "print(\"mae score asii_n3_h4: \"+str(mean_absolute_error(preds_asii_n3_h4, y_test_asii_3)))\n",
    "print(\"r2 score asii_n3_h4: \"+str(r2_score(preds_asii_n3_h4, y_test_asii_3)))\n",
    "print(\"mape score asii_n3_h4: \"+str(mean_absolute_percentage_error(preds_asii_n3_h4, y_test_asii_3)))\n",
    "print(\"rmse score asii_n3_h4: \"+str(np.sqrt(mean_squared_error(preds_asii_n3_h4, y_test_asii_3))))\n",
    "\n",
    "mae_asii_hl = {'hidden layer_n3_h1':mean_absolute_error(preds_asii_n3_h1, y_test_asii_3),'hidden layer_n3_h2':mean_absolute_error(preds_asii_n3_h2, y_test_asii_3),'hidden layer_n3_h3':mean_absolute_error(preds_asii_n3_h3, y_test_asii_3),'hidden layer_n3_h4':mean_absolute_error(preds_asii_n3_h4, y_test_asii_3)}\n",
    "\n",
    "mape_asii_hl = {'hidden layer_n3_h1':mean_absolute_percentage_error(preds_asii_n3_h1, y_test_asii_3),'hidden layer_n3_h2':mean_absolute_percentage_error(preds_asii_n3_h2, y_test_asii_3),'hidden layer_n3_h3':mean_absolute_percentage_error(preds_asii_n3_h3, y_test_asii_3),'hidden layer_n3_h4':mean_absolute_percentage_error(preds_asii_n3_h4, y_test_asii_3)}\n",
    "\n",
    "rmse_asii_hl = {'hidden layer_n3_h1':np.sqrt(mean_squared_error(preds_asii_n3_h1, y_test_asii_3)),'hidden layer_n3_h2':np.sqrt(mean_squared_error(preds_asii_n3_h2, y_test_asii_3)),'hidden layer_n3_h3':np.sqrt(mean_squared_error(preds_asii_n3_h3, y_test_asii_3)),'hidden layer_n3_h4':np.sqrt(mean_squared_error(preds_asii_n3_h4, y_test_asii_3))}\n",
    "\n",
    "r2_asii_hl = {'hidden layer_n3_h1':r2_score(preds_asii_n3_h1, y_test_asii_3),'hidden layer_n3_h2':r2_score(preds_asii_n3_h2, y_test_asii_3),'hidden layer_n3_h3':r2_score(preds_asii_n3_h3, y_test_asii_3),'hidden layer_n3_h4':r2_score(preds_asii_n3_h4, y_test_asii_3)} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 1\n",
    "# mae score asii_n3_h1: 0.014757829851888154\n",
    "# r2 score asii_n3_h1: 0.9901460537730357\n",
    "# mape score asii_n3_h1: 0.03297773155326224\n",
    "# rmse score asii_n3_h1: 0.019715849938205837\n",
    "# hidden layer 2\n",
    "# mae score asii_n3_h2: 0.016830770740112168\n",
    "# r2 score asii_n3_h2: 0.988113325668868\n",
    "# mape score asii_n3_h2: 0.03642616801224726\n",
    "# rmse score asii_n3_h2: 0.02168610640315832\n",
    "# hidden layer 3\n",
    "# mae score asii_n3_h3: 0.018521870427194456\n",
    "# r2 score asii_n3_h3: 0.9867944354102721\n",
    "# mape score asii_n3_h3: 0.03875453612873989\n",
    "# rmse score asii_n3_h3: 0.023499048452511002\n",
    "# hidden layer 4\n",
    "# mae score asii_n3_h4: 0.016859120812902816\n",
    "# r2 score asii_n3_h4: 0.9868324360049459\n",
    "# mape score asii_n3_h4: 0.04349796703399346\n",
    "# rmse score asii_n3_h4: 0.021919565859925376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'hidden layer_n3_h1': 0.014757829851888154, 'hidden layer_n3_h2': 0.016830770740112168, 'hidden layer_n3_h4': 0.016859120812902816, 'hidden layer_n3_h3': 0.018521870427194456}\n",
      "sorted rmse\n",
      "{'hidden layer_n3_h1': 0.019715849938205837, 'hidden layer_n3_h2': 0.02168610640315832, 'hidden layer_n3_h4': 0.021919565859925376, 'hidden layer_n3_h3': 0.023499048452511002}\n",
      "sorted mape\n",
      "{'hidden layer_n3_h1': 0.03297773155326224, 'hidden layer_n3_h2': 0.03642616801224726, 'hidden layer_n3_h3': 0.03875453612873989, 'hidden layer_n3_h4': 0.04349796703399346}\n",
      "sorted r2\n",
      "{'hidden layer_n3_h1': 0.9901460537730357, 'hidden layer_n3_h2': 0.988113325668868, 'hidden layer_n3_h4': 0.9868324360049459, 'hidden layer_n3_h3': 0.9867944354102721}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_asii_hl_sorted = dict(sorted(mae_asii_hl.items(),key=lambda item: item[1]))\n",
    "print(mae_asii_hl_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_asii_hl_sorted = dict(sorted(rmse_asii_hl.items(),key=lambda item: item[1]))\n",
    "print(rmse_asii_hl_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_asii_hl_sorted = dict(sorted(mape_asii_hl.items(),key=lambda item: item[1]))\n",
    "print(mape_asii_hl_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_asii_hl_sorted = dict(sorted(r2_asii_hl.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_asii_hl_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted mae\n",
    "# {'hidden layer_n3_h1': 0.014757829851888154, 'hidden layer_n3_h2': 0.016830770740112168, 'hidden layer_n3_h4': 0.016859120812902816, 'hidden layer_n3_h3': 0.018521870427194456}\n",
    "# sorted rmse\n",
    "# {'hidden layer_n3_h1': 0.019715849938205837, 'hidden layer_n3_h2': 0.02168610640315832, 'hidden layer_n3_h4': 0.021919565859925376, 'hidden layer_n3_h3': 0.023499048452511002}\n",
    "# sorted mape\n",
    "# {'hidden layer_n3_h1': 0.03297773155326224, 'hidden layer_n3_h2': 0.03642616801224726, 'hidden layer_n3_h3': 0.03875453612873989, 'hidden layer_n3_h4': 0.04349796703399346}\n",
    "# sorted r2\n",
    "# {'hidden layer_n3_h1': 0.9901460537730357, 'hidden layer_n3_h2': 0.988113325668868, 'hidden layer_n3_h4': 0.9868324360049459, 'hidden layer_n3_h3': 0.9867944354102721}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_119 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 329\n",
      "Trainable params: 329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_icbp_n1_h1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_n1_h1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_icbp_n1_h1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_120 (LSTM)             (None, 1, 8)              320       \n",
      "                                                                 \n",
      " lstm_121 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_icbp_n1_h2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_n1_h2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_icbp_n1_h2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_122 (LSTM)             (None, 1, 8)              320       \n",
      "                                                                 \n",
      " lstm_123 (LSTM)             (None, 1, 8)              544       \n",
      "                                                                 \n",
      " lstm_124 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,417\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_icbp_n1_h3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_n1_h3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_icbp_n1_h3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_125 (LSTM)             (None, 1, 8)              320       \n",
      "                                                                 \n",
      " lstm_126 (LSTM)             (None, 1, 8)              544       \n",
      "                                                                 \n",
      " lstm_127 (LSTM)             (None, 1, 8)              544       \n",
      "                                                                 \n",
      " lstm_128 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,961\n",
      "Trainable params: 1,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_icbp_n1_h4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_icbp_n1_h4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_icbp_n1_h4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 8ms/step - loss: 0.2424 - mae: 0.2424 - val_loss: 0.1269 - val_mae: 0.1269\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0775 - mae: 0.0775 - val_loss: 0.0245 - val_mae: 0.0245\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0209 - val_mae: 0.0209\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0115 - val_mae: 0.0115\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0097 - val_mae: 0.0097\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0076 - mae: 0.0076 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0077 - mae: 0.0077 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0103 - val_mae: 0.0103\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0077 - mae: 0.0077 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0076 - mae: 0.0076 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0083 - mae: 0.008 - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0086 - mae: 0.008 - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0077 - mae: 0.0077 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0077 - mae: 0.0077 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0079 - mae: 0.0079 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0078 - val_loss: 0.0073 - val_mae: 0.0073\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_n3_h1 = simple_model_icbp_n1_h1.fit(X_train_icbp_1, y_train_icbp_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 19.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 4s 14ms/step - loss: 0.2663 - mae: 0.2663 - val_loss: 0.1514 - val_mae: 0.1514\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0855 - mae: 0.0855 - val_loss: 0.0411 - val_mae: 0.0411\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0426 - mae: 0.0426 - val_loss: 0.0367 - val_mae: 0.0367\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0366 - mae: 0.0366 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0253 - val_mae: 0.0253\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0200 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0088 - val_mae: 0.0088\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0121 - mae: 0.0121 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0099 - val_mae: 0.0099\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0104 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0084 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0119 - mae: 0.0119 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0111 - val_mae: 0.0111\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0105 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0089 - val_mae: 0.0089\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0080 - mae: 0.0080 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0072 - val_mae: 0.0072\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0082 - mae: 0.0082 - val_loss: 0.0145 - val_mae: 0.0145\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_n3_h2 = simple_model_icbp_n1_h2.fit(X_train_icbp_1, y_train_icbp_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 28.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 5s 21ms/step - loss: 0.2701 - mae: 0.2701 - val_loss: 0.1240 - val_mae: 0.1240\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0737 - mae: 0.0737 - val_loss: 0.0502 - val_mae: 0.0502\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0499 - mae: 0.0499 - val_loss: 0.0381 - val_mae: 0.0381\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0383 - mae: 0.0383 - val_loss: 0.0293 - val_mae: 0.0293\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0213 - mae: 0.0213 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0094 - val_mae: 0.0094\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0108 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0106 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0100 - mae: 0.0100 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0117 - mae: 0.0117 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0107 - val_mae: 0.0107\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0085 - val_mae: 0.0085\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0092 - val_mae: 0.0092\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0099 - mae: 0.0099 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0183 - val_mae: 0.0183\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0112 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0113 - val_mae: 0.0113\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0103 - val_loss: 0.0078 - val_mae: 0.0078\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0114 - val_mae: 0.0114\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0082 - val_mae: 0.0082\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0114 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0075 - val_mae: 0.0075\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0086 - val_mae: 0.0086\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0091 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0109 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0080 - val_mae: 0.0080\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0083 - mae: 0.0083 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0104 - val_mae: 0.0104\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0085 - mae: 0.0085 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0087 - mae: 0.0087 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0108 - val_mae: 0.0108\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0102 - val_mae: 0.0102\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0081 - val_mae: 0.0081\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0088 - mae: 0.0088 - val_loss: 0.0098 - val_mae: 0.0098\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0089 - mae: 0.0089 - val_loss: 0.0109 - val_mae: 0.0109\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - mae: 0.0090 - val_loss: 0.0084 - val_mae: 0.0084\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - mae: 0.0094 - val_loss: 0.0087 - val_mae: 0.0087\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0092 - mae: 0.0092 - val_loss: 0.0101 - val_mae: 0.0101\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0119 - val_mae: 0.0119\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_n3_h3 = simple_model_icbp_n1_h3.fit(X_train_icbp_1, y_train_icbp_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 31.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 10s 26ms/step - loss: 0.3191 - mae: 0.3191 - val_loss: 0.2390 - val_mae: 0.2390\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2268 - val_mae: 0.2268\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2268 - val_mae: 0.2268\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2263 - val_mae: 0.2263\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2269 - val_mae: 0.2269\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2259 - val_mae: 0.2259\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2262 - val_mae: 0.2262\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2262 - val_mae: 0.2262\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2259 - val_mae: 0.2259\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2264 - val_mae: 0.2264\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2259 - val_mae: 0.2259\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2271 - val_mae: 0.2271\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2263 - val_mae: 0.2263\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2388 - mae: 0.2388 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2269 - val_mae: 0.2269\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2265 - val_mae: 0.2265\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2265 - val_mae: 0.2265\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2262 - val_mae: 0.2262\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2270 - val_mae: 0.2270\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2263 - val_mae: 0.2263\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2264 - val_mae: 0.2264\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2388 - mae: 0.2388 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2271 - val_mae: 0.2271\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2394 - mae: 0.2394 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2265 - val_mae: 0.2265\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2388 - mae: 0.2388 - val_loss: 0.2275 - val_mae: 0.2275\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.2267 - val_mae: 0.2267\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2388 - mae: 0.2388 - val_loss: 0.2274 - val_mae: 0.2274\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2264 - val_mae: 0.2264\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2269 - val_mae: 0.2269\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2253 - val_mae: 0.2253\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2265 - val_mae: 0.2265\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2394 - mae: 0.2394 - val_loss: 0.2267 - val_mae: 0.2267\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2259 - val_mae: 0.2259\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2263 - val_mae: 0.2263\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2280 - val_mae: 0.2280\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2264 - val_mae: 0.2264\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2263 - val_mae: 0.2263\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2259 - val_mae: 0.2259\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2258 - val_mae: 0.2258\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2269 - val_mae: 0.2269\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2265 - val_mae: 0.2265\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2270 - val_mae: 0.2270\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2265 - val_mae: 0.2265\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2288 - val_mae: 0.2288\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2397 - mae: 0.2397 - val_loss: 0.2268 - val_mae: 0.2268\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2265 - val_mae: 0.2265\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2268 - val_mae: 0.2268\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2251 - val_mae: 0.2251\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.2399 - mae: 0.2399 - val_loss: 0.2250 - val_mae: 0.2250\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2270 - val_mae: 0.2270\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2393 - mae: 0.2393 - val_loss: 0.2252 - val_mae: 0.2252\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2267 - val_mae: 0.2267\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2388 - mae: 0.2388 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2271 - val_mae: 0.2271\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2395 - mae: 0.2395 - val_loss: 0.2263 - val_mae: 0.2263\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2271 - val_mae: 0.2271\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2273 - val_mae: 0.2273\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2394 - mae: 0.2394 - val_loss: 0.2256 - val_mae: 0.2256\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2396 - mae: 0.2396 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2392 - mae: 0.2392 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2273 - val_mae: 0.2273\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2394 - mae: 0.2394 - val_loss: 0.2257 - val_mae: 0.2257\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2271 - val_mae: 0.2271\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2269 - val_mae: 0.2269\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2391 - mae: 0.2391 - val_loss: 0.2274 - val_mae: 0.2274\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2261 - val_mae: 0.2261\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2389 - mae: 0.2389 - val_loss: 0.2266 - val_mae: 0.2266\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2390 - mae: 0.2390 - val_loss: 0.2261 - val_mae: 0.2261\n"
     ]
    }
   ],
   "source": [
    "smod_history_icbp_n3_h4 = simple_model_icbp_n1_h4.fit(X_train_icbp_1, y_train_icbp_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 43.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_icbp_n1_h1 = simple_model_icbp_n1_h1.predict(X_test_icbp_1)\n",
    "preds_icbp_n1_h2 = simple_model_icbp_n1_h2.predict(X_test_icbp_1)\n",
    "preds_icbp_n1_h3= simple_model_icbp_n1_h3.predict(X_test_icbp_1)\n",
    "preds_icbp_n1_h4= simple_model_icbp_n1_h4.predict(X_test_icbp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 1\n",
      "mae score icbp_n1_h1: 0.007086988590841109\n",
      "r2 score icbp_n1_h1: 0.9983233676202956\n",
      "mape score icbp_n1_h1: 0.03267820511245089\n",
      "rmse score icbp_n1_h1: 0.011508974319469607\n",
      "hidden layer 2\n",
      "mae score icbp_n1_h2: 0.014612323625379744\n",
      "r2 score icbp_n1_h2: 0.9958080907798392\n",
      "mape score icbp_n1_h2: 0.043830805202935724\n",
      "rmse score icbp_n1_h2: 0.018640083083157434\n",
      "hidden layer 3\n",
      "mae score icbp_n1_h3: 0.012223985095585319\n",
      "r2 score icbp_n1_h3: 0.9969796337720037\n",
      "mape score icbp_n1_h3: 0.04706924282926418\n",
      "rmse score icbp_n1_h3: 0.015606241078365998\n",
      "hidden layer 4\n",
      "mae score icbp_n1_h4: 0.24899680086672094\n",
      "r2 score icbp_n1_h4: -23672082351204.34\n",
      "mape score icbp_n1_h4: 0.47254809998808467\n",
      "rmse score icbp_n1_h4: 0.29000022547373844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 1\")\n",
    "print(\"mae score icbp_n1_h1: \"+str(mean_absolute_error(preds_icbp_n1_h1, y_test_icbp_1)))\n",
    "print(\"r2 score icbp_n1_h1: \"+str(r2_score(preds_icbp_n1_h1, y_test_icbp_1)))\n",
    "print(\"mape score icbp_n1_h1: \"+str(mean_absolute_percentage_error(preds_icbp_n1_h1, y_test_icbp_1)))\n",
    "print(\"rmse score icbp_n1_h1: \"+str(np.sqrt(mean_squared_error(preds_icbp_n1_h1, y_test_icbp_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 2\")\n",
    "print(\"mae score icbp_n1_h2: \"+str(mean_absolute_error(preds_icbp_n1_h2, y_test_icbp_1)))\n",
    "print(\"r2 score icbp_n1_h2: \"+str(r2_score(preds_icbp_n1_h2, y_test_icbp_1)))\n",
    "print(\"mape score icbp_n1_h2: \"+str(mean_absolute_percentage_error(preds_icbp_n1_h2, y_test_icbp_1)))\n",
    "print(\"rmse score icbp_n1_h2: \"+str(np.sqrt(mean_squared_error(preds_icbp_n1_h2, y_test_icbp_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 3\")\n",
    "print(\"mae score icbp_n1_h3: \"+str(mean_absolute_error(preds_icbp_n1_h3, y_test_icbp_1)))\n",
    "print(\"r2 score icbp_n1_h3: \"+str(r2_score(preds_icbp_n1_h3, y_test_icbp_1)))\n",
    "print(\"mape score icbp_n1_h3: \"+str(mean_absolute_percentage_error(preds_icbp_n1_h3, y_test_icbp_1)))\n",
    "print(\"rmse score icbp_n1_h3: \"+str(np.sqrt(mean_squared_error(preds_icbp_n1_h3, y_test_icbp_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 4\")\n",
    "print(\"mae score icbp_n1_h4: \"+str(mean_absolute_error(preds_icbp_n1_h4, y_test_icbp_1)))\n",
    "print(\"r2 score icbp_n1_h4: \"+str(r2_score(preds_icbp_n1_h4, y_test_icbp_1)))\n",
    "print(\"mape score icbp_n1_h4: \"+str(mean_absolute_percentage_error(preds_icbp_n1_h4, y_test_icbp_1)))\n",
    "print(\"rmse score icbp_n1_h4: \"+str(np.sqrt(mean_squared_error(preds_icbp_n1_h4, y_test_icbp_1))))\n",
    "\n",
    "mae_icbp_hl = {'hidden layer_n1_h1':mean_absolute_error(preds_icbp_n1_h1, y_test_icbp_1),'hidden layer_n1_h2':mean_absolute_error(preds_icbp_n1_h2, y_test_icbp_1),'hidden layer_n1_h3':mean_absolute_error(preds_icbp_n1_h3, y_test_icbp_1),'hidden layer_n1_h4':mean_absolute_error(preds_icbp_n1_h4, y_test_icbp_1)}\n",
    "\n",
    "mape_icbp_hl = {'hidden layer_n1_h1':mean_absolute_percentage_error(preds_icbp_n1_h1, y_test_icbp_1),'hidden layer_n1_h2':mean_absolute_percentage_error(preds_icbp_n1_h2, y_test_icbp_1),'hidden layer_n1_h3':mean_absolute_percentage_error(preds_icbp_n1_h3, y_test_icbp_1),'hidden layer_n1_h4':mean_absolute_percentage_error(preds_icbp_n1_h4, y_test_icbp_1)}\n",
    "\n",
    "rmse_icbp_hl = {'hidden layer_n1_h1':np.sqrt(mean_squared_error(preds_icbp_n1_h1, y_test_icbp_1)),'hidden layer_n1_h2':np.sqrt(mean_squared_error(preds_icbp_n1_h2, y_test_icbp_1)),'hidden layer_n1_h3':np.sqrt(mean_squared_error(preds_icbp_n1_h3, y_test_icbp_1)),'hidden layer_n1_h4':np.sqrt(mean_squared_error(preds_icbp_n1_h4, y_test_icbp_1))}\n",
    "\n",
    "r2_icbp_hl = {'hidden layer_n1_h1':r2_score(preds_icbp_n1_h1, y_test_icbp_1),'hidden layer_n1_h2':r2_score(preds_icbp_n1_h2, y_test_icbp_1),'hidden layer_n1_h3':r2_score(preds_icbp_n1_h3, y_test_icbp_1),'hidden layer_n1_h4':r2_score(preds_icbp_n1_h4, y_test_icbp_1)} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 1\n",
    "# mae score icbp_n1_h1: 0.007086988590841109\n",
    "# r2 score icbp_n1_h1: 0.9983233676202956\n",
    "# mape score icbp_n1_h1: 0.03267820511245089\n",
    "# rmse score icbp_n1_h1: 0.011508974319469607\n",
    "# hidden layer 2\n",
    "# mae score icbp_n1_h2: 0.014612323625379744\n",
    "# r2 score icbp_n1_h2: 0.9958080907798392\n",
    "# mape score icbp_n1_h2: 0.043830805202935724\n",
    "# rmse score icbp_n1_h2: 0.018640083083157434\n",
    "# hidden layer 3\n",
    "# mae score icbp_n1_h3: 0.012223985095585319\n",
    "# r2 score icbp_n1_h3: 0.9969796337720037\n",
    "# mape score icbp_n1_h3: 0.04706924282926418\n",
    "# rmse score icbp_n1_h3: 0.015606241078365998\n",
    "# hidden layer 4\n",
    "# mae score icbp_n1_h4: 0.24899680086672094\n",
    "# r2 score icbp_n1_h4: -23672082351204.34\n",
    "# mape score icbp_n1_h4: 0.47254809998808467\n",
    "# rmse score icbp_n1_h4: 0.29000022547373844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'hidden layer_n1_h1': 0.007086988590841109, 'hidden layer_n1_h3': 0.012223985095585319, 'hidden layer_n1_h2': 0.014612323625379744, 'hidden layer_n1_h4': 0.24899680086672094}\n",
      "sorted rmse\n",
      "{'hidden layer_n1_h1': 0.011508974319469607, 'hidden layer_n1_h3': 0.015606241078365998, 'hidden layer_n1_h2': 0.018640083083157434, 'hidden layer_n1_h4': 0.29000022547373844}\n",
      "sorted mape\n",
      "{'hidden layer_n1_h1': 0.03267820511245089, 'hidden layer_n1_h2': 0.043830805202935724, 'hidden layer_n1_h3': 0.04706924282926418, 'hidden layer_n1_h4': 0.47254809998808467}\n",
      "sorted r2\n",
      "{'hidden layer_n1_h1': 0.9983233676202956, 'hidden layer_n1_h3': 0.9969796337720037, 'hidden layer_n1_h2': 0.9958080907798392, 'hidden layer_n1_h4': -23672082351204.34}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_icbp_hl_sorted = dict(sorted(mae_icbp_hl.items(),key=lambda item: item[1]))\n",
    "print(mae_icbp_hl_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_icbp_hl_sorted = dict(sorted(rmse_icbp_hl.items(),key=lambda item: item[1]))\n",
    "print(rmse_icbp_hl_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_icbp_hl_sorted = dict(sorted(mape_icbp_hl.items(),key=lambda item: item[1]))\n",
    "print(mape_icbp_hl_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_icbp_hl_sorted = dict(sorted(r2_icbp_hl.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_icbp_hl_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted mae\n",
    "# {'hidden layer_n1_h1': 0.007086988590841109, 'hidden layer_n1_h3': 0.012223985095585319, 'hidden layer_n1_h2': 0.014612323625379744, 'hidden layer_n1_h4': 0.24899680086672094}\n",
    "# sorted rmse\n",
    "# {'hidden layer_n1_h1': 0.011508974319469607, 'hidden layer_n1_h3': 0.015606241078365998, 'hidden layer_n1_h2': 0.018640083083157434, 'hidden layer_n1_h4': 0.29000022547373844}\n",
    "# sorted mape\n",
    "# {'hidden layer_n1_h1': 0.03267820511245089, 'hidden layer_n1_h2': 0.043830805202935724, 'hidden layer_n1_h3': 0.04706924282926418, 'hidden layer_n1_h4': 0.47254809998808467}\n",
    "# sorted r2\n",
    "# {'hidden layer_n1_h1': 0.9983233676202956, 'hidden layer_n1_h3': 0.9969796337720037, 'hidden layer_n1_h2': 0.9958080907798392, 'hidden layer_n1_h4': -23672082351204.34}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_129 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 329\n",
      "Trainable params: 329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_jsmr_n1_h1 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features)),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_n1_h1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_jsmr_n1_h1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_130 (LSTM)             (None, 1, 8)              320       \n",
      "                                                                 \n",
      " lstm_131 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_jsmr_n1_h2 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_n1_h2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_jsmr_n1_h2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_132 (LSTM)             (None, 1, 8)              320       \n",
      "                                                                 \n",
      " lstm_133 (LSTM)             (None, 1, 8)              544       \n",
      "                                                                 \n",
      " lstm_134 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,417\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_jsmr_n1_h3 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_n1_h3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_jsmr_n1_h3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_135 (LSTM)             (None, 1, 8)              320       \n",
      "                                                                 \n",
      " lstm_136 (LSTM)             (None, 1, 8)              544       \n",
      "                                                                 \n",
      " lstm_137 (LSTM)             (None, 1, 8)              544       \n",
      "                                                                 \n",
      " lstm_138 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,961\n",
      "Trainable params: 1,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_jsmr_n1_h4 = Sequential([\n",
    "  LSTM(8, activation='relu',input_shape=(n_steps_1, n_features),return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu',return_sequences=True),\n",
    "  LSTM(8, activation='relu'),\n",
    "  Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "simple_model_jsmr_n1_h4.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "simple_model_jsmr_n1_h4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 2s 10ms/step - loss: 0.2940 - mae: 0.2940 - val_loss: 0.1198 - val_mae: 0.1198\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0997 - mae: 0.0997 - val_loss: 0.0690 - val_mae: 0.0690\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0445 - mae: 0.0445 - val_loss: 0.0199 - val_mae: 0.0199\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0212 - mae: 0.0212 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0208 - val_mae: 0.0208\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0206 - mae: 0.0206 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0191 - mae: 0.0191 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0147 - mae: 0.014 - 0s 5ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0124 - mae: 0.0124 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0126 - mae: 0.0126 - val_loss: 0.0119 - val_mae: 0.0119\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_n1_h1 = simple_model_jsmr_n1_h1.fit(X_train_jsmr_1, y_train_jsmr_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 27.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 6s 21ms/step - loss: 0.2988 - mae: 0.2988 - val_loss: 0.1345 - val_mae: 0.1345\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1139 - mae: 0.1139 - val_loss: 0.0566 - val_mae: 0.0566\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0399 - mae: 0.0399 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0342 - mae: 0.0342 - val_loss: 0.0345 - val_mae: 0.0345\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0324 - mae: 0.0324 - val_loss: 0.0351 - val_mae: 0.0351\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0314 - mae: 0.0314 - val_loss: 0.0264 - val_mae: 0.0264\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0287 - val_mae: 0.0287\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0291 - val_mae: 0.0291\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0275 - mae: 0.0275 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0260 - mae: 0.0260 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0209 - val_mae: 0.0209\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0207 - mae: 0.0207 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0128 - mae: 0.012 - 0s 5ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0183 - val_mae: 0.0183\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0205 - val_mae: 0.0205\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0129 - mae: 0.0129 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0134 - mae: 0.013 - 0s 5ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - mae: 0.0125 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0118 - val_mae: 0.0118\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0253 - val_mae: 0.0253\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0119 - val_mae: 0.0119\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0242 - val_mae: 0.0242\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_n1_h2 = simple_model_jsmr_n1_h2.fit(X_train_jsmr_1, y_train_jsmr_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 31.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 4s 15ms/step - loss: 0.2793 - mae: 0.2793 - val_loss: 0.1366 - val_mae: 0.1366\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0847 - mae: 0.0847 - val_loss: 0.0441 - val_mae: 0.0441\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.0424 - val_loss: 0.0445 - val_mae: 0.0445\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0328 - mae: 0.0328 - val_loss: 0.0256 - val_mae: 0.0256\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0288 - mae: 0.0288 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0210 - val_mae: 0.0210\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0193 - val_mae: 0.0193\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0370 - val_mae: 0.0370\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0287 - mae: 0.0287 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0425 - val_mae: 0.0425\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0266 - val_mae: 0.0266\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0195 - mae: 0.0195 - val_loss: 0.0265 - val_mae: 0.0265\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0270 - val_mae: 0.0270\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0249 - val_mae: 0.0249\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0270 - val_mae: 0.0270\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0202 - val_mae: 0.0202\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0251 - val_mae: 0.0251\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0236 - val_mae: 0.0236\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0276 - val_mae: 0.0276\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0188 - val_mae: 0.0188\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0121 - val_mae: 0.0121\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0120 - val_mae: 0.0120\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_n1_h3 = simple_model_jsmr_n1_h3.fit(X_train_jsmr_1, y_train_jsmr_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 30.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 7s 25ms/step - loss: 0.2681 - mae: 0.2681 - val_loss: 0.1499 - val_mae: 0.1499\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0957 - mae: 0.0957 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0456 - mae: 0.0456 - val_loss: 0.0571 - val_mae: 0.0571\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0361 - mae: 0.0361 - val_loss: 0.0429 - val_mae: 0.0429\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0322 - mae: 0.0322 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0203 - mae: 0.0203 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0209 - val_mae: 0.0209\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0260 - val_mae: 0.0260\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0298 - val_mae: 0.0298\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0257 - val_mae: 0.0257\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0335 - val_mae: 0.0335\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0191 - mae: 0.0191 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0198 - val_mae: 0.0198\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0202 - mae: 0.0202 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0206 - mae: 0.0206 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0209 - mae: 0.0209 - val_loss: 0.0249 - val_mae: 0.0249\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0123 - val_mae: 0.0123\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0271 - val_mae: 0.0271\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0214 - mae: 0.0214 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0209 - val_mae: 0.0209\n",
      "Epoch 62/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0131 - val_mae: 0.0131\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0280 - val_mae: 0.0280\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0194 - mae: 0.0194 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - mae: 0.0180 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0132 - val_mae: 0.0132\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0126 - val_mae: 0.0126\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0205 - val_mae: 0.0205\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0208 - val_mae: 0.0208\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0124 - val_mae: 0.0124\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0195 - val_mae: 0.0195\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0122 - val_mae: 0.0122\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0120 - val_mae: 0.0120\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0191 - mae: 0.0191 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0127 - val_mae: 0.0127\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0327 - val_mae: 0.0327\n"
     ]
    }
   ],
   "source": [
    "smod_history_jsmr_n1_h4 = simple_model_jsmr_n1_h4.fit(X_train_jsmr_1, y_train_jsmr_1,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "#time 43.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_jsmr_n1_h1 = simple_model_jsmr_n1_h1.predict(X_test_jsmr_1)\n",
    "preds_jsmr_n1_h2 = simple_model_jsmr_n1_h2.predict(X_test_jsmr_1)\n",
    "preds_jsmr_n1_h3= simple_model_jsmr_n1_h3.predict(X_test_jsmr_1)\n",
    "preds_jsmr_n1_h4= simple_model_jsmr_n1_h4.predict(X_test_jsmr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 1\n",
      "mae score jsmr_n1_h1: 0.011318812825305483\n",
      "r2 score jsmr_n1_h1: 0.9943690684509984\n",
      "mape score jsmr_n1_h1: 0.03565450266272184\n",
      "rmse score jsmr_n1_h1: 0.016052783071759058\n",
      "hidden layer 2\n",
      "mae score jsmr_n1_h2: 0.023105861713602516\n",
      "r2 score jsmr_n1_h2: 0.984495483254619\n",
      "mape score jsmr_n1_h2: 0.05353211593323678\n",
      "rmse score jsmr_n1_h2: 0.02702812409054237\n",
      "hidden layer 3\n",
      "mae score jsmr_n1_h3: 0.01157825894564719\n",
      "r2 score jsmr_n1_h3: 0.9941938767627192\n",
      "mape score jsmr_n1_h3: 0.03242430534782007\n",
      "rmse score jsmr_n1_h3: 0.016230213939194986\n",
      "hidden layer 4\n",
      "mae score jsmr_n1_h4: 0.032494819793814744\n",
      "r2 score jsmr_n1_h4: 0.965350130015121\n",
      "mape score jsmr_n1_h4: 0.06866716078656558\n",
      "rmse score jsmr_n1_h4: 0.0374025859927203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 1\")\n",
    "print(\"mae score jsmr_n1_h1: \"+str(mean_absolute_error(preds_jsmr_n1_h1, y_test_jsmr_1)))\n",
    "print(\"r2 score jsmr_n1_h1: \"+str(r2_score(preds_jsmr_n1_h1, y_test_jsmr_1)))\n",
    "print(\"mape score jsmr_n1_h1: \"+str(mean_absolute_percentage_error(preds_jsmr_n1_h1, y_test_jsmr_1)))\n",
    "print(\"rmse score jsmr_n1_h1: \"+str(np.sqrt(mean_squared_error(preds_jsmr_n1_h1, y_test_jsmr_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 2\")\n",
    "print(\"mae score jsmr_n1_h2: \"+str(mean_absolute_error(preds_jsmr_n1_h2, y_test_jsmr_1)))\n",
    "print(\"r2 score jsmr_n1_h2: \"+str(r2_score(preds_jsmr_n1_h2, y_test_jsmr_1)))\n",
    "print(\"mape score jsmr_n1_h2: \"+str(mean_absolute_percentage_error(preds_jsmr_n1_h2, y_test_jsmr_1)))\n",
    "print(\"rmse score jsmr_n1_h2: \"+str(np.sqrt(mean_squared_error(preds_jsmr_n1_h2, y_test_jsmr_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 3\")\n",
    "print(\"mae score jsmr_n1_h3: \"+str(mean_absolute_error(preds_jsmr_n1_h3, y_test_jsmr_1)))\n",
    "print(\"r2 score jsmr_n1_h3: \"+str(r2_score(preds_jsmr_n1_h3, y_test_jsmr_1)))\n",
    "print(\"mape score jsmr_n1_h3: \"+str(mean_absolute_percentage_error(preds_jsmr_n1_h3, y_test_jsmr_1)))\n",
    "print(\"rmse score jsmr_n1_h3: \"+str(np.sqrt(mean_squared_error(preds_jsmr_n1_h3, y_test_jsmr_1))))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"hidden layer 4\")\n",
    "print(\"mae score jsmr_n1_h4: \"+str(mean_absolute_error(preds_jsmr_n1_h4, y_test_jsmr_1)))\n",
    "print(\"r2 score jsmr_n1_h4: \"+str(r2_score(preds_jsmr_n1_h4, y_test_jsmr_1)))\n",
    "print(\"mape score jsmr_n1_h4: \"+str(mean_absolute_percentage_error(preds_jsmr_n1_h4, y_test_jsmr_1)))\n",
    "print(\"rmse score jsmr_n1_h4: \"+str(np.sqrt(mean_squared_error(preds_jsmr_n1_h4, y_test_jsmr_1))))\n",
    "\n",
    "mae_jsmr_hl = {'hidden layer_n1_h1':mean_absolute_error(preds_jsmr_n1_h1, y_test_jsmr_1),'hidden layer_n1_h2':mean_absolute_error(preds_jsmr_n1_h2, y_test_jsmr_1),'hidden layer_n1_h3':mean_absolute_error(preds_jsmr_n1_h3, y_test_jsmr_1),'hidden layer_n1_h4':mean_absolute_error(preds_jsmr_n1_h4, y_test_jsmr_1)}\n",
    "\n",
    "mape_jsmr_hl = {'hidden layer_n1_h1':mean_absolute_percentage_error(preds_jsmr_n1_h1, y_test_jsmr_1),'hidden layer_n1_h2':mean_absolute_percentage_error(preds_jsmr_n1_h2, y_test_jsmr_1),'hidden layer_n1_h3':mean_absolute_percentage_error(preds_jsmr_n1_h3, y_test_jsmr_1),'hidden layer_n1_h4':mean_absolute_percentage_error(preds_jsmr_n1_h4, y_test_jsmr_1)}\n",
    "\n",
    "rmse_jsmr_hl = {'hidden layer_n1_h1':np.sqrt(mean_squared_error(preds_jsmr_n1_h1, y_test_jsmr_1)),'hidden layer_n1_h2':np.sqrt(mean_squared_error(preds_jsmr_n1_h2, y_test_jsmr_1)),'hidden layer_n1_h3':np.sqrt(mean_squared_error(preds_jsmr_n1_h3, y_test_jsmr_1)),'hidden layer_n1_h4':np.sqrt(mean_squared_error(preds_jsmr_n1_h4, y_test_jsmr_1))}\n",
    "\n",
    "r2_jsmr_hl = {'hidden layer_n1_h1':r2_score(preds_jsmr_n1_h1, y_test_jsmr_1),'hidden layer_n1_h2':r2_score(preds_jsmr_n1_h2, y_test_jsmr_1),'hidden layer_n1_h3':r2_score(preds_jsmr_n1_h3, y_test_jsmr_1),'hidden layer_n1_h4':r2_score(preds_jsmr_n1_h4, y_test_jsmr_1)} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted mae\n",
      "{'hidden layer_n1_h1': 0.011318812825305483, 'hidden layer_n1_h3': 0.01157825894564719, 'hidden layer_n1_h2': 0.023105861713602516, 'hidden layer_n1_h4': 0.032494819793814744}\n",
      "sorted rmse\n",
      "{'hidden layer_n1_h1': 0.016052783071759058, 'hidden layer_n1_h3': 0.016230213939194986, 'hidden layer_n1_h2': 0.02702812409054237, 'hidden layer_n1_h4': 0.0374025859927203}\n",
      "sorted mape\n",
      "{'hidden layer_n1_h3': 0.03242430534782007, 'hidden layer_n1_h1': 0.03565450266272184, 'hidden layer_n1_h2': 0.05353211593323678, 'hidden layer_n1_h4': 0.06866716078656558}\n",
      "sorted r2\n",
      "{'hidden layer_n1_h1': 0.9943690684509984, 'hidden layer_n1_h3': 0.9941938767627192, 'hidden layer_n1_h2': 0.984495483254619, 'hidden layer_n1_h4': 0.965350130015121}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted mae\")\n",
    "mae_jsmr_hl_sorted = dict(sorted(mae_jsmr_hl.items(),key=lambda item: item[1]))\n",
    "print(mae_jsmr_hl_sorted)\n",
    "print(\"sorted rmse\")\n",
    "rmse_jsmr_hl_sorted = dict(sorted(rmse_jsmr_hl.items(),key=lambda item: item[1]))\n",
    "print(rmse_jsmr_hl_sorted)\n",
    "print(\"sorted mape\")\n",
    "mape_jsmr_hl_sorted = dict(sorted(mape_jsmr_hl.items(),key=lambda item: item[1]))\n",
    "print(mape_jsmr_hl_sorted)\n",
    "print(\"sorted r2\")\n",
    "r2_jsmr_hl_sorted = dict(sorted(r2_jsmr_hl.items(),key=lambda item: item[1] ,reverse=True))\n",
    "print(r2_jsmr_hl_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURON/Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train test split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train_antm_3, X_test_antm_3, y_train_antm_3, y_test_antm_3 = train_test_split(antm_X_3, antm_y_3, test_size=0.33, random_state=42)\n",
    "# X_train_antm_7, X_test_antm_7, y_train_antm_7, y_test_antm_7 = train_test_split(antm_X_7, antm_y_7, test_size=0.33, random_state=42)\n",
    "# X_train_antm_20, X_test_antm_20, y_train_antm_20, y_test_antm_20 = train_test_split(antm_X_20, antm_y_20, test_size=0.33, random_state=42)\n",
    "# X_train_antm_30, X_test_antm_30, y_train_antm_30, y_test_antm_30 = train_test_split(antm_X_30, antm_y_30, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from sklearn.metrics import r2_score\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM\n",
    "# random.seed(42)\n",
    "\n",
    "# batch_size = 32\n",
    "# simple_model_one_antm = Sequential([\n",
    "#   LSTM(8, activation='relu',input_shape=(n_steps, n_features)),\n",
    "#   Dense(1),\n",
    "# ])\n",
    "\n",
    "\n",
    "# simple_model_one_antm.compile(\n",
    "#   optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "#   loss='mean_absolute_error',\n",
    "#   metrics=['mae'],\n",
    "# )\n",
    "\n",
    "# simple_model_one_antm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smod_history_one_antm = simple_model_one_antm.fit(X_train_antm, y_train_antm,\n",
    "#           validation_split=0.2,\n",
    "#           epochs=100,\n",
    "#           batch_size=batch_size,\n",
    "#           shuffle = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_one_antm = simple_model_one_antm.predict(X_test_antm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# print(\"mape score antm: \"+str(mean_absolute_error(preds_one_antm, y_test_antm)))\n",
    "# print(\"mape score antm: \"+str(r2_score(preds_one_antm, y_test_antm)))\n",
    "# print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_one_antm, y_test_antm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_one_antm = scaler.inverse_transform(preds_one_antm)\n",
    "# preds_one_antm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3816365fdcd687a07caedfe721e5894fb1dd0a24482efb967fc5a605423a1021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
