{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import date, datetime\n",
    "from multiprocessing.spawn import import_main_path\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Attributes</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Adj Close</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Close</th>\n",
       "      <th colspan=\"2\" halign=\"left\">High</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Low</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Open</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symbols</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>...</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>1576.277466</td>\n",
       "      <td>2454.149170</td>\n",
       "      <td>1443.428467</td>\n",
       "      <td>1530.937988</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1837.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1762.5</td>\n",
       "      <td>1805.770874</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1787.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>39619544.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>26442000.0</td>\n",
       "      <td>6978806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1576.277466</td>\n",
       "      <td>2468.053711</td>\n",
       "      <td>1472.885986</td>\n",
       "      <td>1547.488525</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>62041590.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42874000.0</td>\n",
       "      <td>7988164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1576.277466</td>\n",
       "      <td>2454.149170</td>\n",
       "      <td>1463.066772</td>\n",
       "      <td>1555.763916</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1875.607300</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1855.654053</td>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1875.607300</td>\n",
       "      <td>30916328.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44946000.0</td>\n",
       "      <td>7538113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1610.544067</td>\n",
       "      <td>2377.674316</td>\n",
       "      <td>1472.885986</td>\n",
       "      <td>1547.488525</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "      <td>30624653.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24863000.0</td>\n",
       "      <td>2048787.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1610.544067</td>\n",
       "      <td>2391.578613</td>\n",
       "      <td>1482.705200</td>\n",
       "      <td>1539.213379</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>1887.5</td>\n",
       "      <td>1855.654053</td>\n",
       "      <td>1994.945068</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1845.677368</td>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1855.654053</td>\n",
       "      <td>15857579.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19118000.0</td>\n",
       "      <td>2441705.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Attributes    Adj Close                                               Close  \\\n",
       "Symbols         ANTM.JK      ASII.JK      ICBP.JK      JSMR.JK      ANTM.JK   \n",
       "Date                                                                          \n",
       "2010-01-04  1576.277466  2454.149170  1443.428467  1530.937988  1931.946777   \n",
       "2010-01-05  1576.277466  2468.053711  1472.885986  1547.488525  1931.946777   \n",
       "2010-01-06  1576.277466  2454.149170  1463.066772  1555.763916  1931.946777   \n",
       "2010-01-07  1610.544067  2377.674316  1472.885986  1547.488525  1973.945557   \n",
       "2010-01-08  1610.544067  2391.578613  1482.705200  1539.213379  1973.945557   \n",
       "\n",
       "Attributes                                      High          ...     Low  \\\n",
       "Symbols    ASII.JK ICBP.JK      JSMR.JK      ANTM.JK ASII.JK  ... ICBP.JK   \n",
       "Date                                                          ...           \n",
       "2010-01-04  3530.0  1837.5  1845.677368  1931.946777  3550.0  ...  1762.5   \n",
       "2010-01-05  3550.0  1875.0  1865.630737  1994.945068  3570.0  ...  1825.0   \n",
       "2010-01-06  3530.0  1862.5  1875.607300  1994.945068  3580.0  ...  1862.5   \n",
       "2010-01-07  3420.0  1875.0  1865.630737  1994.945068  3560.0  ...  1862.5   \n",
       "2010-01-08  3440.0  1887.5  1855.654053  1994.945068  3450.0  ...  1862.5   \n",
       "\n",
       "Attributes                      Open                                   Volume  \\\n",
       "Symbols         JSMR.JK      ANTM.JK ASII.JK ICBP.JK      JSMR.JK     ANTM.JK   \n",
       "Date                                                                            \n",
       "2010-01-04  1805.770874  1931.946777  3530.0  1787.5  1845.677368  39619544.0   \n",
       "2010-01-05  1845.677368  1931.946777  3550.0  1875.0  1865.630737  62041590.0   \n",
       "2010-01-06  1855.654053  1931.946777  3530.0  1900.0  1875.607300  30916328.0   \n",
       "2010-01-07  1845.677368  1973.945557  3420.0  1875.0  1865.630737  30624653.0   \n",
       "2010-01-08  1845.677368  1973.945557  3440.0  1900.0  1855.654053  15857579.0   \n",
       "\n",
       "Attributes                                 \n",
       "Symbols    ASII.JK     ICBP.JK    JSMR.JK  \n",
       "Date                                       \n",
       "2010-01-04    40.0  26442000.0  6978806.0  \n",
       "2010-01-05    40.0  42874000.0  7988164.0  \n",
       "2010-01-06    40.0  44946000.0  7538113.0  \n",
       "2010-01-07    40.0  24863000.0  2048787.0  \n",
       "2010-01-08    40.0  19118000.0  2441705.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = [\"ANTM.JK\",\"ASII.JK\",\"ICBP.JK\",\"JSMR.JK\"]\n",
    "\n",
    "today = date.today()\n",
    "# print(today)\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2022-06-27'\n",
    "\n",
    "panel_data = data.DataReader(tickers,'yahoo',start_date, end_date)\n",
    "panel_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Symbols</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>1931.95</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1837.5</td>\n",
       "      <td>1845.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1931.95</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1931.95</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1875.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1973.95</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1973.95</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>1887.5</td>\n",
       "      <td>1855.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Symbols     ANTM.JK  ASII.JK  ICBP.JK  JSMR.JK\n",
       "Date                                          \n",
       "2010-01-04  1931.95   3530.0   1837.5  1845.68\n",
       "2010-01-05  1931.95   3550.0   1875.0  1865.63\n",
       "2010-01-06  1931.95   3530.0   1862.5  1875.61\n",
       "2010-01-07  1973.95   3420.0   1875.0  1865.63\n",
       "2010-01-08  1973.95   3440.0   1887.5  1855.65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_close = panel_data[\"Close\"]\n",
    "data_close.head(5).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Symbols</th>\n",
       "      <th>ANTM.JK</th>\n",
       "      <th>ASII.JK</th>\n",
       "      <th>ICBP.JK</th>\n",
       "      <th>JSMR.JK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3091.00</td>\n",
       "      <td>3091.00</td>\n",
       "      <td>3091.00</td>\n",
       "      <td>3091.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1184.22</td>\n",
       "      <td>6690.25</td>\n",
       "      <td>6757.51</td>\n",
       "      <td>4781.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>606.62</td>\n",
       "      <td>1199.57</td>\n",
       "      <td>2857.67</td>\n",
       "      <td>1180.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>287.00</td>\n",
       "      <td>3280.00</td>\n",
       "      <td>1675.00</td>\n",
       "      <td>1686.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>760.09</td>\n",
       "      <td>5850.00</td>\n",
       "      <td>4050.00</td>\n",
       "      <td>3965.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>957.57</td>\n",
       "      <td>6900.00</td>\n",
       "      <td>7562.50</td>\n",
       "      <td>4863.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1612.76</td>\n",
       "      <td>7575.00</td>\n",
       "      <td>8850.00</td>\n",
       "      <td>5686.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3190.00</td>\n",
       "      <td>9150.00</td>\n",
       "      <td>12400.00</td>\n",
       "      <td>7183.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Symbols  ANTM.JK  ASII.JK   ICBP.JK  JSMR.JK\n",
       "count    3091.00  3091.00   3091.00  3091.00\n",
       "mean     1184.22  6690.25   6757.51  4781.62\n",
       "std       606.62  1199.57   2857.67  1180.73\n",
       "min       287.00  3280.00   1675.00  1686.05\n",
       "25%       760.09  5850.00   4050.00  3965.71\n",
       "50%       957.57  6900.00   7562.50  4863.61\n",
       "75%      1612.76  7575.00   8850.00  5686.68\n",
       "max      3190.00  9150.00  12400.00  7183.18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_close.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2614b659640>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot everything by leveraging the very powerful matplotlib package\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "for name in tickers:\n",
    "    ax.plot(data_close[name].index, data_close[name], label='Saham {}'.format(name))\n",
    "\n",
    "\n",
    "# Define the date format\n",
    "date_form = DateFormatter(\"%b-%y\")\n",
    "ax.xaxis.set_major_formatter(date_form)\n",
    "\n",
    "ax.set_title('Harga saham ')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Closing price (Rp)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antm</th>\n",
       "      <th>asii</th>\n",
       "      <th>icbp</th>\n",
       "      <th>jsmr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1837.5</td>\n",
       "      <td>1845.677368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1931.946777</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>1862.5</td>\n",
       "      <td>1875.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1865.630737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1973.945557</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>1887.5</td>\n",
       "      <td>1855.654053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-21</th>\n",
       "      <td>2150.000000</td>\n",
       "      <td>6850.0</td>\n",
       "      <td>9150.0</td>\n",
       "      <td>3690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-22</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>9225.0</td>\n",
       "      <td>3670.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-23</th>\n",
       "      <td>1965.000000</td>\n",
       "      <td>6875.0</td>\n",
       "      <td>9325.0</td>\n",
       "      <td>3660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-24</th>\n",
       "      <td>1970.000000</td>\n",
       "      <td>6675.0</td>\n",
       "      <td>9425.0</td>\n",
       "      <td>3700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27</th>\n",
       "      <td>1890.000000</td>\n",
       "      <td>6650.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>3740.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3091 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   antm    asii    icbp         jsmr\n",
       "Date                                                \n",
       "2010-01-04  1931.946777  3530.0  1837.5  1845.677368\n",
       "2010-01-05  1931.946777  3550.0  1875.0  1865.630737\n",
       "2010-01-06  1931.946777  3530.0  1862.5  1875.607300\n",
       "2010-01-07  1973.945557  3420.0  1875.0  1865.630737\n",
       "2010-01-08  1973.945557  3440.0  1887.5  1855.654053\n",
       "...                 ...     ...     ...          ...\n",
       "2022-06-21  2150.000000  6850.0  9150.0  3690.000000\n",
       "2022-06-22  2020.000000  6800.0  9225.0  3670.000000\n",
       "2022-06-23  1965.000000  6875.0  9325.0  3660.000000\n",
       "2022-06-24  1970.000000  6675.0  9425.0  3700.000000\n",
       "2022-06-27  1890.000000  6650.0  9500.0  3740.000000\n",
       "\n",
       "[3091 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_close)\n",
    "data_close.columns = ['antm', 'asii', 'icbp', 'jsmr']\n",
    "data_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMALIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_antm = pd.DataFrame(data_close.antm)\n",
    "df_asii = pd.DataFrame(data_close.asii)\n",
    "df_icbp = pd.DataFrame(data_close.icbp)\n",
    "df_jsmr = pd.DataFrame(data_close.jsmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_antm = pd.DataFrame(scaler.fit_transform(df_antm), columns = ['antm'])\n",
    "df_asii = pd.DataFrame(scaler.fit_transform(df_asii), columns = ['asii'])\n",
    "df_icbp = pd.DataFrame(scaler.fit_transform(df_icbp), columns = ['icbp'])\n",
    "df_jsmr = pd.DataFrame(scaler.fit_transform(df_jsmr), columns = ['jsmr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to windowed data sets\n",
    "ylist_antm = list(df_antm['antm'])\n",
    "ylist_asii = list(df_asii['asii'])\n",
    "ylist_icbp = list(df_icbp['icbp'])\n",
    "ylist_jsmr = list(df_jsmr['jsmr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAGS and PERIOD\n",
    "n_future = 6\n",
    "n_past = 3*6\n",
    "total_period = 4*6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARAING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_end_antm = len(ylist_antm)\n",
    "idx_start_antm = idx_end_antm - total_period\n",
    "\n",
    "X_new_antm = []\n",
    "y_new_antm = []\n",
    "\n",
    "while idx_start_antm > 0:\n",
    "  x_line_antm = ylist_antm[idx_start_antm:idx_start_antm+n_past]\n",
    "  y_line_antm = ylist_antm[idx_start_antm+n_past:idx_start_antm+total_period]\n",
    "\n",
    "  X_new_antm.append(x_line_antm)\n",
    "  y_new_antm.append(y_line_antm)\n",
    "\n",
    "  idx_start_antm = idx_start_antm - 1\n",
    "\n",
    "X_new_antm = np.array(X_new_antm)\n",
    "y_new_antm = np.array(y_new_antm)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_antm, X_test_antm, y_train_antm, y_test_antm = train_test_split(X_new_antm, y_new_antm, test_size=0.33, random_state=42)\n",
    "\n",
    "# reshape data into the right format for RNNs\n",
    "n_samples = X_train_antm.shape[0]\n",
    "n_timesteps = X_train_antm.shape[1]\n",
    "n_steps = y_train_antm.shape[1]\n",
    "n_features = 1\n",
    "\n",
    "X_train_rs_antm = X_train_antm.reshape(n_samples, n_timesteps, n_features )\n",
    "X_test_rs_antm = X_test_antm.reshape(X_test_antm.shape[0], n_timesteps, n_features )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_end_asii = len(ylist_asii)\n",
    "idx_start_asii = idx_end_asii - total_period\n",
    "\n",
    "X_new_asii = []\n",
    "y_new_asii = []\n",
    "\n",
    "while idx_start_asii > 0:\n",
    "  x_line_asii = ylist_asii[idx_start_asii:idx_start_asii+n_past]\n",
    "  y_line_asii = ylist_asii[idx_start_asii+n_past:idx_start_asii+total_period]\n",
    "\n",
    "  X_new_asii.append(x_line_asii)\n",
    "  y_new_asii.append(y_line_asii)\n",
    "\n",
    "  idx_start_asii = idx_start_asii - 1\n",
    "\n",
    "import numpy as np\n",
    "X_new_asii = np.array(X_new_asii)\n",
    "y_new_asii = np.array(y_new_asii)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_asii, X_test_asii, y_train_asii, y_test_asii = train_test_split(X_new_asii, y_new_asii, test_size=0.33, random_state=42)\n",
    "\n",
    "# reshape data into the right format for RNNs\n",
    "n_samples = X_train_asii.shape[0]\n",
    "n_timesteps = X_train_asii.shape[1]\n",
    "n_steps = y_train_asii.shape[1]\n",
    "n_features = 1\n",
    "\n",
    "X_train_rs_asii = X_train_asii.reshape(n_samples, n_timesteps, n_features )\n",
    "X_test_rs_asii = X_test_asii.reshape(X_test_asii.shape[0], n_timesteps, n_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_end_icbp = len(ylist_icbp)\n",
    "idx_start_icbp = idx_end_icbp - total_period\n",
    "\n",
    "X_new_icbp = []\n",
    "y_new_icbp = []\n",
    "\n",
    "while idx_start_icbp > 0:\n",
    "  x_line_icbp = ylist_icbp[idx_start_icbp:idx_start_icbp+n_past]\n",
    "  y_line_icbp = ylist_icbp[idx_start_icbp+n_past:idx_start_icbp+total_period]\n",
    "\n",
    "  X_new_icbp.append(x_line_icbp)\n",
    "  y_new_icbp.append(y_line_icbp)\n",
    "\n",
    "  idx_start_icbp = idx_start_icbp - 1\n",
    "\n",
    "import numpy as np\n",
    "X_new_icbp = np.array(X_new_icbp)\n",
    "y_new_icbp = np.array(y_new_icbp)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_icbp, X_test_icbp, y_train_icbp, y_test_icbp = train_test_split(X_new_icbp, y_new_icbp, test_size=0.33, random_state=42)\n",
    "\n",
    "# reshape data into the right format for RNNs\n",
    "n_samples = X_train_icbp.shape[0]\n",
    "n_timesteps = X_train_icbp.shape[1]\n",
    "n_steps = y_train_icbp.shape[1]\n",
    "n_features = 1\n",
    "\n",
    "X_train_rs_icbp = X_train_icbp.reshape(n_samples, n_timesteps, n_features )\n",
    "X_test_rs_icbp = X_test_icbp.reshape(X_test_icbp.shape[0], n_timesteps, n_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_end_jsmr = len(ylist_jsmr)\n",
    "idx_start_jsmr = idx_end_jsmr - total_period\n",
    "\n",
    "X_new_jsmr = []\n",
    "y_new_jsmr = []\n",
    "\n",
    "while idx_start_jsmr > 0:\n",
    "  x_line_jsmr = ylist_jsmr[idx_start_jsmr:idx_start_jsmr+n_past]\n",
    "  y_line_jsmr = ylist_jsmr[idx_start_jsmr+n_past:idx_start_jsmr+total_period]\n",
    "\n",
    "  X_new_jsmr.append(x_line_jsmr)\n",
    "  y_new_jsmr.append(y_line_jsmr)\n",
    "\n",
    "  idx_start_jsmr = idx_start_jsmr - 1\n",
    "\n",
    "import numpy as np\n",
    "X_new_jsmr = np.array(X_new_jsmr)\n",
    "y_new_jsmr = np.array(y_new_jsmr)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_jsmr, X_test_jsmr, y_train_jsmr, y_test_jsmr = train_test_split(X_new_jsmr, y_new_jsmr, test_size=0.33, random_state=42)\n",
    "\n",
    "# reshape data into the right format for RNNs\n",
    "n_samples = X_train_jsmr.shape[0]\n",
    "n_timesteps = X_train_jsmr.shape[1]\n",
    "n_steps = y_train_jsmr.shape[1]\n",
    "n_features = 1\n",
    "\n",
    "X_train_rs_jsmr = X_train_jsmr.reshape(n_samples, n_timesteps, n_features )\n",
    "X_test_rs_jsmr = X_test_jsmr.reshape(X_test_jsmr.shape[0], n_timesteps, n_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNING PARAMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_240 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 20ms/step - loss: 0.0754 - mae: 0.0754 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0192 - mae: 0.0192 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0193 - val_mae: 0.0193\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm.summary()\n",
    "\n",
    "simple_model_one_antm.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm = simple_model_one_antm.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm = simple_model_one_antm.predict(X_test_rs_antm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_241 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_242 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 918\n",
      "Trainable params: 918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 5s 31ms/step - loss: 0.0807 - mae: 0.0807 - mape: 38540.9141 - mse: 0.0223 - val_loss: 0.0273 - val_mae: 0.0273 - val_mape: 19.6008 - val_mse: 0.0014\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0261 - mae: 0.0261 - mape: 23966.2285 - mse: 0.0016 - val_loss: 0.0229 - val_mae: 0.0229 - val_mape: 20.2084 - val_mse: 0.0010\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0222 - mae: 0.0222 - mape: 13008.6582 - mse: 0.0012 - val_loss: 0.0232 - val_mae: 0.0232 - val_mape: 10.0205 - val_mse: 0.0012\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.0199 - mae: 0.0199 - mape: 12717.6689 - mse: 9.5041e-04 - val_loss: 0.0206 - val_mae: 0.0206 - val_mape: 9.6279 - val_mse: 8.7466e-04\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.0186 - mae: 0.0186 - mape: 9637.5820 - mse: 8.4964e-04 - val_loss: 0.0195 - val_mae: 0.0195 - val_mape: 9.8047 - val_mse: 7.5916e-04\n",
      "0.9747236321126537\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_two_antm = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_two_antm.summary()\n",
    "\n",
    "simple_model_two_antm.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_two_antm = simple_model_two_antm.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_two_antm = simple_model_two_antm.predict(X_test_rs_antm)\n",
    "\n",
    "print(r2_score(preds_two_antm, y_test_antm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_243 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_244 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_245 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,462\n",
      "Trainable params: 1,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 7s 45ms/step - loss: 0.0798 - mae: 0.0798 - mape: 37507.3047 - mse: 0.0183 - val_loss: 0.0378 - val_mae: 0.0378 - val_mape: 22.2452 - val_mse: 0.0025\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 0.0312 - mae: 0.0312 - mape: 19981.5664 - mse: 0.0022 - val_loss: 0.0245 - val_mae: 0.0245 - val_mape: 16.0581 - val_mse: 0.0012\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.0305 - mape: 12498.8594 - mse: 0.0020 - val_loss: 0.0240 - val_mae: 0.0240 - val_mape: 13.9626 - val_mse: 0.0012\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0241 - mae: 0.0241 - mape: 9272.2969 - mse: 0.0014 - val_loss: 0.0244 - val_mae: 0.0244 - val_mape: 18.8833 - val_mse: 0.0011\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 0.0216 - mae: 0.0216 - mape: 10577.3477 - mse: 0.0011 - val_loss: 0.0295 - val_mae: 0.0295 - val_mape: 15.2313 - val_mse: 0.0014\n",
      "0.9661004304389403\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_three_antm = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_three_antm.summary()\n",
    "\n",
    "simple_model_three_antm.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_three_antm = simple_model_three_antm.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_three_antm = simple_model_three_antm.predict(X_test_rs_antm)\n",
    "\n",
    "print(r2_score(preds_three_antm, y_test_antm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_246 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_247 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_248 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_249 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,006\n",
      "Trainable params: 2,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 9s 58ms/step - loss: 0.0828 - mae: 0.0828 - mape: 68315.4375 - mse: 0.0206 - val_loss: 0.0306 - val_mae: 0.0306 - val_mape: 22.4613 - val_mse: 0.0019\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 29ms/step - loss: 0.0358 - mae: 0.0358 - mape: 22921.4551 - mse: 0.0027 - val_loss: 0.0479 - val_mae: 0.0479 - val_mape: 24.4993 - val_mse: 0.0040\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 29ms/step - loss: 0.0331 - mae: 0.0331 - mape: 18089.5566 - mse: 0.0024 - val_loss: 0.0244 - val_mae: 0.0244 - val_mape: 13.2217 - val_mse: 0.0012\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 0.0276 - mae: 0.0276 - mape: 16154.0293 - mse: 0.0018 - val_loss: 0.0250 - val_mae: 0.0250 - val_mape: 21.0246 - val_mse: 0.0011\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 29ms/step - loss: 0.0246 - mae: 0.0246 - mape: 15404.0312 - mse: 0.0014 - val_loss: 0.0209 - val_mae: 0.0209 - val_mape: 10.9991 - val_mse: 8.9627e-04\n",
      "0.9707454251092681\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_four_antm = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_four_antm.summary()\n",
    "\n",
    "simple_model_four_antm.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_four_antm = simple_model_four_antm.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_four_antm = simple_model_four_antm.predict(X_test_rs_antm)\n",
    "\n",
    "print(r2_score(preds_four_antm, y_test_antm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_250 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_251 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_252 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_253 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_254 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,550\n",
      "Trainable params: 2,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 12s 73ms/step - loss: 0.0838 - mae: 0.0838 - mape: 39259.0195 - mse: 0.0193 - val_loss: 0.0393 - val_mae: 0.0393 - val_mape: 24.1412 - val_mse: 0.0027\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.0398 - mae: 0.0398 - mape: 31900.1309 - mse: 0.0033 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 23.6849 - val_mse: 0.0020\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.0317 - mae: 0.0317 - mape: 25667.4434 - mse: 0.0024 - val_loss: 0.0363 - val_mae: 0.0363 - val_mape: 26.3011 - val_mse: 0.0024\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.0355 - mae: 0.0355 - mape: 13139.8652 - mse: 0.0027 - val_loss: 0.0264 - val_mae: 0.0264 - val_mape: 12.5987 - val_mse: 0.0015\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.0287 - mae: 0.0287 - mape: 20974.1758 - mse: 0.0019 - val_loss: 0.0214 - val_mae: 0.0214 - val_mape: 11.5824 - val_mse: 9.6512e-04\n",
      "0.9577206861273039\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_five_antm = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_five_antm.summary()\n",
    "\n",
    "simple_model_five_antm.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_five_antm = simple_model_five_antm.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_five_antm = simple_model_five_antm.predict(X_test_rs_antm)\n",
    "\n",
    "print(r2_score(preds_five_antm, y_test_antm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_255 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.1501 - mae: 0.1501 - mape: 78834.7109 - mse: 0.0544 - val_loss: 0.0485 - val_mae: 0.0485 - val_mape: 114837.6953 - val_mse: 0.0039\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0416 - mae: 0.0416 - mape: 41445.1211 - mse: 0.0028 - val_loss: 0.0413 - val_mae: 0.0413 - val_mape: 100371.8672 - val_mse: 0.0029\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0364 - mae: 0.0364 - mape: 44218.1953 - mse: 0.0023 - val_loss: 0.0381 - val_mae: 0.0381 - val_mape: 102041.9531 - val_mse: 0.0025\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0352 - mae: 0.0352 - mape: 40717.3281 - mse: 0.0022 - val_loss: 0.0342 - val_mae: 0.0342 - val_mape: 87126.0156 - val_mse: 0.0020\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0335 - mae: 0.0335 - mape: 37520.7266 - mse: 0.0020 - val_loss: 0.0348 - val_mae: 0.0348 - val_mape: 82313.4766 - val_mse: 0.0021\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii.summary()\n",
    "\n",
    "simple_model_one_asii.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_one_asii = simple_model_one_asii.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii = simple_model_one_asii.predict(X_test_rs_asii)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_256 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_257 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 918\n",
      "Trainable params: 918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 9s 37ms/step - loss: 0.1280 - mae: 0.1280 - mape: 70076.5781 - mse: 0.0416 - val_loss: 0.0496 - val_mae: 0.0496 - val_mape: 137750.7969 - val_mse: 0.0041\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.0455 - mae: 0.0455 - mape: 57335.1328 - mse: 0.0034 - val_loss: 0.0425 - val_mae: 0.0425 - val_mape: 125939.5547 - val_mse: 0.0031\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.0411 - mae: 0.0411 - mape: 53535.3945 - mse: 0.0028 - val_loss: 0.0439 - val_mae: 0.0439 - val_mape: 104039.6562 - val_mse: 0.0030\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.0405 - mae: 0.0405 - mape: 53320.4141 - mse: 0.0028 - val_loss: 0.0369 - val_mae: 0.0369 - val_mape: 98947.7266 - val_mse: 0.0024\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.0383 - mae: 0.0383 - mape: 50351.3867 - mse: 0.0025 - val_loss: 0.0375 - val_mae: 0.0375 - val_mape: 93743.3281 - val_mse: 0.0024\n",
      "0.9479421665748983\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_two_asii = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_two_asii.summary()\n",
    "\n",
    "simple_model_two_asii.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_two_asii = simple_model_two_asii.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_two_asii = simple_model_two_asii.predict(X_test_rs_asii)\n",
    "\n",
    "print(r2_score(preds_two_asii, y_test_asii))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_258 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_259 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_260 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,462\n",
      "Trainable params: 1,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 8s 47ms/step - loss: 0.1545 - mae: 0.1545 - mape: 98852.9062 - mse: 0.0549 - val_loss: 0.0508 - val_mae: 0.0508 - val_mape: 136755.6562 - val_mse: 0.0044\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0501 - mae: 0.0501 - mape: 57763.4102 - mse: 0.0040 - val_loss: 0.0474 - val_mae: 0.0474 - val_mape: 124535.0781 - val_mse: 0.0038\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 0.0432 - mae: 0.0432 - mape: 58601.1680 - mse: 0.0031 - val_loss: 0.0401 - val_mae: 0.0401 - val_mape: 103416.1484 - val_mse: 0.0027\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 2s 32ms/step - loss: 0.0431 - mae: 0.0431 - mape: 57531.2461 - mse: 0.0031 - val_loss: 0.0484 - val_mae: 0.0484 - val_mape: 92912.5859 - val_mse: 0.0035\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0401 - mae: 0.0401 - mape: 58544.9648 - mse: 0.00 - 1s 28ms/step - loss: 0.0401 - mae: 0.0401 - mape: 58153.0469 - mse: 0.0027 - val_loss: 0.0374 - val_mae: 0.0374 - val_mape: 102082.5234 - val_mse: 0.0024\n",
      "0.9357214442157414\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_three_asii = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_three_asii.summary()\n",
    "\n",
    "simple_model_three_asii.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_three_asii = simple_model_three_asii.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_three_asii = simple_model_three_asii.predict(X_test_rs_asii)\n",
    "\n",
    "print(r2_score(preds_three_asii, y_test_asii))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_261 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_262 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_263 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_264 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,006\n",
      "Trainable params: 2,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 11s 54ms/step - loss: 0.1631 - mae: 0.1631 - mape: 96396.3594 - mse: 0.0587 - val_loss: 0.0575 - val_mae: 0.0575 - val_mape: 170528.4531 - val_mse: 0.0054\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0534 - mae: 0.0534 - mape: 72936.5000 - mse: 0.0047 - val_loss: 0.0463 - val_mae: 0.0463 - val_mape: 142201.6875 - val_mse: 0.0036\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0525 - mae: 0.0525 - mape: 65620.2578 - mse: 0.0045 - val_loss: 0.0545 - val_mae: 0.0545 - val_mape: 129867.2891 - val_mse: 0.0046\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 2s 32ms/step - loss: 0.0495 - mae: 0.0495 - mape: 70855.4609 - mse: 0.0041 - val_loss: 0.0488 - val_mae: 0.0488 - val_mape: 143902.8594 - val_mse: 0.0041\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0450 - mae: 0.0450 - mape: 68259.2578 - mse: 0.0034 - val_loss: 0.0460 - val_mae: 0.0460 - val_mape: 134045.7188 - val_mse: 0.0037\n",
      "0.8994058590196198\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_four_asii = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_four_asii.summary()\n",
    "\n",
    "simple_model_four_asii.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_four_asii = simple_model_four_asii.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_four_asii = simple_model_four_asii.predict(X_test_rs_asii)\n",
    "\n",
    "print(r2_score(preds_four_asii, y_test_asii))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_265 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_266 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_267 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_268 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_269 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,550\n",
      "Trainable params: 2,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 12s 80ms/step - loss: 0.2081 - mae: 0.2081 - mape: 112120.8516 - mse: 0.0780 - val_loss: 0.1115 - val_mae: 0.1115 - val_mape: 132972.3281 - val_mse: 0.0191\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.0726 - mae: 0.0726 - mape: 78644.3203 - mse: 0.0088 - val_loss: 0.0595 - val_mae: 0.0595 - val_mape: 172561.5938 - val_mse: 0.0060\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 2s 32ms/step - loss: 0.0551 - mae: 0.0551 - mape: 83013.9688 - mse: 0.0050 - val_loss: 0.0555 - val_mae: 0.0555 - val_mape: 161301.8125 - val_mse: 0.0052\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0488 - mae: 0.0488 - mape: 70698.9688 - mse: 0.0039 - val_loss: 0.0491 - val_mae: 0.0491 - val_mape: 131035.4297 - val_mse: 0.0038\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.0460 - mae: 0.0460 - mape: 57870.7734 - mse: 0.0036 - val_loss: 0.0520 - val_mae: 0.0520 - val_mape: 126045.3906 - val_mse: 0.0043\n",
      "0.8546837467796854\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_five_asii = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_five_asii.summary()\n",
    "\n",
    "simple_model_five_asii.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_five_asii = simple_model_five_asii.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_five_asii = simple_model_five_asii.predict(X_test_rs_asii)\n",
    "\n",
    "print(r2_score(preds_five_asii, y_test_asii))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_270 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 16ms/step - loss: 0.1411 - mae: 0.1411 - val_loss: 0.0350 - val_mae: 0.0350\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0162 - val_mae: 0.0162\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp.summary()\n",
    "\n",
    "simple_model_one_icbp.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp = simple_model_one_icbp.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp = simple_model_one_icbp.predict(X_test_rs_icbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_271 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_272 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 918\n",
      "Trainable params: 918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 9s 28ms/step - loss: 0.0893 - mae: 0.0893 - mape: 26119.6133 - mse: 0.0269 - val_loss: 0.0268 - val_mae: 0.0268 - val_mape: 17031.8535 - val_mse: 0.0013\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0230 - mae: 0.0230 - mape: 9014.2021 - mse: 9.7683e-04 - val_loss: 0.0333 - val_mae: 0.0333 - val_mape: 10595.5654 - val_mse: 0.0017\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0230 - mae: 0.0230 - mape: 11543.7432 - mse: 9.6554e-04 - val_loss: 0.0189 - val_mae: 0.0189 - val_mape: 7002.1406 - val_mse: 7.4935e-04\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0179 - mae: 0.0179 - mape: 4586.6265 - mse: 6.4514e-04 - val_loss: 0.0199 - val_mae: 0.0199 - val_mape: 5439.3022 - val_mse: 8.1622e-04\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0184 - mae: 0.0184 - mape: 3617.9670 - mse: 6.6576e-04 - val_loss: 0.0168 - val_mae: 0.0168 - val_mape: 7384.3413 - val_mse: 6.1890e-04\n",
      "0.9914614387602406\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_two_icbp = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_two_icbp.summary()\n",
    "\n",
    "simple_model_two_icbp.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_two_icbp = simple_model_two_icbp.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_two_icbp = simple_model_two_icbp.predict(X_test_rs_icbp)\n",
    "\n",
    "print(r2_score(preds_two_icbp, y_test_icbp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_273 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_274 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_275 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,462\n",
      "Trainable params: 1,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 7s 42ms/step - loss: 0.0827 - mae: 0.0827 - mape: 12531.1953 - mse: 0.0196 - val_loss: 0.0291 - val_mae: 0.0291 - val_mape: 8717.3311 - val_mse: 0.0018\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.0286 - mae: 0.0286 - mape: 12435.2305 - mse: 0.0014 - val_loss: 0.0269 - val_mae: 0.0269 - val_mape: 14923.8721 - val_mse: 0.0013\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 20ms/step - loss: 0.0229 - mae: 0.0229 - mape: 7677.7407 - mse: 9.5959e-04 - val_loss: 0.0245 - val_mae: 0.0245 - val_mape: 14486.8408 - val_mse: 0.0011\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.0250 - mae: 0.0250 - mape: 5699.9580 - mse: 0.0011 - val_loss: 0.0196 - val_mae: 0.0196 - val_mape: 9604.9873 - val_mse: 8.0203e-04\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 20ms/step - loss: 0.0240 - mae: 0.0240 - mape: 7678.9609 - mse: 0.0010 - val_loss: 0.0212 - val_mae: 0.0212 - val_mape: 17444.5527 - val_mse: 8.2629e-04\n",
      "0.9883544792810203\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_three_icbp = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_three_icbp.summary()\n",
    "\n",
    "simple_model_three_icbp.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_three_icbp = simple_model_three_icbp.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_three_icbp = simple_model_three_icbp.predict(X_test_rs_icbp)\n",
    "\n",
    "print(r2_score(preds_three_icbp, y_test_icbp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_276 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_277 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_278 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_279 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,006\n",
      "Trainable params: 2,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 9s 59ms/step - loss: 0.0936 - mae: 0.0936 - mape: 17341.2031 - mse: 0.0269 - val_loss: 0.0397 - val_mae: 0.0397 - val_mape: 25588.9629 - val_mse: 0.0025\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 0.0376 - mae: 0.0376 - mape: 12386.4658 - mse: 0.0025 - val_loss: 0.0318 - val_mae: 0.0318 - val_mape: 29146.2910 - val_mse: 0.0016\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0342 - mae: 0.0342 - mape: 10563.8613 - mse: 0.0020 - val_loss: 0.0378 - val_mae: 0.0378 - val_mape: 22384.1855 - val_mse: 0.0024\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0274 - mae: 0.0274 - mape: 9176.1494 - mse: 0.0013 - val_loss: 0.0340 - val_mae: 0.0340 - val_mape: 26404.3887 - val_mse: 0.0018\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0276 - mae: 0.0276 - mape: 10047.2090 - mse: 0.0014 - val_loss: 0.0345 - val_mae: 0.0345 - val_mape: 18994.3594 - val_mse: 0.0018\n",
      "0.975269773274066\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_four_icbp = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_four_icbp.summary()\n",
    "\n",
    "simple_model_four_icbp.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_four_icbp = simple_model_four_icbp.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_four_icbp = simple_model_four_icbp.predict(X_test_rs_icbp)\n",
    "\n",
    "print(r2_score(preds_four_icbp, y_test_icbp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_280 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_281 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_282 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_283 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_284 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,550\n",
      "Trainable params: 2,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 13s 79ms/step - loss: 0.1115 - mae: 0.1115 - mape: 24587.1230 - mse: 0.0352 - val_loss: 0.0398 - val_mae: 0.0398 - val_mape: 21309.4922 - val_mse: 0.0033\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0341 - mae: 0.0341 - mape: 22327.3223 - mse: 0.0020 - val_loss: 0.0274 - val_mae: 0.0274 - val_mape: 6580.4712 - val_mse: 0.0013\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0303 - mae: 0.0303 - mape: 11298.5195 - mse: 0.0016 - val_loss: 0.0245 - val_mae: 0.0245 - val_mape: 13001.4863 - val_mse: 0.0012\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0290 - mae: 0.0290 - mape: 10242.9336 - mse: 0.0015 - val_loss: 0.0265 - val_mae: 0.0265 - val_mape: 19207.4766 - val_mse: 0.0013\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 2s 37ms/step - loss: 0.0303 - mae: 0.0303 - mape: 10508.4639 - mse: 0.0015 - val_loss: 0.0244 - val_mae: 0.0244 - val_mape: 14496.6025 - val_mse: 0.0011\n",
      "0.9840274582084483\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_five_icbp = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_five_icbp.summary()\n",
    "\n",
    "simple_model_five_icbp.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_five_icbp = simple_model_five_icbp.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_five_icbp = simple_model_five_icbp.predict(X_test_rs_icbp)\n",
    "\n",
    "print(r2_score(preds_five_icbp, y_test_icbp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_285 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 18ms/step - loss: 0.0976 - mae: 0.0976 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0272 - mae: 0.0272 - val_loss: 0.0267 - val_mae: 0.0267\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0275 - mae: 0.0275 - val_loss: 0.0283 - val_mae: 0.0283\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0272 - mae: 0.0272 - val_loss: 0.0253 - val_mae: 0.0253\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr.summary()\n",
    "\n",
    "simple_model_one_jsmr.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr = simple_model_one_jsmr.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr = simple_model_one_jsmr.predict(X_test_rs_jsmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_286 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_287 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 918\n",
      "Trainable params: 918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 6s 39ms/step - loss: 0.1406 - mae: 0.1406 - mape: 63718.7031 - mse: 0.0499 - val_loss: 0.0429 - val_mae: 0.0429 - val_mape: 24.4951 - val_mse: 0.0032\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 0.0383 - mae: 0.0383 - mape: 51301.1094 - mse: 0.0025 - val_loss: 0.0333 - val_mae: 0.0333 - val_mape: 21.1471 - val_mse: 0.0021\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0325 - mae: 0.0325 - mape: 33254.7227 - mse: 0.0019 - val_loss: 0.0312 - val_mae: 0.0312 - val_mape: 12.3407 - val_mse: 0.0018\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0372 - mae: 0.0372 - mape: 20471.0801 - mse: 0.0023 - val_loss: 0.0347 - val_mae: 0.0347 - val_mape: 12.8696 - val_mse: 0.0021\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.0299 - mae: 0.0299 - mape: 20712.7188 - mse: 0.0016 - val_loss: 0.0301 - val_mae: 0.0301 - val_mape: 9.2829 - val_mse: 0.0016\n",
      "0.9566111604755297\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_two_jsmr = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_two_jsmr.summary()\n",
    "\n",
    "simple_model_two_jsmr.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_two_jsmr = simple_model_two_jsmr.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_two_jsmr = simple_model_two_jsmr.predict(X_test_rs_jsmr)\n",
    "\n",
    "print(r2_score(preds_two_jsmr, y_test_jsmr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_288 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_289 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_290 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,462\n",
      "Trainable params: 1,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 9s 44ms/step - loss: 0.1463 - mae: 0.1463 - mape: 118924.1328 - mse: 0.0623 - val_loss: 0.0544 - val_mae: 0.0544 - val_mape: 39.1054 - val_mse: 0.0052\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 21ms/step - loss: 0.0430 - mae: 0.0430 - mape: 70994.4375 - mse: 0.0033 - val_loss: 0.0627 - val_mae: 0.0627 - val_mape: 30.6042 - val_mse: 0.0055\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 21ms/step - loss: 0.0438 - mae: 0.0438 - mape: 46205.9688 - mse: 0.0032 - val_loss: 0.0451 - val_mae: 0.0451 - val_mape: 23.3375 - val_mse: 0.0035\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 20ms/step - loss: 0.0451 - mae: 0.0451 - mape: 30943.3867 - mse: 0.0033 - val_loss: 0.0371 - val_mae: 0.0371 - val_mape: 23.8082 - val_mse: 0.0025\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.0335 - mae: 0.0335 - mape: 29650.8574 - mse: 0.0020 - val_loss: 0.0317 - val_mae: 0.0317 - val_mape: 15.1870 - val_mse: 0.0019\n",
      "0.9439742227115658\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_three_jsmr = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_three_jsmr.summary()\n",
    "\n",
    "simple_model_three_jsmr.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_three_jsmr = simple_model_three_jsmr.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_three_jsmr = simple_model_three_jsmr.predict(X_test_rs_jsmr)\n",
    "\n",
    "print(r2_score(preds_three_jsmr, y_test_jsmr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_291 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_292 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_293 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_294 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,006\n",
      "Trainable params: 2,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 9s 53ms/step - loss: 0.1372 - mae: 0.1372 - mape: 105033.1641 - mse: 0.0474 - val_loss: 0.0514 - val_mae: 0.0514 - val_mape: 32.2157 - val_mse: 0.0047\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0438 - mae: 0.0438 - mape: 32641.0703 - mse: 0.0033 - val_loss: 0.0464 - val_mae: 0.0464 - val_mape: 14.5879 - val_mse: 0.0040\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 24ms/step - loss: 0.0412 - mae: 0.0412 - mape: 28350.7832 - mse: 0.0029 - val_loss: 0.0343 - val_mae: 0.0343 - val_mape: 10.9035 - val_mse: 0.0023\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0385 - mae: 0.0385 - mape: 18235.7812 - mse: 0.0026 - val_loss: 0.0488 - val_mae: 0.0488 - val_mape: 12.4562 - val_mse: 0.0040\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 24ms/step - loss: 0.0384 - mae: 0.0384 - mape: 21382.6699 - mse: 0.0026 - val_loss: 0.0520 - val_mae: 0.0520 - val_mape: 21.2256 - val_mse: 0.0042\n",
      "0.8941238081697076\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_four_jsmr = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_four_jsmr.summary()\n",
    "\n",
    "simple_model_four_jsmr.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_four_jsmr = simple_model_four_jsmr.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_four_jsmr = simple_model_four_jsmr.predict(X_test_rs_jsmr)\n",
    "\n",
    "print(r2_score(preds_four_jsmr, y_test_jsmr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_295 (LSTM)             (None, 18, 8)             320       \n",
      "                                                                 \n",
      " lstm_296 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_297 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_298 (LSTM)             (None, 18, 8)             544       \n",
      "                                                                 \n",
      " lstm_299 (LSTM)             (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,550\n",
      "Trainable params: 2,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 11s 64ms/step - loss: 0.1691 - mae: 0.1691 - mape: 115429.6953 - mse: 0.0579 - val_loss: 0.0643 - val_mae: 0.0643 - val_mape: 36.5445 - val_mse: 0.0068\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 0.0542 - mae: 0.0542 - mape: 62016.6133 - mse: 0.0048 - val_loss: 0.0421 - val_mae: 0.0421 - val_mape: 14.0289 - val_mse: 0.0032\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 2s 32ms/step - loss: 0.0430 - mae: 0.0430 - mape: 15585.1240 - mse: 0.0032 - val_loss: 0.0365 - val_mae: 0.0365 - val_mape: 14.9138 - val_mse: 0.0025\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.0430 - mae: 0.0430 - mape: 15600.3271 - mse: 0.0031 - val_loss: 0.0408 - val_mae: 0.0408 - val_mape: 14.3269 - val_mse: 0.0030\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.0381 - mae: 0.0381 - mape: 24609.9160 - mse: 0.0025 - val_loss: 0.0327 - val_mae: 0.0327 - val_mape: 15.7122 - val_mse: 0.0020\n",
      "0.9405535459240015\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "simple_model_five_jsmr = Sequential([\n",
    "   LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features), return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh', return_sequences=True),\n",
    "    LSTM(8, activation='tanh'),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_five_jsmr.summary()\n",
    "\n",
    "simple_model_five_jsmr.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    " metrics=['mae','mape','mse'],\n",
    ")\n",
    "\n",
    "smod_history_five_jsmr = simple_model_five_jsmr.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_five_jsmr = simple_model_five_jsmr.predict(X_test_rs_jsmr)\n",
    "\n",
    "print(r2_score(preds_five_jsmr, y_test_jsmr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATRIKS EVALUASI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer\n",
      "mape score antm: 0.02\n",
      "mape score asii: 0.03\n",
      "mape score icbp: 0.02\n",
      "mape score jsmr: 0.03\n",
      "2 layer\n",
      "mape score antm: 0.02\n",
      "mape score asii: 0.04\n",
      "mape score icbp: 0.02\n",
      "mape score jsmr: 0.03\n",
      "3 layer\n",
      "mape score antm: 0.03\n",
      "mape score asii: 0.04\n",
      "mape score icbp: 0.02\n",
      "mape score jsmr: 0.03\n",
      "4 layer\n",
      "mape score antm: 0.02\n",
      "mape score asii: 0.04\n",
      "mape score icbp: 0.03\n",
      "mape score jsmr: 0.05\n",
      "5 layer\n",
      "mape score antm: 0.02\n",
      "mape score asii: 0.05\n",
      "mape score icbp: 0.02\n",
      "mape score jsmr: 0.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forecast\n",
    "# mape score\n",
    "print('1 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_error(preds_one_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_error(preds_one_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_error(preds_one_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_error(preds_one_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('2 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_error(preds_two_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_error(preds_two_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_error(preds_two_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_error(preds_two_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('3 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_error(preds_three_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_error(preds_three_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_error(preds_three_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_error(preds_three_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('4 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_error(preds_four_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_error(preds_four_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_error(preds_four_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_error(preds_four_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('5 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_error(preds_five_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_error(preds_five_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_error(preds_five_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_error(preds_five_jsmr, y_test_jsmr).round(2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer\n",
      "mape score antm: 0.97\n",
      "mape score asii: 0.95\n",
      "mape score icbp: 0.99\n",
      "mape score jsmr: 0.97\n",
      "2 layer\n",
      "mape score antm: 0.97\n",
      "mape score asii: 0.95\n",
      "mape score icbp: 0.99\n",
      "mape score jsmr: 0.96\n",
      "3 layer\n",
      "mape score antm: 0.97\n",
      "mape score asii: 0.94\n",
      "mape score icbp: 0.99\n",
      "mape score jsmr: 0.94\n",
      "4 layer\n",
      "mape score antm: 0.97\n",
      "mape score asii: 0.9\n",
      "mape score icbp: 0.98\n",
      "mape score jsmr: 0.89\n",
      "5 layer\n",
      "mape score antm: 0.96\n",
      "mape score asii: 0.85\n",
      "mape score icbp: 0.98\n",
      "mape score jsmr: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Forecast\n",
    "# mape score\n",
    "print('1 layer')\n",
    "print(\"mape score antm: \"+str(r2_score(preds_one_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(r2_score(preds_one_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(r2_score(preds_one_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(r2_score(preds_one_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('2 layer')\n",
    "print(\"mape score antm: \"+str(r2_score(preds_two_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(r2_score(preds_two_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(r2_score(preds_two_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(r2_score(preds_two_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('3 layer')\n",
    "print(\"mape score antm: \"+str(r2_score(preds_three_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(r2_score(preds_three_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(r2_score(preds_three_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(r2_score(preds_three_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('4 layer')\n",
    "print(\"mape score antm: \"+str(r2_score(preds_four_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(r2_score(preds_four_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(r2_score(preds_four_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(r2_score(preds_four_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('5 layer')\n",
    "print(\"mape score antm: \"+str(r2_score(preds_five_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(r2_score(preds_five_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(r2_score(preds_five_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(r2_score(preds_five_jsmr, y_test_jsmr).round(2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer\n",
      "mape score antm: 0.14\n",
      "mape score asii: 0.07\n",
      "mape score icbp: 0.06\n",
      "mape score jsmr: 0.07\n",
      "2 layer\n",
      "mape score antm: 0.23\n",
      "mape score asii: 0.08\n",
      "mape score icbp: 0.05\n",
      "mape score jsmr: 0.19\n",
      "3 layer\n",
      "mape score antm: 0.13\n",
      "mape score asii: 0.08\n",
      "mape score icbp: 0.08\n",
      "mape score jsmr: 0.08\n",
      "4 layer\n",
      "mape score antm: 0.14\n",
      "mape score asii: 0.1\n",
      "mape score icbp: 0.11\n",
      "mape score jsmr: 0.11\n",
      "5 layer\n",
      "mape score antm: 0.71\n",
      "mape score asii: 0.1\n",
      "mape score icbp: 0.08\n",
      "mape score jsmr: 0.08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "# Forecast\n",
    "# mape score\n",
    "print('1 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_one_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_one_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('2 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_two_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_two_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_two_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_two_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('3 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_three_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_three_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_three_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_three_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('4 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_four_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_four_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_four_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_four_jsmr, y_test_jsmr).round(2)))\n",
    "\n",
    "print('5 layer')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_five_antm, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_five_asii, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_five_icbp, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_five_jsmr, y_test_jsmr).round(2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model and plot learning curve\n",
    "def fit_model_antm(X_train_rs_antm, y_train_antm, X_test_rs_antm, y_test_antm, lrate):\n",
    "\t# define model\n",
    "\tsimple_model_one_antm_lr = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\t# compile model\n",
    "\topt = tf.keras.optimizers.Adam(learning_rate=lrate)\n",
    "\tsimple_model_one_antm_lr.compile(\n",
    "  optimizer=opt,\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\t# fit model\n",
    "\thistory = simple_model_one_antm_lr.fit(X_train_rs_antm, y_train_antm, validation_data=(X_test_rs_antm, y_test_antm), epochs=100, verbose=0)\n",
    "\t# plot learning curves\n",
    "\tplt.plot(history.history['mae'], label='train')\n",
    "\tplt.plot(history.history['val_mae'], label='test')\n",
    "\tplt.title('lrate='+str(lrate), pad=-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create learning curves for different learning rates\n",
    "learning_rates = [1E-0, 1E-1, 1E-2, 1E-3]\n",
    "for i in range(len(learning_rates)):\n",
    "\t# determine the plot number\n",
    "\tplot_no =410+(i+1)\n",
    "\tplt.subplot(plot_no)\n",
    "\t# fit model and plot learning curves for a learning rate\n",
    "\tfit_model_antm(X_train_rs_antm, y_train_antm, X_test_rs_antm, y_test_antm, learning_rates[i])\n",
    "# show learning curves\n",
    "plt.savefig('learning_rate_plot',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_235\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_435 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_235 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.0556 - mae: 0.0556 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0273 - val_mae: 0.0273\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0192 - mae: 0.0192 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0205 - mae: 0.0205 - val_loss: 0.0204 - val_mae: 0.0204\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_lr1 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_lr1.summary()\n",
    "\n",
    "simple_model_one_antm_lr1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_lr1 = simple_model_one_antm_lr1.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_lr1 = simple_model_one_antm_lr1.predict(X_test_rs_antm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_236\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_436 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_236 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 22ms/step - loss: 0.0922 - mae: 0.0922 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0158 - val_mae: 0.0158\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_lr2 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_lr2.summary()\n",
    "\n",
    "simple_model_one_antm_lr2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_lr2 = simple_model_one_antm_lr2.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_lr2 = simple_model_one_antm_lr2.predict(X_test_rs_antm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_237\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_437 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.2149 - mae: 0.2149 - val_loss: 0.1452 - val_mae: 0.1452\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.1084 - mae: 0.1084 - val_loss: 0.0698 - val_mae: 0.0698\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0562 - mae: 0.0562 - val_loss: 0.0441 - val_mae: 0.0441\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0394 - mae: 0.0394 - val_loss: 0.0301 - val_mae: 0.0301\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0269 - mae: 0.0269 - val_loss: 0.0226 - val_mae: 0.0226\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_lr3 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_lr3.summary()\n",
    "\n",
    "simple_model_one_antm_lr3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_lr3 = simple_model_one_antm_lr3.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_lr3 = simple_model_one_antm_lr3.predict(X_test_rs_antm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_238\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_438 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.0788 - mae: 0.0788 - val_loss: 0.0363 - val_mae: 0.0363\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0400 - mae: 0.0400 - val_loss: 0.0378 - val_mae: 0.0378\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0389 - mae: 0.0389 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0363 - mae: 0.0363 - val_loss: 0.0342 - val_mae: 0.0342\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0358 - mae: 0.0358 - val_loss: 0.0328 - val_mae: 0.0328\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_lr1 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_lr1.summary()\n",
    "\n",
    "simple_model_one_asii_lr1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_lr1 = simple_model_one_asii_lr1.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_lr1 = simple_model_one_asii_lr1.predict(X_test_rs_asii)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_239\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_439 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.1394 - mae: 0.1394 - val_loss: 0.0469 - val_mae: 0.0469\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0387 - mae: 0.0387 - val_loss: 0.0362 - val_mae: 0.0362\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0363 - mae: 0.0363 - val_loss: 0.0369 - val_mae: 0.0369\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0345 - mae: 0.0345 - val_loss: 0.0346 - val_mae: 0.0346\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0335 - mae: 0.0335 - val_loss: 0.0348 - val_mae: 0.0348\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_lr2 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_lr2.summary()\n",
    "\n",
    "simple_model_one_asii_lr2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_lr2 = simple_model_one_asii_lr2.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_lr2 = simple_model_one_asii_lr2.predict(X_test_rs_asii)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_240\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_440 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_240 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 23ms/step - loss: 0.5062 - mae: 0.5062 - val_loss: 0.3515 - val_mae: 0.3515\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.2374 - mae: 0.2374 - val_loss: 0.1501 - val_mae: 0.1501\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.1017 - mae: 0.1017 - val_loss: 0.0674 - val_mae: 0.0674\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0521 - mae: 0.0521 - val_loss: 0.0463 - val_mae: 0.0463\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0429 - mae: 0.0429 - val_loss: 0.0420 - val_mae: 0.0420\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_lr3 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_lr3.summary()\n",
    "\n",
    "simple_model_one_asii_lr3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_lr3 = simple_model_one_asii_lr3.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_lr3 = simple_model_one_asii_lr3.predict(X_test_rs_asii)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_241\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_441 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 20ms/step - loss: 0.0745 - mae: 0.0745 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0262 - val_mae: 0.0262\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0381 - val_mae: 0.0381\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0213 - mae: 0.0213 - val_loss: 0.0222 - val_mae: 0.0222\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_lr1 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_lr1.summary()\n",
    "\n",
    "simple_model_one_icbp_lr1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_lr1 = simple_model_one_icbp_lr1.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_lr1 = simple_model_one_icbp_lr1.predict(X_test_rs_icbp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_242\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_442 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 18ms/step - loss: 0.1191 - mae: 0.1191 - val_loss: 0.0290 - val_mae: 0.0290\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0214 - mae: 0.0214 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0185 - mae: 0.0185 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0204 - mae: 0.0204 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0156 - val_mae: 0.0156\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_lr2 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_lr2.summary()\n",
    "\n",
    "simple_model_one_icbp_lr2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_lr2 = simple_model_one_icbp_lr2.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_lr2 = simple_model_one_icbp_lr2.predict(X_test_rs_icbp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_243\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_443 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.3723 - mae: 0.3723 - val_loss: 0.2653 - val_mae: 0.2653\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.2005 - mae: 0.2005 - val_loss: 0.1281 - val_mae: 0.1281\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0817 - mae: 0.0817 - val_loss: 0.0516 - val_mae: 0.0516\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0361 - mae: 0.0361 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0291 - mae: 0.0291 - val_loss: 0.0280 - val_mae: 0.0280\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_lr3 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_lr3.summary()\n",
    "\n",
    "simple_model_one_icbp_lr3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_lr3 = simple_model_one_icbp_lr3.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_lr3 = simple_model_one_icbp_lr3.predict(X_test_rs_icbp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_244\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_444 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.0944 - mae: 0.0944 - val_loss: 0.0327 - val_mae: 0.0327\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0294 - mae: 0.0294 - val_loss: 0.0461 - val_mae: 0.0461\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0328 - mae: 0.0328 - val_loss: 0.0386 - val_mae: 0.0386\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0285 - mae: 0.0285 - val_loss: 0.0327 - val_mae: 0.0327\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0262 - mae: 0.0262 - val_loss: 0.0231 - val_mae: 0.0231\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_lr1 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_lr1.summary()\n",
    "\n",
    "simple_model_one_jsmr_lr1.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_lr1 = simple_model_one_jsmr_lr1.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_lr1 = simple_model_one_jsmr_lr1.predict(X_test_rs_jsmr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_245\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_445 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_245 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 18ms/step - loss: 0.1299 - mae: 0.1299 - val_loss: 0.0353 - val_mae: 0.0353\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0326 - mae: 0.0326 - val_loss: 0.0292 - val_mae: 0.0292\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0297 - mae: 0.0297 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0270 - mae: 0.0270 - val_loss: 0.0262 - val_mae: 0.0262\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0262 - mae: 0.0262 - val_loss: 0.0269 - val_mae: 0.0269\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_lr2 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_lr2.summary()\n",
    "\n",
    "simple_model_one_jsmr_lr2.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_lr2 = simple_model_one_jsmr_lr2.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_lr2 = simple_model_one_jsmr_lr2.predict(X_test_rs_jsmr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_246\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_446 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.4930 - mae: 0.4930 - val_loss: 0.3084 - val_mae: 0.3084\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.1904 - mae: 0.1904 - val_loss: 0.1261 - val_mae: 0.1261\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0784 - mae: 0.0784 - val_loss: 0.0460 - val_mae: 0.0460\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0422 - mae: 0.0422 - val_loss: 0.0438 - val_mae: 0.0438\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0397 - mae: 0.0397 - val_loss: 0.0397 - val_mae: 0.0397\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_lr3 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_lr3.summary()\n",
    "\n",
    "simple_model_one_jsmr_lr3.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_lr3 = simple_model_one_jsmr_lr3.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=5,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_lr3 = simple_model_one_jsmr_lr3.predict(X_test_rs_jsmr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATRIKS EVALUASI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrarning rate 0.1\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.02\n",
      "mae score jsmr: 0.02\n",
      "lrarning rate 0.01\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.02\n",
      "mae score jsmr: 0.03\n",
      "lrarning rate 0.001\n",
      "mae score antm: 0.03\n",
      "mae score asii: 0.04\n",
      "mae score icbp: 0.03\n",
      "mae score jsmr: 0.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forecast\n",
    "# mae score\n",
    "print('lrarning rate 0.1')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_lr1, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_lr1, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_lr1, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_lr1, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.01')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_lr2, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_lr2, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_lr2, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_lr2, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.001')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_lr3, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_lr3, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_lr3, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_lr3, y_test_jsmr).round(2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrarning rate 0.1\n",
      "R2 score antm: 0.97\n",
      "R2 score asii: 0.96\n",
      "R2 score icbp: 0.99\n",
      "R2 score jsmr: 0.98\n",
      "lrarning rate 0.01\n",
      "R2 score antm: 0.98\n",
      "R2 score asii: 0.95\n",
      "R2 score icbp: 0.99\n",
      "R2 score jsmr: 0.97\n",
      "lrarning rate 0.001\n",
      "R2 score antm: 0.95\n",
      "R2 score asii: 0.92\n",
      "R2 score icbp: 0.97\n",
      "R2 score jsmr: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Forecast\n",
    "# R2 score\n",
    "print('lrarning rate 0.1')\n",
    "print(\"R2 score antm: \"+str(r2_score(preds_one_antm_lr1, y_test_antm).round(2)))\n",
    "print(\"R2 score asii: \"+str(r2_score(preds_one_asii_lr1, y_test_asii).round(2)))\n",
    "print(\"R2 score icbp: \"+str(r2_score(preds_one_icbp_lr1, y_test_icbp).round(2)))\n",
    "print(\"R2 score jsmr: \"+str(r2_score(preds_one_jsmr_lr1, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.01')\n",
    "print(\"R2 score antm: \"+str(r2_score(preds_one_antm_lr2, y_test_antm).round(2)))\n",
    "print(\"R2 score asii: \"+str(r2_score(preds_one_asii_lr2, y_test_asii).round(2)))\n",
    "print(\"R2 score icbp: \"+str(r2_score(preds_one_icbp_lr2, y_test_icbp).round(2)))\n",
    "print(\"R2 score jsmr: \"+str(r2_score(preds_one_jsmr_lr2, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.001')\n",
    "print(\"R2 score antm: \"+str(r2_score(preds_one_antm_lr3, y_test_antm).round(2)))\n",
    "print(\"R2 score asii: \"+str(r2_score(preds_one_asii_lr3, y_test_asii).round(2)))\n",
    "print(\"R2 score icbp: \"+str(r2_score(preds_one_icbp_lr3, y_test_icbp).round(2)))\n",
    "print(\"R2 score jsmr: \"+str(r2_score(preds_one_jsmr_lr3, y_test_jsmr).round(2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrarning rate 0.1\n",
      "mape score antm: 0.1\n",
      "mape score asii: 0.07\n",
      "mape score icbp: 0.15\n",
      "mape score jsmr: 0.05\n",
      "lrarning rate 0.01\n",
      "mape score antm: 0.08\n",
      "mape score asii: 0.07\n",
      "mape score icbp: 0.08\n",
      "mape score jsmr: 0.06\n",
      "lrarning rate 0.001\n",
      "mape score antm: 0.11\n",
      "mape score asii: 0.09\n",
      "mape score icbp: 0.09\n",
      "mape score jsmr: 0.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Forecast\n",
    "# mape score\n",
    "print('lrarning rate 0.1')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_lr1, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_lr1, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_lr1, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_lr1, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.01')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_lr2, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_lr2, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_lr2, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_lr2, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.001')\n",
    "print(\"mape score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_lr3, y_test_antm).round(2)))\n",
    "print(\"mape score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_lr3, y_test_asii).round(2)))\n",
    "print(\"mape score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_lr3, y_test_icbp).round(2)))\n",
    "print(\"mape score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_lr3, y_test_jsmr).round(2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUNING EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 8)                 320       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "52/52 [==============================] - 3s 22ms/step - loss: 0.0843 - mae: 0.0843 - val_loss: 0.0268 - val_mae: 0.0268\n",
      "Epoch 2/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0243 - mae: 0.0243 - val_loss: 0.0248 - val_mae: 0.0248\n",
      "Epoch 3/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0205 - mae: 0.0205 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 4/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 5/25\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0198 - mae: 0.0198 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 6/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 7/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 8/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 9/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 10/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 11/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 12/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 13/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 14/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 15/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 16/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 17/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 18/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 19/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 20/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 21/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 22/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 23/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 24/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 25/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0145 - val_mae: 0.0145\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_epoch25 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_epoch25.summary()\n",
    "\n",
    "simple_model_one_antm_epoch25.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_epoch25 = simple_model_one_antm_epoch25.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=25,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_epoch25 = simple_model_one_antm_epoch25.predict(X_test_rs_antm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "52/52 [==============================] - 3s 17ms/step - loss: 0.0749 - mae: 0.0749 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 2/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 3/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0248 - val_mae: 0.0248\n",
      "Epoch 4/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0191 - mae: 0.0191 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 5/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 6/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 7/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 8/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 9/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 10/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 11/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 12/50\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 13/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 14/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 15/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 16/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 17/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 18/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 19/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 20/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 21/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 22/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 23/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 24/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 25/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 26/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 27/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 28/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 29/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 30/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 31/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 32/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 33/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 34/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 35/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 36/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 37/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 38/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 39/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 40/50\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 41/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 42/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 43/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 44/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 45/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 46/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 47/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 48/50\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0151 - mae: 0.015 - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 49/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 50/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0140 - val_mae: 0.0140\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_epoch50 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_epoch50.summary()\n",
    "\n",
    "simple_model_one_antm_epoch50.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_epoch50 = simple_model_one_antm_epoch50.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=50,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_epoch50 = simple_model_one_antm_epoch50.predict(X_test_rs_antm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 17ms/step - loss: 0.0808 - mae: 0.0808 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0193 - mae: 0.0193 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0167 - mae: 0.0167 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0148 - mae: 0.014 - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0182 - val_mae: 0.0182\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0141 - val_mae: 0.0141\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_epoch100 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_epoch100.summary()\n",
    "\n",
    "simple_model_one_antm_epoch100.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_epoch100 = simple_model_one_antm_epoch100.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_epoch100 = simple_model_one_antm_epoch100.predict(X_test_rs_antm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "52/52 [==============================] - 3s 20ms/step - loss: 0.1379 - mae: 0.1379 - val_loss: 0.0437 - val_mae: 0.0437\n",
      "Epoch 2/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0395 - mae: 0.0395 - val_loss: 0.0369 - val_mae: 0.0369\n",
      "Epoch 3/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0376 - mae: 0.0376 - val_loss: 0.0392 - val_mae: 0.0392\n",
      "Epoch 4/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0361 - mae: 0.0361 - val_loss: 0.0345 - val_mae: 0.0345\n",
      "Epoch 5/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0359 - mae: 0.0359 - val_loss: 0.0339 - val_mae: 0.0339\n",
      "Epoch 6/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0336 - mae: 0.0336 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 7/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0331 - mae: 0.0331 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 8/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0341 - mae: 0.0341 - val_loss: 0.0398 - val_mae: 0.0398\n",
      "Epoch 9/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0342 - mae: 0.0342 - val_loss: 0.0337 - val_mae: 0.0337\n",
      "Epoch 10/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0335 - mae: 0.0335 - val_loss: 0.0369 - val_mae: 0.0369\n",
      "Epoch 11/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0330 - mae: 0.0330 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 12/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0330 - mae: 0.0330 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 13/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0324 - mae: 0.0324 - val_loss: 0.0372 - val_mae: 0.0372\n",
      "Epoch 14/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0340 - mae: 0.0340 - val_loss: 0.0337 - val_mae: 0.0337\n",
      "Epoch 15/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0327 - mae: 0.0327 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 16/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0324 - mae: 0.0324 - val_loss: 0.0321 - val_mae: 0.0321\n",
      "Epoch 17/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 18/25\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0315 - mae: 0.0315 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 19/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0329 - mae: 0.0329 - val_loss: 0.0340 - val_mae: 0.0340\n",
      "Epoch 20/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0320 - mae: 0.0320 - val_loss: 0.0331 - val_mae: 0.0331\n",
      "Epoch 21/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0314 - mae: 0.0314 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 22/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0321 - val_mae: 0.0321\n",
      "Epoch 23/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 24/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 25/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0310 - val_mae: 0.0310\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_epoch25 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_epoch25.summary()\n",
    "\n",
    "simple_model_one_asii_epoch25.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_epoch25 = simple_model_one_asii_epoch25.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=25,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_epoch25 = simple_model_one_asii_epoch25.predict(X_test_rs_asii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "52/52 [==============================] - 3s 17ms/step - loss: 0.1885 - mae: 0.1885 - val_loss: 0.0500 - val_mae: 0.0500\n",
      "Epoch 2/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0420 - mae: 0.0420 - val_loss: 0.0385 - val_mae: 0.0385\n",
      "Epoch 3/50\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0372 - mae: 0.037 - 0s 8ms/step - loss: 0.0368 - mae: 0.0368 - val_loss: 0.0347 - val_mae: 0.0347\n",
      "Epoch 4/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0342 - mae: 0.0342 - val_loss: 0.0335 - val_mae: 0.0335\n",
      "Epoch 5/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0344 - mae: 0.0344 - val_loss: 0.0416 - val_mae: 0.0416\n",
      "Epoch 6/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0334 - mae: 0.0334 - val_loss: 0.0330 - val_mae: 0.0330\n",
      "Epoch 7/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0324 - mae: 0.0324 - val_loss: 0.0359 - val_mae: 0.0359\n",
      "Epoch 8/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0328 - mae: 0.0328 - val_loss: 0.0348 - val_mae: 0.0348\n",
      "Epoch 9/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0331 - mae: 0.0331 - val_loss: 0.0362 - val_mae: 0.0362\n",
      "Epoch 10/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0336 - mae: 0.0336 - val_loss: 0.0334 - val_mae: 0.0334\n",
      "Epoch 11/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0326 - mae: 0.0326 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 12/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0326 - mae: 0.0326 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 13/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 14/50\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0343 - val_mae: 0.0343\n",
      "Epoch 15/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 16/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 17/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0335 - val_mae: 0.0335\n",
      "Epoch 18/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 19/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0314 - mae: 0.0314 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 20/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0336 - mae: 0.0336 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 21/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 22/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 23/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0331 - val_mae: 0.0331\n",
      "Epoch 24/50\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 25/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 26/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 27/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 28/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 29/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 30/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 31/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0327 - val_mae: 0.0327\n",
      "Epoch 32/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 33/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0372 - val_mae: 0.0372\n",
      "Epoch 34/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 35/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 36/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0343 - val_mae: 0.0343\n",
      "Epoch 37/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 38/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 39/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0326 - val_mae: 0.0326\n",
      "Epoch 40/50\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0300 - mae: 0.030 - 1s 10ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 41/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0319 - mae: 0.0319 - val_loss: 0.0412 - val_mae: 0.0412\n",
      "Epoch 42/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0340 - mae: 0.0340 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 43/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 44/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 45/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 46/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 47/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 48/50\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0303 - mae: 0.030 - 0s 8ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 49/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0320 - val_mae: 0.0320\n",
      "Epoch 50/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0316 - val_mae: 0.0316\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_epoch50 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_epoch50.summary()\n",
    "\n",
    "simple_model_one_asii_epoch50.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_epoch50 = simple_model_one_asii_epoch50.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=50,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_epoch50 = simple_model_one_asii_epoch50.predict(X_test_rs_asii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 20ms/step - loss: 0.1661 - mae: 0.1661 - val_loss: 0.0469 - val_mae: 0.0469\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0389 - mae: 0.0389 - val_loss: 0.0356 - val_mae: 0.0356\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0362 - mae: 0.0362 - val_loss: 0.0433 - val_mae: 0.0433\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0376 - mae: 0.0376 - val_loss: 0.0340 - val_mae: 0.0340\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0344 - mae: 0.0344 - val_loss: 0.0377 - val_mae: 0.0377\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0348 - mae: 0.0348 - val_loss: 0.0362 - val_mae: 0.0362\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0323 - mae: 0.0323 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0322 - mae: 0.0322 - val_loss: 0.0343 - val_mae: 0.0343\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0319 - mae: 0.0319 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0318 - mae: 0.0318 - val_loss: 0.0325 - val_mae: 0.0325\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0330 - mae: 0.0330 - val_loss: 0.0321 - val_mae: 0.0321\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0318 - mae: 0.0318 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0315 - mae: 0.0315 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0318 - mae: 0.0318 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0335 - val_mae: 0.0335\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0347 - val_mae: 0.0347\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0331 - val_mae: 0.0331\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0336 - val_mae: 0.0336\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0326 - val_mae: 0.0326\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0341 - val_mae: 0.0341\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0297 - mae: 0.0297 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0352 - val_mae: 0.0352\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0297 - mae: 0.0297 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0340 - val_mae: 0.0340\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0298 - mae: 0.0298 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0339 - val_mae: 0.0339\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0344 - val_mae: 0.0344\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0315 - val_mae: 0.0315\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_epoch100 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_epoch100.summary()\n",
    "\n",
    "simple_model_one_asii_epoch100.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_epoch100 = simple_model_one_asii_epoch100.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_epoch100 = simple_model_one_asii_epoch100.predict(X_test_rs_asii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "52/52 [==============================] - 3s 18ms/step - loss: 0.1508 - mae: 0.1508 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 2/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 3/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 4/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 5/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0188 - mae: 0.0188 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 6/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 7/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 8/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 9/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 10/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 11/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 12/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 13/25\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 14/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 15/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 16/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 17/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 18/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 19/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 20/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 21/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 22/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 23/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 24/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 25/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0147 - val_mae: 0.0147\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_epoch25 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_epoch25.summary()\n",
    "\n",
    "simple_model_one_icbp_epoch25.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_epoch25 = simple_model_one_icbp_epoch25.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=25,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_epoch25 = simple_model_one_icbp_epoch25.predict(X_test_rs_icbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "52/52 [==============================] - 3s 17ms/step - loss: 0.0768 - mae: 0.0768 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 2/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0196 - mae: 0.0196 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 3/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0188 - mae: 0.0188 - val_loss: 0.0285 - val_mae: 0.0285\n",
      "Epoch 4/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0203 - mae: 0.0203 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 5/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 6/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 7/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0240 - val_mae: 0.0240\n",
      "Epoch 8/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 9/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 10/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 11/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 12/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 13/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 14/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 15/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 16/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 17/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 18/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 19/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 20/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 21/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 22/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 23/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 24/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 25/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 26/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 27/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 28/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 29/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 30/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 31/50\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 32/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 33/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 34/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 35/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 36/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 37/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 38/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 39/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 40/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 41/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 42/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 43/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 44/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 45/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 46/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 47/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 48/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0163 - val_mae: 0.0163\n",
      "Epoch 49/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 50/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0153 - val_mae: 0.0153\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_epoch50 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_epoch50.summary()\n",
    "\n",
    "simple_model_one_icbp_epoch50.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_epoch50 = simple_model_one_icbp_epoch50.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=50,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_epoch50 = simple_model_one_icbp_epoch50.predict(X_test_rs_icbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 4s 18ms/step - loss: 0.1292 - mae: 0.1292 - val_loss: 0.0277 - val_mae: 0.0277\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0207 - val_mae: 0.0207\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0187 - val_mae: 0.0187\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0135 - mae: 0.0135 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0165 - val_mae: 0.0165\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_epoch100 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_epoch100.summary()\n",
    "\n",
    "simple_model_one_icbp_epoch100.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_epoch100 = simple_model_one_icbp_epoch100.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_epoch100 = simple_model_one_icbp_epoch100.predict(X_test_rs_icbp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "52/52 [==============================] - 3s 21ms/step - loss: 0.1054 - mae: 0.1054 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 2/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0327 - mae: 0.0327 - val_loss: 0.0263 - val_mae: 0.0263\n",
      "Epoch 3/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0277 - mae: 0.0277 - val_loss: 0.0279 - val_mae: 0.0279\n",
      "Epoch 4/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0269 - mae: 0.0269 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 5/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0263 - mae: 0.0263 - val_loss: 0.0262 - val_mae: 0.0262\n",
      "Epoch 6/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 7/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0270 - mae: 0.0270 - val_loss: 0.0264 - val_mae: 0.0264\n",
      "Epoch 8/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0240 - val_mae: 0.0240\n",
      "Epoch 9/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0261 - val_mae: 0.0261\n",
      "Epoch 10/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 11/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0276 - val_mae: 0.0276\n",
      "Epoch 12/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0261 - mae: 0.0261 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 13/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 14/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 15/25\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 16/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 17/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 18/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 19/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 20/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 21/25\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 22/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 23/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 24/25\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 25/25\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0234 - mae: 0.023 - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0222 - val_mae: 0.0222\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_epoch25 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_epoch25.summary()\n",
    "\n",
    "simple_model_one_jsmr_epoch25.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_epoch25 = simple_model_one_jsmr_epoch25.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=25,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_epoch25 = simple_model_one_jsmr_epoch25.predict(X_test_rs_jsmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_7 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "52/52 [==============================] - 5s 37ms/step - loss: 0.1368 - mae: 0.1368 - val_loss: 0.0413 - val_mae: 0.0413\n",
      "Epoch 2/50\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0332 - mae: 0.0332 - val_loss: 0.0301 - val_mae: 0.0301\n",
      "Epoch 3/50\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0267 - val_mae: 0.0267\n",
      "Epoch 4/50\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0290 - mae: 0.0290 - val_loss: 0.0262 - val_mae: 0.0262\n",
      "Epoch 5/50\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0270 - mae: 0.0270 - val_loss: 0.0256 - val_mae: 0.0256\n",
      "Epoch 6/50\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0265 - mae: 0.0265 - val_loss: 0.0268 - val_mae: 0.0268\n",
      "Epoch 7/50\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0260 - mae: 0.0260 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 8/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 9/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0260 - mae: 0.0260 - val_loss: 0.0240 - val_mae: 0.0240\n",
      "Epoch 10/50\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 11/50\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0257 - mae: 0.0257 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 12/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 13/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0263 - val_mae: 0.0263\n",
      "Epoch 14/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 15/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 16/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 17/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 18/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 19/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0295 - val_mae: 0.0295\n",
      "Epoch 20/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0268 - mae: 0.0268 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 21/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 22/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 23/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 24/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 25/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 26/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 27/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 28/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 29/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 30/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 31/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 32/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 33/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 34/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 35/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 36/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 37/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 38/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 39/50\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0250 - val_mae: 0.0250\n",
      "Epoch 40/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 41/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 42/50\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0277 - val_mae: 0.0277\n",
      "Epoch 43/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0253 - val_mae: 0.0253\n",
      "Epoch 44/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 45/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 46/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 47/50\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 48/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 49/50\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 50/50\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0221 - val_mae: 0.0221\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_epoch50 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_epoch50.summary()\n",
    "\n",
    "simple_model_one_jsmr_epoch50.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_epoch50 = simple_model_one_jsmr_epoch50.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=50,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_epoch50 = simple_model_one_jsmr_epoch50.predict(X_test_rs_jsmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 18ms/step - loss: 0.1030 - mae: 0.1030 - val_loss: 0.0385 - val_mae: 0.0385\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0337 - mae: 0.0337 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0318 - mae: 0.0318 - val_loss: 0.0280 - val_mae: 0.0280\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0275 - mae: 0.0275 - val_loss: 0.0277 - val_mae: 0.0277\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0272 - mae: 0.0272 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0282 - mae: 0.0282 - val_loss: 0.0251 - val_mae: 0.0251\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0265 - mae: 0.0265 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0262 - mae: 0.0262 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0266 - mae: 0.0266 - val_loss: 0.0260 - val_mae: 0.0260\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0249 - mae: 0.0249 - val_loss: 0.0266 - val_mae: 0.0266\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0255 - val_mae: 0.0255\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0245 - mae: 0.0245 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0249 - mae: 0.0249 - val_loss: 0.0255 - val_mae: 0.0255\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0257 - mae: 0.0257 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0272 - val_mae: 0.0272\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0249 - mae: 0.0249 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0275 - val_mae: 0.0275\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0260 - val_mae: 0.0260\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0284 - val_mae: 0.0284\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0263 - val_mae: 0.0263\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0236 - val_mae: 0.0236\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0240 - val_mae: 0.0240\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0286 - val_mae: 0.0286\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0274 - val_mae: 0.0274\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0265 - val_mae: 0.0265\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0266 - val_mae: 0.0266\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0229 - mae: 0.022 - 0s 8ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0273 - val_mae: 0.0273\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0214 - val_mae: 0.0214\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0241 - val_mae: 0.0241\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_epoch100 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_epoch100.summary()\n",
    "\n",
    "simple_model_one_jsmr_epoch100.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_epoch100 = simple_model_one_jsmr_epoch100.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_epoch100 = simple_model_one_jsmr_epoch100.predict(X_test_rs_jsmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAtRIKS EVALUASI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrarning rate 0.1\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.02\n",
      "mae score jsmr: 0.02\n",
      "lrarning rate 0.01\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.01\n",
      "mae score jsmr: 0.02\n",
      "lrarning rate 0.001\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.01\n",
      "mae score jsmr: 0.02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forecast\n",
    "# mae score\n",
    "print('lrarning rate 0.1')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_epoch25, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_epoch25, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_epoch25, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_epoch25, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.01')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_epoch50, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_epoch50, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_epoch50, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_epoch50, y_test_jsmr).round(2)))\n",
    "\n",
    "print('lrarning rate 0.001')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_epoch100, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_epoch100, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_epoch100, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_epoch100, y_test_jsmr).round(2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 SKOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25\n",
      "R2 score antm: 0.98\n",
      "R2 score asii: 0.96\n",
      "R2 score icbp: 0.99\n",
      "R2 score jsmr: 0.98\n",
      "epoch 50\n",
      "R2 score antm: 0.98\n",
      "R2 score asii: 0.96\n",
      "R2 score icbp: 0.99\n",
      "R2 score jsmr: 0.98\n",
      "epoch 100\n",
      "R2 score antm: 0.98\n",
      "R2 score asii: 0.96\n",
      "R2 score icbp: 0.99\n",
      "R2 score jsmr: 0.98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forecast\n",
    "# R2 score\n",
    "print('epoch 25')\n",
    "print(\"R2 score antm: \"+str(r2_score(preds_one_antm_epoch25, y_test_antm).round(2)))\n",
    "print(\"R2 score asii: \"+str(r2_score(preds_one_asii_epoch25, y_test_asii).round(2)))\n",
    "print(\"R2 score icbp: \"+str(r2_score(preds_one_icbp_epoch25, y_test_icbp).round(2)))\n",
    "print(\"R2 score jsmr: \"+str(r2_score(preds_one_jsmr_epoch25, y_test_jsmr).round(2)))\n",
    "\n",
    "print('epoch 50')\n",
    "print(\"R2 score antm: \"+str(r2_score(preds_one_antm_epoch50, y_test_antm).round(2)))\n",
    "print(\"R2 score asii: \"+str(r2_score(preds_one_asii_epoch50, y_test_asii).round(2)))\n",
    "print(\"R2 score icbp: \"+str(r2_score(preds_one_icbp_epoch50, y_test_icbp).round(2)))\n",
    "print(\"R2 score jsmr: \"+str(r2_score(preds_one_jsmr_epoch50, y_test_jsmr).round(2)))\n",
    "\n",
    "print('epoch 100')\n",
    "print(\"R2 score antm: \"+str(r2_score(preds_one_antm_epoch100, y_test_antm).round(2)))\n",
    "print(\"R2 score asii: \"+str(r2_score(preds_one_asii_epoch100, y_test_asii).round(2)))\n",
    "print(\"R2 score icbp: \"+str(r2_score(preds_one_icbp_epoch100, y_test_icbp).round(2)))\n",
    "print(\"R2 score jsmr: \"+str(r2_score(preds_one_jsmr_epoch100, y_test_jsmr).round(2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25\n",
      "MAPE score antm: 0.2\n",
      "MAPE score asii: 0.06\n",
      "MAPE score icbp: 0.05\n",
      "MAPE score jsmr: 0.05\n",
      "epoch 50\n",
      "MAPE score antm: 0.15\n",
      "MAPE score asii: 0.06\n",
      "MAPE score icbp: 0.06\n",
      "MAPE score jsmr: 0.05\n",
      "epoch 100\n",
      "MAPE score antm: 0.07\n",
      "MAPE score asii: 0.06\n",
      "MAPE score icbp: 0.05\n",
      "MAPE score jsmr: 0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "# Forecast\n",
    "# MAPE score\n",
    "print('epoch 25')\n",
    "print(\"MAPE score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_epoch25, y_test_antm).round(2)))\n",
    "print(\"MAPE score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_epoch25, y_test_asii).round(2)))\n",
    "print(\"MAPE score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_epoch25, y_test_icbp).round(2)))\n",
    "print(\"MAPE score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_epoch25, y_test_jsmr).round(2)))\n",
    "\n",
    "print('epoch 50')\n",
    "print(\"MAPE score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_epoch50, y_test_antm).round(2)))\n",
    "print(\"MAPE score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_epoch50, y_test_asii).round(2)))\n",
    "print(\"MAPE score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_epoch50, y_test_icbp).round(2)))\n",
    "print(\"MAPE score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_epoch50, y_test_jsmr).round(2)))\n",
    "\n",
    "print('epoch 100')\n",
    "print(\"MAPE score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_epoch100, y_test_antm).round(2)))\n",
    "print(\"MAPE score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_epoch100, y_test_asii).round(2)))\n",
    "print(\"MAPE score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_epoch100, y_test_icbp).round(2)))\n",
    "print(\"MAPE score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_epoch100, y_test_jsmr).round(2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUNING NEURON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_279\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_479 (LSTM)             (None, 16)                1152      \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,254\n",
      "Trainable params: 1,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 20ms/step - loss: 0.0557 - mae: 0.0557 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0203 - mae: 0.0203 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0172 - mae: 0.0172 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0161 - val_mae: 0.0161\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0141 - val_mae: 0.0141\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_unit16 = Sequential([\n",
    "  LSTM(16, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_unit16.summary()\n",
    "\n",
    "simple_model_one_antm_unit16.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_unit16 = simple_model_one_antm_unit16.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_unit16 = simple_model_one_antm_unit16.predict(X_test_rs_antm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_280\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_480 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 20ms/step - loss: 0.0769 - mae: 0.0769 - val_loss: 0.0285 - val_mae: 0.0285\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0187 - mae: 0.0187 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0181 - mae: 0.0181 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0168 - mae: 0.0168 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0145 - val_mae: 0.0145\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_unit8 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_unit8.summary()\n",
    "\n",
    "simple_model_one_antm_unit8.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_unit8 = simple_model_one_antm_unit8.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_unit8 = simple_model_one_antm_unit8.predict(X_test_rs_antm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_281\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_481 (LSTM)             (None, 64)                16896     \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,286\n",
      "Trainable params: 17,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 6s 68ms/step - loss: 0.0581 - mae: 0.0581 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0193 - val_mae: 0.0193\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0183 - mae: 0.0183 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0190 - mae: 0.0190 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 3s 59ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0179 - mae: 0.0179 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 3s 57ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0195 - val_mae: 0.0195\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 4s 72ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0171 - mae: 0.0171 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0178 - mae: 0.0178 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 4s 74ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 4s 80ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 4s 74ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 4s 72ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0159 - mae: 0.0159 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0136 - val_mae: 0.0136\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0134 - val_mae: 0.0134\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0135 - val_mae: 0.0135\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0149 - val_mae: 0.0149\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_antm_unit64 = Sequential([\n",
    "  LSTM(64, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_antm.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_antm_unit64.summary()\n",
    "\n",
    "simple_model_one_antm_unit64.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_antm_unit64 = simple_model_one_antm_unit64.fit(X_train_rs_antm, y_train_antm,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_antm_unit64 = simple_model_one_antm_unit64.predict(X_test_rs_antm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_282\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_482 (LSTM)             (None, 16)                1152      \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,254\n",
      "Trainable params: 1,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 18ms/step - loss: 0.1157 - mae: 0.1157 - val_loss: 0.0447 - val_mae: 0.0447\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0379 - mae: 0.0379 - val_loss: 0.0363 - val_mae: 0.0363\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0356 - mae: 0.0356 - val_loss: 0.0366 - val_mae: 0.0366\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0367 - mae: 0.0367 - val_loss: 0.0468 - val_mae: 0.0468\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0361 - mae: 0.0361 - val_loss: 0.0592 - val_mae: 0.0592\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0354 - mae: 0.0354 - val_loss: 0.0327 - val_mae: 0.0327\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0344 - mae: 0.0344 - val_loss: 0.0335 - val_mae: 0.0335\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0334 - mae: 0.0334 - val_loss: 0.0384 - val_mae: 0.0384\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0345 - mae: 0.0345 - val_loss: 0.0369 - val_mae: 0.0369\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0333 - mae: 0.0333 - val_loss: 0.0320 - val_mae: 0.0320\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0324 - mae: 0.0324 - val_loss: 0.0353 - val_mae: 0.0353\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0339 - mae: 0.0339 - val_loss: 0.0326 - val_mae: 0.0326\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0337 - mae: 0.0337 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0319 - mae: 0.0319 - val_loss: 0.0334 - val_mae: 0.0334\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0320 - mae: 0.0320 - val_loss: 0.0353 - val_mae: 0.0353\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0314 - mae: 0.0314 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0315 - mae: 0.0315 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0326 - val_mae: 0.0326\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0354 - val_mae: 0.0354\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0328 - val_mae: 0.0328\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0321 - mae: 0.0321 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0318 - mae: 0.0318 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0330 - val_mae: 0.0330\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0347 - val_mae: 0.0347\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0335 - val_mae: 0.0335\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0319 - mae: 0.0319 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0302 - mae: 0.030 - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0363 - val_mae: 0.0363\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0319 - mae: 0.0319 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0334 - val_mae: 0.0334\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0330 - val_mae: 0.0330\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0350 - val_mae: 0.0350\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0315 - mae: 0.031 - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0321 - val_mae: 0.0321\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0298 - mae: 0.0298 - val_loss: 0.0319 - val_mae: 0.0319\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0298 - mae: 0.0298 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0360 - val_mae: 0.0360\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0325 - val_mae: 0.0325\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0319 - mae: 0.0319 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0327 - val_mae: 0.0327\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0330 - val_mae: 0.0330\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_unit16 = Sequential([\n",
    "  LSTM(16, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_unit16.summary()\n",
    "\n",
    "simple_model_one_asii_unit16.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_unit16 = simple_model_one_asii_unit16.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_unit16 = simple_model_one_asii_unit16.predict(X_test_rs_asii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_283\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_483 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.1417 - mae: 0.1417 - val_loss: 0.0481 - val_mae: 0.0481\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0403 - mae: 0.0403 - val_loss: 0.0362 - val_mae: 0.0362\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0356 - mae: 0.0356 - val_loss: 0.0344 - val_mae: 0.0344\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0352 - mae: 0.0352 - val_loss: 0.0356 - val_mae: 0.0356\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0341 - mae: 0.0341 - val_loss: 0.0328 - val_mae: 0.0328\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0332 - mae: 0.0332 - val_loss: 0.0328 - val_mae: 0.0328\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0330 - mae: 0.0330 - val_loss: 0.0342 - val_mae: 0.0342\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0328 - mae: 0.0328 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0326 - mae: 0.0326 - val_loss: 0.0320 - val_mae: 0.0320\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0326 - mae: 0.0326 - val_loss: 0.0327 - val_mae: 0.0327\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0347 - mae: 0.0347 - val_loss: 0.0402 - val_mae: 0.0402\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0322 - mae: 0.0322 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0323 - mae: 0.0323 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0320 - val_mae: 0.0320\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0320 - mae: 0.0320 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0314 - mae: 0.0314 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0315 - mae: 0.0315 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0315 - mae: 0.0315 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0328 - val_mae: 0.0328\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0321 - mae: 0.0321 - val_loss: 0.0325 - val_mae: 0.0325\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0340 - val_mae: 0.0340\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0318 - val_mae: 0.0318\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0315 - mae: 0.0315 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0320 - val_mae: 0.0320\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0327 - val_mae: 0.0327\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0328 - val_mae: 0.0328\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0301 - mae: 0.030 - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0326 - val_mae: 0.0326\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0330 - val_mae: 0.0330\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0321 - mae: 0.0321 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0298 - mae: 0.0298 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0302 - val_mae: 0.0302\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0298 - mae: 0.0298 - val_loss: 0.0346 - val_mae: 0.0346\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0307 - mae: 0.030 - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0302 - val_mae: 0.0302\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0334 - val_mae: 0.0334\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0347 - val_mae: 0.0347\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0306 - val_mae: 0.0306\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_unit8 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_unit8.summary()\n",
    "\n",
    "simple_model_one_asii_unit8.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_unit8 = simple_model_one_asii_unit8.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_unit8 = simple_model_one_asii_unit8.predict(X_test_rs_asii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_284\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_484 (LSTM)             (None, 64)                16896     \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,286\n",
      "Trainable params: 17,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 6s 72ms/step - loss: 0.0868 - mae: 0.0868 - val_loss: 0.0636 - val_mae: 0.0636\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0430 - mae: 0.0430 - val_loss: 0.0408 - val_mae: 0.0408\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0353 - mae: 0.0353 - val_loss: 0.0354 - val_mae: 0.0354\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0365 - mae: 0.0365 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0364 - mae: 0.0364 - val_loss: 0.0396 - val_mae: 0.0396\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0344 - mae: 0.0344 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0353 - mae: 0.0353 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0328 - mae: 0.0328 - val_loss: 0.0347 - val_mae: 0.0347\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0331 - mae: 0.0331 - val_loss: 0.0382 - val_mae: 0.0382\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0329 - mae: 0.0329 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0322 - mae: 0.0322 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0355 - val_mae: 0.0355\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0309 - mae: 0.0309 - val_loss: 0.0323 - val_mae: 0.0323\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0336 - mae: 0.0336 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0314 - mae: 0.0314 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0330 - val_mae: 0.0330\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0422 - val_mae: 0.0422\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0352 - val_mae: 0.0352\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0353 - val_mae: 0.0353\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0330 - mae: 0.0330 - val_loss: 0.0362 - val_mae: 0.0362\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0319 - mae: 0.0319 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0325 - mae: 0.0325 - val_loss: 0.0349 - val_mae: 0.0349\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0330 - val_mae: 0.0330\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0314 - mae: 0.0314 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0330 - val_mae: 0.0330\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0329 - val_mae: 0.0329\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0387 - val_mae: 0.0387\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0318 - mae: 0.0318 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0323 - mae: 0.0323 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 0.0325 - mae: 0.0325 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0336 - val_mae: 0.0336\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0311 - val_mae: 0.0311\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0316 - val_mae: 0.0316\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0315 - val_mae: 0.0315\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0326 - val_mae: 0.0326\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0325 - val_mae: 0.0325\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0324 - val_mae: 0.0324\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0369 - val_mae: 0.0369\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0340 - mae: 0.0340 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0297 - mae: 0.0297 - val_loss: 0.0314 - val_mae: 0.0314\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0343 - val_mae: 0.0343\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0310 - val_mae: 0.0310\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0303 - val_mae: 0.0303\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0302 - mae: 0.0302 - val_loss: 0.0321 - val_mae: 0.0321\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0358 - val_mae: 0.0358\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0306 - val_mae: 0.0306\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0336 - val_mae: 0.033603 - mae: 0\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0308 - mae: 0.0308 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0312 - val_mae: 0.0312\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0343 - val_mae: 0.0343\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0325 - val_mae: 0.0325\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0313 - mae: 0.0313 - val_loss: 0.0335 - val_mae: 0.0335\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0321 - val_mae: 0.0321\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0308 - val_mae: 0.0308\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0325 - val_mae: 0.0325\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0313 - val_mae: 0.0313\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0317 - val_mae: 0.0317\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0306 - mae: 0.0306 - val_loss: 0.0353 - val_mae: 0.0353\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0309 - val_mae: 0.0309\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0311 - mae: 0.0311 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0301 - mae: 0.0301 - val_loss: 0.0307 - val_mae: 0.0307\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0304 - val_mae: 0.0304\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0320 - val_mae: 0.0320\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_asii_unit64 = Sequential([\n",
    "  LSTM(64, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_asii.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_asii_unit64.summary()\n",
    "\n",
    "simple_model_one_asii_unit64.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_asii_unit64 = simple_model_one_asii_unit64.fit(X_train_rs_asii, y_train_asii,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_asii_unit64 = simple_model_one_asii_unit64.predict(X_test_rs_asii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_285\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_485 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_285 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.1369 - mae: 0.1369 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0184 - mae: 0.0184 - val_loss: 0.0171 - val_mae: 0.0171\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0192 - val_mae: 0.0192\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0194 - val_mae: 0.0194\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0166 - val_mae: 0.0166\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0168 - val_mae: 0.0168\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0164 - mae: 0.0164 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0177 - val_mae: 0.0177\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0200 - val_mae: 0.0200\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0197 - val_mae: 0.0197\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0183 - val_mae: 0.0183\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0165 - val_mae: 0.0165\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0140 - val_mae: 0.0140\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_unit8 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_unit8.summary()\n",
    "\n",
    "simple_model_one_icbp_unit8.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_unit8 = simple_model_one_icbp_unit8.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_unit8 = simple_model_one_icbp_unit8.predict(X_test_rs_icbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_286\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_486 (LSTM)             (None, 16)                1152      \n",
      "                                                                 \n",
      " dense_286 (Dense)           (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,254\n",
      "Trainable params: 1,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.1069 - mae: 0.1069 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0219 - mae: 0.0219 - val_loss: 0.0258 - val_mae: 0.0258\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0189 - mae: 0.0189 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0175 - mae: 0.0175 - val_loss: 0.0190 - val_mae: 0.0190\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0186 - val_mae: 0.0186\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0176 - mae: 0.0176 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0201 - val_mae: 0.0201\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0177 - mae: 0.0177 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0155 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0157 - val_mae: 0.0157\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0162 - mae: 0.0162 - val_loss: 0.0176 - val_mae: 0.0176\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0170 - mae: 0.0170 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0144 - mae: 0.0144- ETA: 0s - loss: 0.0143 - mae: 0.0 - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0184 - val_mae: 0.0184\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0178 - val_mae: 0.0178\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0173 - val_mae: 0.0173\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0189 - val_mae: 0.0189\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0156 - mae: 0.015 - 0s 9ms/step - loss: 0.0156 - mae: 0.0156 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0169 - val_mae: 0.0169\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0191 - val_mae: 0.0191\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0185 - val_mae: 0.0185\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0151 - val_mae: 0.0151\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0158 - val_mae: 0.0158\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0175 - val_mae: 0.0175\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_unit16 = Sequential([\n",
    "  LSTM(16, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_unit16.summary()\n",
    "\n",
    "simple_model_one_icbp_unit16.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_unit16 = simple_model_one_icbp_unit16.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_unit16 = simple_model_one_icbp_unit16.predict(X_test_rs_icbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_287\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_487 (LSTM)             (None, 64)                16896     \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,286\n",
      "Trainable params: 17,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 6s 74ms/step - loss: 0.0591 - mae: 0.0591 - val_loss: 0.0336 - val_mae: 0.0336\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0300 - mae: 0.0300 - val_loss: 0.0211 - val_mae: 0.0211\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0197 - mae: 0.0197 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0174 - mae: 0.0174 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0182 - mae: 0.0182 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0166 - mae: 0.0166 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0150 - val_mae: 0.0150\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 4s 72ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0175 - val_mae: 0.0175\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 0.0157 - mae: 0.0157 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 4s 72ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.0147 - mae: 0.0147 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0164 - val_mae: 0.0164\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0174 - val_mae: 0.0174\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0169 - mae: 0.0169 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0206 - val_mae: 0.0206\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0158 - mae: 0.0158 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 4s 74ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0180 - val_mae: 0.0180\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 4s 79ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0154 - val_mae: 0.0154\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0253 - val_mae: 0.0253\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.0165 - mae: 0.0165 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0159 - val_mae: 0.0159\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 4s 80ms/step - loss: 0.0154 - mae: 0.0154 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 5s 96ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 5s 99ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 5s 93ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 4s 74ms/step - loss: 0.0160 - mae: 0.0160 - val_loss: 0.0155 - val_mae: 0.0155\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0172 - val_mae: 0.0172\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 5s 87ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 5s 96ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0170 - val_mae: 0.0170\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 5s 98ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0196 - val_mae: 0.0196\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 5s 99ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 4s 87ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 6s 112ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 6s 109ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 5s 100ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 5s 89ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0162 - val_mae: 0.0162\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 4s 86ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 4s 85ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 5s 101ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 5s 99ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 4s 85ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0179 - val_mae: 0.0179\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0161 - mae: 0.0161 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0156 - val_mae: 0.0156\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 4s 79ms/step - loss: 0.0150 - mae: 0.0150 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 4s 74ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0160 - val_mae: 0.0160\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 4s 72ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 4s 72ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 4s 79ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 4s 79ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0167 - val_mae: 0.0167\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0143 - mae: 0.0143 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0139 - val_mae: 0.0139\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0156 - val_mae: 0.0156oss: 0.0138 - mae: 0.0\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0152 - val_mae: 0.0152\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0145 - val_mae: 0.0145\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0139 - mae: 0.0139 - val_loss: 0.0138 - val_mae: 0.0138\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0153 - mae: 0.0153 - val_loss: 0.0146 - val_mae: 0.0146\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0149 - mae: 0.0149 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 4s 76ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0146 - mae: 0.0146 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0137 - mae: 0.0137 - val_loss: 0.0142 - val_mae: 0.0142\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0141 - mae: 0.0141 - val_loss: 0.0149 - val_mae: 0.0149\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0152 - mae: 0.0152 - val_loss: 0.0147 - val_mae: 0.0147\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0144 - mae: 0.0144 - val_loss: 0.0148 - val_mae: 0.0148\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0148 - mae: 0.0148 - val_loss: 0.0163 - val_mae: 0.0163\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_icbp_unit64 = Sequential([\n",
    "  LSTM(64, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_icbp.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_icbp_unit64.summary()\n",
    "\n",
    "simple_model_one_icbp_unit64.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_icbp_unit64 = simple_model_one_icbp_unit64.fit(X_train_rs_icbp, y_train_icbp,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_icbp_unit64 = simple_model_one_icbp_unit64.predict(X_test_rs_icbp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_288\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_488 (LSTM)             (None, 8)                 320       \n",
      "                                                                 \n",
      " dense_288 (Dense)           (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374\n",
      "Trainable params: 374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.1613 - mae: 0.1613 - val_loss: 0.0336 - val_mae: 0.0336\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0305 - mae: 0.0305 - val_loss: 0.0340 - val_mae: 0.0340\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0286 - mae: 0.0286 - val_loss: 0.0281 - val_mae: 0.0281\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0258 - mae: 0.0258 - val_loss: 0.0274 - val_mae: 0.0274\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0253 - mae: 0.0253 - val_loss: 0.0245 - val_mae: 0.0245\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0292 - val_mae: 0.0292\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0264 - mae: 0.0264 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0250 - val_mae: 0.0250\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0278 - val_mae: 0.0278\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0236 - val_mae: 0.0236\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0236 - mae: 0.023 - 0s 9ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0242 - mae: 0.0242 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0243 - mae: 0.0243 - val_loss: 0.0261 - val_mae: 0.0261\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0250 - val_mae: 0.0250\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0270 - val_mae: 0.0270\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0214 - val_mae: 0.0214\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0270 - val_mae: 0.0270\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0289 - val_mae: 0.0289\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0245 - val_mae: 0.0245\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0214 - val_mae: 0.0214\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0213 - val_mae: 0.0213\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_unit8 = Sequential([\n",
    "  LSTM(8, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_unit8.summary()\n",
    "\n",
    "simple_model_one_jsmr_unit8.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_unit8 = simple_model_one_jsmr_unit8.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_unit8 = simple_model_one_jsmr_unit8.predict(X_test_rs_jsmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_289\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_489 (LSTM)             (None, 16)                1152      \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,254\n",
      "Trainable params: 1,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 3s 19ms/step - loss: 0.0959 - mae: 0.0959 - val_loss: 0.0302 - val_mae: 0.0302\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0296 - mae: 0.0296 - val_loss: 0.0293 - val_mae: 0.0293\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0360 - val_mae: 0.0360\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0278 - mae: 0.0278 - val_loss: 0.0279 - val_mae: 0.0279\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0287 - mae: 0.0287 - val_loss: 0.0293 - val_mae: 0.0293\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0262 - mae: 0.0262 - val_loss: 0.0282 - val_mae: 0.0282\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0258 - mae: 0.0258 - val_loss: 0.0254 - val_mae: 0.0254\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0290 - val_mae: 0.0290\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0262 - mae: 0.0262 - val_loss: 0.0305 - val_mae: 0.0305\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0286 - val_mae: 0.0286\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0249 - mae: 0.0249 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0269 - val_mae: 0.0269\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0256 - val_mae: 0.0256\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0253 - mae: 0.0253 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0261 - mae: 0.0261 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0240 - val_mae: 0.0240\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0243 - val_mae: 0.0243\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0237 - val_mae: 0.0237\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0265 - val_mae: 0.0265\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0292 - val_mae: 0.0292\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0289 - val_mae: 0.0289\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0245 - mae: 0.0245 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0272 - val_mae: 0.0272\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0236 - val_mae: 0.0236\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0245 - val_mae: 0.0245\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0226 - val_mae: 0.0226\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_unit16 = Sequential([\n",
    "  LSTM(16, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_unit16.summary()\n",
    "\n",
    "simple_model_one_jsmr_unit16.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=[\n",
    "           'mae',\n",
    "           ],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_unit16 = simple_model_one_jsmr_unit16.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_unit16 = simple_model_one_jsmr_unit16.predict(X_test_rs_jsmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_290\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_490 (LSTM)             (None, 64)                16896     \n",
      "                                                                 \n",
      " dense_290 (Dense)           (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,286\n",
      "Trainable params: 17,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 6s 74ms/step - loss: 0.0725 - mae: 0.0725 - val_loss: 0.0302 - val_mae: 0.0302\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 3s 52ms/step - loss: 0.0344 - mae: 0.0344 - val_loss: 0.0376 - val_mae: 0.0376\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 3s 57ms/step - loss: 0.0304 - mae: 0.0304 - val_loss: 0.0451 - val_mae: 0.0451\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 3s 53ms/step - loss: 0.0316 - mae: 0.0316 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 3s 59ms/step - loss: 0.0285 - mae: 0.0285 - val_loss: 0.0252 - val_mae: 0.0252\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0258 - mae: 0.0258 - val_loss: 0.0233 - val_mae: 0.0233\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0283 - val_mae: 0.0283\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0245 - val_mae: 0.0245\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0229 - val_mae: 0.0229\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0258 - mae: 0.0258 - val_loss: 0.0322 - val_mae: 0.0322\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 3s 58ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0279 - val_mae: 0.0279\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0243 - mae: 0.0243 - val_loss: 0.0254 - val_mae: 0.0254\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0216 - val_mae: 0.0216s - loss: 0.0230 - ma\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0282 - val_mae: 0.0282\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0245 - mae: 0.0245 - val_loss: 0.0254 - val_mae: 0.0254\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0243 - mae: 0.0243 - val_loss: 0.0249 - val_mae: 0.0249\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0257 - val_mae: 0.0257\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 3s 59ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0245 - val_mae: 0.0245\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0255 - val_mae: 0.0255\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 5s 94ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 5s 91ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0276 - val_mae: 0.0276\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 5s 94ms/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 4s 85ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0251 - val_mae: 0.0251\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 5s 88ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 5s 89ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0269 - val_mae: 0.0269\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 5s 88ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 5s 97ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 5s 96ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 4s 86ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 5s 89ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0242 - val_mae: 0.0242\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 5s 91ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 5s 94ms/step - loss: 0.0245 - mae: 0.0245 - val_loss: 0.0263 - val_mae: 0.0263\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 5s 92ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0259 - val_mae: 0.0259\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0235 - val_mae: 0.0235\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.0243 - mae: 0.0243 - val_loss: 0.0226 - val_mae: 0.0226\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 4s 85ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0227 - val_mae: 0.0227\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0234 - val_mae: 0.0234\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0249 - val_mae: 0.0249\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0251 - val_mae: 0.02510 \n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0332 - val_mae: 0.0332\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0244 - val_mae: 0.0244\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 4s 75ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0230 - val_mae: 0.0230\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0270 - val_mae: 0.0270\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0246 - val_mae: 0.0246\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0257 - val_mae: 0.0257\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0231 - val_mae: 0.0231\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 3s 62ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0224 - val_mae: 0.0224\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0221 - val_mae: 0.0221\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0213 - val_mae: 0.0213\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0257 - val_mae: 0.0257\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0238 - val_mae: 0.0238\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0247 - val_mae: 0.0247\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0225 - val_mae: 0.0225\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0266 - val_mae: 0.0266\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0236 - val_mae: 0.0236\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0239 - val_mae: 0.0239\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0242 - mae: 0.0242 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0222 - val_mae: 0.0222\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0220 - val_mae: 0.0220\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0218 - val_mae: 0.0218\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0232 - val_mae: 0.0232\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0264 - val_mae: 0.0264\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0219 - val_mae: 0.0219\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0215 - val_mae: 0.0215\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0223 - val_mae: 0.0223\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0241 - val_mae: 0.0241\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0228 - val_mae: 0.0228\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0216 - val_mae: 0.0216\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0217 - val_mae: 0.0217\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 3s 67ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0225 - val_mae: 0.0225\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "simple_model_one_jsmr_unit64 = Sequential([\n",
    "  LSTM(64, activation='tanh',input_shape=(n_timesteps, n_features)),\n",
    "  Dense(y_train_jsmr.shape[1]),\n",
    "])\n",
    "\n",
    "simple_model_one_jsmr_unit64.summary()\n",
    "\n",
    "simple_model_one_jsmr_unit64.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mae'],\n",
    ")\n",
    "\n",
    "smod_history_one_jsmr_unit64 = simple_model_one_jsmr_unit64.fit(X_train_rs_jsmr, y_train_jsmr,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          shuffle = True\n",
    ")\n",
    "\n",
    "preds_one_jsmr_unit64 = simple_model_one_jsmr_unit64.predict(X_test_rs_jsmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATRIKS EVALUASI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron 8\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.01\n",
      "mae score jsmr: 0.02\n",
      "neuron 16\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.02\n",
      "mae score jsmr: 0.02\n",
      "neuron 64\n",
      "mae score antm: 0.02\n",
      "mae score asii: 0.03\n",
      "mae score icbp: 0.02\n",
      "mae score jsmr: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Forecast\n",
    "# mae score\n",
    "print('neuron 8')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_unit8, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_unit8, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_unit8, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_unit8, y_test_jsmr).round(2)))\n",
    "\n",
    "print('neuron 16')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_unit16, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_unit16, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_unit16, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_unit16, y_test_jsmr).round(2)))\n",
    "\n",
    "print('neuron 64')\n",
    "print(\"mae score antm: \"+str(mean_absolute_error(preds_one_antm_unit64, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_error(preds_one_asii_unit64, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_error(preds_one_icbp_unit64, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_error(preds_one_jsmr_unit64, y_test_jsmr).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron 8\n",
      "mae score antm: 0.98\n",
      "mae score asii: 0.96\n",
      "mae score icbp: 0.99\n",
      "mae score jsmr: 0.98\n",
      "neuron 16\n",
      "mae score antm: 0.98\n",
      "mae score asii: 0.96\n",
      "mae score icbp: 0.99\n",
      "mae score jsmr: 0.98\n",
      "neuron 64\n",
      "mae score antm: 0.98\n",
      "mae score asii: 0.96\n",
      "mae score icbp: 0.99\n",
      "mae score jsmr: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Forecast\n",
    "# mae score\n",
    "print('neuron 8')\n",
    "print(\"mae score antm: \"+str(r2_score(preds_one_antm_unit8, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(r2_score(preds_one_asii_unit8, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(r2_score(preds_one_icbp_unit8, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(r2_score(preds_one_jsmr_unit8, y_test_jsmr).round(2)))\n",
    "\n",
    "print('neuron 16')\n",
    "print(\"mae score antm: \"+str(r2_score(preds_one_antm_unit16, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(r2_score(preds_one_asii_unit16, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(r2_score(preds_one_icbp_unit16, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(r2_score(preds_one_jsmr_unit16, y_test_jsmr).round(2)))\n",
    "\n",
    "print('neuron 64')\n",
    "print(\"mae score antm: \"+str(r2_score(preds_one_antm_unit64, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(r2_score(preds_one_asii_unit64, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(r2_score(preds_one_icbp_unit64, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(r2_score(preds_one_jsmr_unit64, y_test_jsmr).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron 8\n",
      "mae score antm: 0.07\n",
      "mae score asii: 0.06\n",
      "mae score icbp: 0.04\n",
      "mae score jsmr: 0.07\n",
      "neuron 16\n",
      "mae score antm: 0.07\n",
      "mae score asii: 0.07\n",
      "mae score icbp: 0.05\n",
      "mae score jsmr: 0.35\n",
      "neuron 64\n",
      "mae score antm: 0.07\n",
      "mae score asii: 0.06\n",
      "mae score icbp: 0.07\n",
      "mae score jsmr: 0.06\n"
     ]
    }
   ],
   "source": [
    "# Forecast\n",
    "# mae score\n",
    "print('neuron 8')\n",
    "print(\"mae score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_unit8, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_unit8, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_unit8, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_unit8, y_test_jsmr).round(2)))\n",
    "\n",
    "print('neuron 16')\n",
    "print(\"mae score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_unit16, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_unit16, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_unit16, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_unit16, y_test_jsmr).round(2)))\n",
    "\n",
    "print('neuron 64')\n",
    "print(\"mae score antm: \"+str(mean_absolute_percentage_error(preds_one_antm_unit64, y_test_antm).round(2)))\n",
    "print(\"mae score asii: \"+str(mean_absolute_percentage_error(preds_one_asii_unit64, y_test_asii).round(2)))\n",
    "print(\"mae score icbp: \"+str(mean_absolute_percentage_error(preds_one_icbp_unit64, y_test_icbp).round(2)))\n",
    "print(\"mae score jsmr: \"+str(mean_absolute_percentage_error(preds_one_jsmr_unit64, y_test_jsmr).round(2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3816365fdcd687a07caedfe721e5894fb1dd0a24482efb967fc5a605423a1021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
